---
title: Evaluate model performance in Machine Learning | Microsoft Docs
description: Explains how to evaluate model performance in Azure Machine Learning.
services: machine-learning
documentationcenter: ''
author: heatherbshapiro
ms.author: hshapiro
manager: hjerez
editor: cgronlun
ms.assetid: 5dc5348a-4488-4536-99eb-ff105be9b160
ms.service: machine-learning
ms.component: studio
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/20/2017
ms.openlocfilehash: bb49fd2fe7f72e211fbbda7cffdd2308c2c36fba
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44966356"
---
# <a name="how-to-evaluate-model-performance-in-azure-machine-learning"></a><span data-ttu-id="b1868-103">How to evaluate model performance in Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="b1868-103">How to evaluate model performance in Azure Machine Learning</span></span>
<span data-ttu-id="b1868-104">This article demonstrates how to evaluate the performance of a model in Azure Machine Learning Studio and provides a brief explanation of the metrics available for this task.</span><span class="sxs-lookup"><span data-stu-id="b1868-104">This article demonstrates how to evaluate the performance of a model in Azure Machine Learning Studio and provides a brief explanation of the metrics available for this task.</span></span> <span data-ttu-id="b1868-105">Three common supervised learning scenarios are presented:</span><span class="sxs-lookup"><span data-stu-id="b1868-105">Three common supervised learning scenarios are presented:</span></span> 

* <span data-ttu-id="b1868-106">regression</span><span class="sxs-lookup"><span data-stu-id="b1868-106">regression</span></span>
* <span data-ttu-id="b1868-107">binary classification</span><span class="sxs-lookup"><span data-stu-id="b1868-107">binary classification</span></span> 
* <span data-ttu-id="b1868-108">multiclass classification</span><span class="sxs-lookup"><span data-stu-id="b1868-108">multiclass classification</span></span>

[!INCLUDE [machine-learning-free-trial](../../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="b1868-109">Evaluating the performance of a model is one of the core stages in the data science process.</span><span class="sxs-lookup"><span data-stu-id="b1868-109">Evaluating the performance of a model is one of the core stages in the data science process.</span></span> <span data-ttu-id="b1868-110">It indicates how successful the scoring (predictions) of a dataset has been by a trained model.</span><span class="sxs-lookup"><span data-stu-id="b1868-110">It indicates how successful the scoring (predictions) of a dataset has been by a trained model.</span></span> 

<span data-ttu-id="b1868-111">Azure Machine Learning supports model evaluation through two of its main machine learning modules: [Evaluate Model][evaluate-model] and [Cross-Validate Model][cross-validate-model].</span><span class="sxs-lookup"><span data-stu-id="b1868-111">Azure Machine Learning supports model evaluation through two of its main machine learning modules: [Evaluate Model][evaluate-model] and [Cross-Validate Model][cross-validate-model].</span></span> <span data-ttu-id="b1868-112">These modules allow you to see how your model performs in terms of a number of metrics that are commonly used in machine learning and statistics.</span><span class="sxs-lookup"><span data-stu-id="b1868-112">These modules allow you to see how your model performs in terms of a number of metrics that are commonly used in machine learning and statistics.</span></span>

## <a name="evaluation-vs-cross-validation"></a><span data-ttu-id="b1868-113">Evaluation vs. Cross Validation</span><span class="sxs-lookup"><span data-stu-id="b1868-113">Evaluation vs. Cross Validation</span></span>
<span data-ttu-id="b1868-114">Evaluation and cross validation are standard ways to measure the performance of your model.</span><span class="sxs-lookup"><span data-stu-id="b1868-114">Evaluation and cross validation are standard ways to measure the performance of your model.</span></span> <span data-ttu-id="b1868-115">They both generate evaluation metrics that you can inspect or compare against those of other models.</span><span class="sxs-lookup"><span data-stu-id="b1868-115">They both generate evaluation metrics that you can inspect or compare against those of other models.</span></span>

<span data-ttu-id="b1868-116">[Evaluate Model][evaluate-model] expects a scored dataset as input (or 2 in case you would like to compare the performance of 2 different models).</span><span class="sxs-lookup"><span data-stu-id="b1868-116">[Evaluate Model][evaluate-model] expects a scored dataset as input (or 2 in case you would like to compare the performance of 2 different models).</span></span> <span data-ttu-id="b1868-117">This means that you need to train your model using the [Train Model][train-model] module and make predictions on some dataset using the [Score Model][score-model] module, before you can evaluate the results.</span><span class="sxs-lookup"><span data-stu-id="b1868-117">This means that you need to train your model using the [Train Model][train-model] module and make predictions on some dataset using the [Score Model][score-model] module, before you can evaluate the results.</span></span> <span data-ttu-id="b1868-118">The evaluation is based on the scored labels/probabilities along with the true labels, all of which are output by the [Score Model][score-model] module.</span><span class="sxs-lookup"><span data-stu-id="b1868-118">The evaluation is based on the scored labels/probabilities along with the true labels, all of which are output by the [Score Model][score-model] module.</span></span>

<span data-ttu-id="b1868-119">Alternatively, you can use cross validation to perform a number of train-score-evaluate operations (10 folds) automatically on different subsets of the input data.</span><span class="sxs-lookup"><span data-stu-id="b1868-119">Alternatively, you can use cross validation to perform a number of train-score-evaluate operations (10 folds) automatically on different subsets of the input data.</span></span> <span data-ttu-id="b1868-120">The input data is split into 10 parts, where one is reserved for testing, and the other 9 for training.</span><span class="sxs-lookup"><span data-stu-id="b1868-120">The input data is split into 10 parts, where one is reserved for testing, and the other 9 for training.</span></span> <span data-ttu-id="b1868-121">This process is repeated 10 times and the evaluation metrics are averaged.</span><span class="sxs-lookup"><span data-stu-id="b1868-121">This process is repeated 10 times and the evaluation metrics are averaged.</span></span> <span data-ttu-id="b1868-122">This helps in determining how well a model would generalize to new datasets.</span><span class="sxs-lookup"><span data-stu-id="b1868-122">This helps in determining how well a model would generalize to new datasets.</span></span> <span data-ttu-id="b1868-123">The [Cross-Validate Model][cross-validate-model] module takes in an untrained model and some labeled dataset and outputs the evaluation results of each of the 10 folds, in addition to the averaged results.</span><span class="sxs-lookup"><span data-stu-id="b1868-123">The [Cross-Validate Model][cross-validate-model] module takes in an untrained model and some labeled dataset and outputs the evaluation results of each of the 10 folds, in addition to the averaged results.</span></span>

<span data-ttu-id="b1868-124">In the following sections, we will build simple regression and classification models and evaluate their performance, using both the [Evaluate Model][evaluate-model] and the [Cross-Validate Model][cross-validate-model] modules.</span><span class="sxs-lookup"><span data-stu-id="b1868-124">In the following sections, we will build simple regression and classification models and evaluate their performance, using both the [Evaluate Model][evaluate-model] and the [Cross-Validate Model][cross-validate-model] modules.</span></span>

## <a name="evaluating-a-regression-model"></a><span data-ttu-id="b1868-125">Evaluating a Regression Model</span><span class="sxs-lookup"><span data-stu-id="b1868-125">Evaluating a Regression Model</span></span>
<span data-ttu-id="b1868-126">Assume we want to predict a car’s price using some features such as dimensions, horsepower, engine specs, and so on.</span><span class="sxs-lookup"><span data-stu-id="b1868-126">Assume we want to predict a car’s price using some features such as dimensions, horsepower, engine specs, and so on.</span></span> <span data-ttu-id="b1868-127">This is a typical regression problem, where the target variable (*price*) is a continuous numeric value.</span><span class="sxs-lookup"><span data-stu-id="b1868-127">This is a typical regression problem, where the target variable (*price*) is a continuous numeric value.</span></span> <span data-ttu-id="b1868-128">We can fit a simple linear regression model that, given the feature values of a certain car, can predict the price of that car.</span><span class="sxs-lookup"><span data-stu-id="b1868-128">We can fit a simple linear regression model that, given the feature values of a certain car, can predict the price of that car.</span></span> <span data-ttu-id="b1868-129">This regression model can be used to score the same dataset we trained on.</span><span class="sxs-lookup"><span data-stu-id="b1868-129">This regression model can be used to score the same dataset we trained on.</span></span> <span data-ttu-id="b1868-130">Once we have the predicted prices for all of the cars, we can evaluate the performance of the model by looking at how much the predictions deviate from the actual prices on average.</span><span class="sxs-lookup"><span data-stu-id="b1868-130">Once we have the predicted prices for all of the cars, we can evaluate the performance of the model by looking at how much the predictions deviate from the actual prices on average.</span></span> <span data-ttu-id="b1868-131">To illustrate this, we use the *Automobile price data (Raw) dataset* available in the **Saved Datasets** section in Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="b1868-131">To illustrate this, we use the *Automobile price data (Raw) dataset* available in the **Saved Datasets** section in Azure Machine Learning Studio.</span></span>

### <a name="creating-the-experiment"></a><span data-ttu-id="b1868-132">Creating the Experiment</span><span class="sxs-lookup"><span data-stu-id="b1868-132">Creating the Experiment</span></span>
<span data-ttu-id="b1868-133">Add the following modules to your workspace in Azure Machine Learning Studio:</span><span class="sxs-lookup"><span data-stu-id="b1868-133">Add the following modules to your workspace in Azure Machine Learning Studio:</span></span>

* <span data-ttu-id="b1868-134">Automobile price data (Raw)</span><span class="sxs-lookup"><span data-stu-id="b1868-134">Automobile price data (Raw)</span></span>
* <span data-ttu-id="b1868-135">[Linear Regression][linear-regression]</span><span class="sxs-lookup"><span data-stu-id="b1868-135">[Linear Regression][linear-regression]</span></span>
* <span data-ttu-id="b1868-136">[Train Model][train-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-136">[Train Model][train-model]</span></span>
* <span data-ttu-id="b1868-137">[Score Model][score-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-137">[Score Model][score-model]</span></span>
* <span data-ttu-id="b1868-138">[Evaluate Model][evaluate-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-138">[Evaluate Model][evaluate-model]</span></span>

<span data-ttu-id="b1868-139">Connect the ports as shown below in Figure 1 and set the Label column of the [Train Model][train-model] module to *price*.</span><span class="sxs-lookup"><span data-stu-id="b1868-139">Connect the ports as shown below in Figure 1 and set the Label column of the [Train Model][train-model] module to *price*.</span></span>

![Evaluating a Regression Model](./media/evaluate-model-performance/1.png)

<span data-ttu-id="b1868-141">Figure 1.</span><span class="sxs-lookup"><span data-stu-id="b1868-141">Figure 1.</span></span> <span data-ttu-id="b1868-142">Evaluating a Regression Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-142">Evaluating a Regression Model.</span></span>

### <a name="inspecting-the-evaluation-results"></a><span data-ttu-id="b1868-143">Inspecting the Evaluation Results</span><span class="sxs-lookup"><span data-stu-id="b1868-143">Inspecting the Evaluation Results</span></span>
<span data-ttu-id="b1868-144">After running the experiment, you can click on the output port of the [Evaluate Model][evaluate-model] module and select *Visualize* to see the evaluation results.</span><span class="sxs-lookup"><span data-stu-id="b1868-144">After running the experiment, you can click on the output port of the [Evaluate Model][evaluate-model] module and select *Visualize* to see the evaluation results.</span></span> <span data-ttu-id="b1868-145">The evaluation metrics available for regression models are: *Mean Absolute Error*, *Root Mean Absolute Error*, *Relative Absolute Error*, *Relative Squared Error*, and the *Coefficient of Determination*.</span><span class="sxs-lookup"><span data-stu-id="b1868-145">The evaluation metrics available for regression models are: *Mean Absolute Error*, *Root Mean Absolute Error*, *Relative Absolute Error*, *Relative Squared Error*, and the *Coefficient of Determination*.</span></span>

<span data-ttu-id="b1868-146">The term "error" here represents the difference between the predicted value and the true value.</span><span class="sxs-lookup"><span data-stu-id="b1868-146">The term "error" here represents the difference between the predicted value and the true value.</span></span> <span data-ttu-id="b1868-147">The absolute value or the square of this difference are usually computed to capture the total magnitude of error across all instances, as the difference between the predicted and true value could be negative in some cases.</span><span class="sxs-lookup"><span data-stu-id="b1868-147">The absolute value or the square of this difference are usually computed to capture the total magnitude of error across all instances, as the difference between the predicted and true value could be negative in some cases.</span></span> <span data-ttu-id="b1868-148">The error metrics measure the predictive performance of a regression model in terms of the mean deviation of its predictions from the true values.</span><span class="sxs-lookup"><span data-stu-id="b1868-148">The error metrics measure the predictive performance of a regression model in terms of the mean deviation of its predictions from the true values.</span></span> <span data-ttu-id="b1868-149">Lower error values mean the model is more accurate in making predictions.</span><span class="sxs-lookup"><span data-stu-id="b1868-149">Lower error values mean the model is more accurate in making predictions.</span></span> <span data-ttu-id="b1868-150">An overall error metric of 0 means that the model fits the data perfectly.</span><span class="sxs-lookup"><span data-stu-id="b1868-150">An overall error metric of 0 means that the model fits the data perfectly.</span></span>

<span data-ttu-id="b1868-151">The coefficient of determination, which is also known as R squared, is also a standard way of measuring how well the model fits the data.</span><span class="sxs-lookup"><span data-stu-id="b1868-151">The coefficient of determination, which is also known as R squared, is also a standard way of measuring how well the model fits the data.</span></span> <span data-ttu-id="b1868-152">It can be interpreted as the proportion of variation explained by the model.</span><span class="sxs-lookup"><span data-stu-id="b1868-152">It can be interpreted as the proportion of variation explained by the model.</span></span> <span data-ttu-id="b1868-153">A higher proportion is better in this case, where 1 indicates a perfect fit.</span><span class="sxs-lookup"><span data-stu-id="b1868-153">A higher proportion is better in this case, where 1 indicates a perfect fit.</span></span>

![Linear Regression Evaluation Metrics](./media/evaluate-model-performance/2.png)

<span data-ttu-id="b1868-155">Figure 2.</span><span class="sxs-lookup"><span data-stu-id="b1868-155">Figure 2.</span></span> <span data-ttu-id="b1868-156">Linear Regression Evaluation Metrics.</span><span class="sxs-lookup"><span data-stu-id="b1868-156">Linear Regression Evaluation Metrics.</span></span>

### <a name="using-cross-validation"></a><span data-ttu-id="b1868-157">Using Cross Validation</span><span class="sxs-lookup"><span data-stu-id="b1868-157">Using Cross Validation</span></span>
<span data-ttu-id="b1868-158">As mentioned earlier, you can perform repeated training, scoring and evaluations automatically using the [Cross-Validate Model][cross-validate-model] module.</span><span class="sxs-lookup"><span data-stu-id="b1868-158">As mentioned earlier, you can perform repeated training, scoring and evaluations automatically using the [Cross-Validate Model][cross-validate-model] module.</span></span> <span data-ttu-id="b1868-159">All you need in this case is a dataset, an untrained model, and a [Cross-Validate Model][cross-validate-model] module (see figure below).</span><span class="sxs-lookup"><span data-stu-id="b1868-159">All you need in this case is a dataset, an untrained model, and a [Cross-Validate Model][cross-validate-model] module (see figure below).</span></span> <span data-ttu-id="b1868-160">Note that you need to set the label column to *price* in the [Cross-Validate Model][cross-validate-model] module’s properties.</span><span class="sxs-lookup"><span data-stu-id="b1868-160">Note that you need to set the label column to *price* in the [Cross-Validate Model][cross-validate-model] module’s properties.</span></span>

![Cross-Validating a Regression Model](./media/evaluate-model-performance/3.png)

<span data-ttu-id="b1868-162">Figure 3.</span><span class="sxs-lookup"><span data-stu-id="b1868-162">Figure 3.</span></span> <span data-ttu-id="b1868-163">Cross-Validating a Regression Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-163">Cross-Validating a Regression Model.</span></span>

<span data-ttu-id="b1868-164">After running the experiment, you can inspect the evaluation results by clicking on the right output port of the [Cross-Validate Model][cross-validate-model] module.</span><span class="sxs-lookup"><span data-stu-id="b1868-164">After running the experiment, you can inspect the evaluation results by clicking on the right output port of the [Cross-Validate Model][cross-validate-model] module.</span></span> <span data-ttu-id="b1868-165">This will provide a detailed view of the metrics for each iteration (fold), and the averaged results of each of the metrics (Figure 4).</span><span class="sxs-lookup"><span data-stu-id="b1868-165">This will provide a detailed view of the metrics for each iteration (fold), and the averaged results of each of the metrics (Figure 4).</span></span>

![Cross-Validation Results of a Regression Model](./media/evaluate-model-performance/4.png)

<span data-ttu-id="b1868-167">Figure 4.</span><span class="sxs-lookup"><span data-stu-id="b1868-167">Figure 4.</span></span> <span data-ttu-id="b1868-168">Cross-Validation Results of a Regression Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-168">Cross-Validation Results of a Regression Model.</span></span>

## <a name="evaluating-a-binary-classification-model"></a><span data-ttu-id="b1868-169">Evaluating a Binary Classification Model</span><span class="sxs-lookup"><span data-stu-id="b1868-169">Evaluating a Binary Classification Model</span></span>
<span data-ttu-id="b1868-170">In a binary classification scenario, the target variable has only two possible outcomes, for example: {0, 1} or {false, true}, {negative, positive}.</span><span class="sxs-lookup"><span data-stu-id="b1868-170">In a binary classification scenario, the target variable has only two possible outcomes, for example: {0, 1} or {false, true}, {negative, positive}.</span></span> <span data-ttu-id="b1868-171">Assume you are given a dataset of adult employees with some demographic and employment variables, and that you are asked to predict the income level, a binary variable with the values {“<=50K”, “>50K”}.</span><span class="sxs-lookup"><span data-stu-id="b1868-171">Assume you are given a dataset of adult employees with some demographic and employment variables, and that you are asked to predict the income level, a binary variable with the values {“<=50K”, “>50K”}.</span></span> <span data-ttu-id="b1868-172">In other words, the negative class represents the employees who make less than or equal to 50K per year, and the positive class represents all other employees.</span><span class="sxs-lookup"><span data-stu-id="b1868-172">In other words, the negative class represents the employees who make less than or equal to 50K per year, and the positive class represents all other employees.</span></span> <span data-ttu-id="b1868-173">As in the regression scenario, we would train a model, score some data, and evaluate the results.</span><span class="sxs-lookup"><span data-stu-id="b1868-173">As in the regression scenario, we would train a model, score some data, and evaluate the results.</span></span> <span data-ttu-id="b1868-174">The main difference here is the choice of metrics Azure Machine Learning computes and outputs.</span><span class="sxs-lookup"><span data-stu-id="b1868-174">The main difference here is the choice of metrics Azure Machine Learning computes and outputs.</span></span> <span data-ttu-id="b1868-175">To illustrate the income level prediction scenario, we will use the [Adult](http://archive.ics.uci.edu/ml/datasets/Adult) dataset to create an Azure Machine Learning experiment and evaluate the performance of a two-class logistic regression model, a commonly used binary classifier.</span><span class="sxs-lookup"><span data-stu-id="b1868-175">To illustrate the income level prediction scenario, we will use the [Adult](http://archive.ics.uci.edu/ml/datasets/Adult) dataset to create an Azure Machine Learning experiment and evaluate the performance of a two-class logistic regression model, a commonly used binary classifier.</span></span>

### <a name="creating-the-experiment"></a><span data-ttu-id="b1868-176">Creating the Experiment</span><span class="sxs-lookup"><span data-stu-id="b1868-176">Creating the Experiment</span></span>
<span data-ttu-id="b1868-177">Add the following modules to your workspace in Azure Machine Learning Studio:</span><span class="sxs-lookup"><span data-stu-id="b1868-177">Add the following modules to your workspace in Azure Machine Learning Studio:</span></span>

* <span data-ttu-id="b1868-178">Adult Census Income Binary Classification dataset</span><span class="sxs-lookup"><span data-stu-id="b1868-178">Adult Census Income Binary Classification dataset</span></span>
* <span data-ttu-id="b1868-179">[Two-Class Logistic Regression][two-class-logistic-regression]</span><span class="sxs-lookup"><span data-stu-id="b1868-179">[Two-Class Logistic Regression][two-class-logistic-regression]</span></span>
* <span data-ttu-id="b1868-180">[Train Model][train-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-180">[Train Model][train-model]</span></span>
* <span data-ttu-id="b1868-181">[Score Model][score-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-181">[Score Model][score-model]</span></span>
* <span data-ttu-id="b1868-182">[Evaluate Model][evaluate-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-182">[Evaluate Model][evaluate-model]</span></span>

<span data-ttu-id="b1868-183">Connect the ports as shown below in Figure 5 and set the Label column of the [Train Model][train-model] module to *income*.</span><span class="sxs-lookup"><span data-stu-id="b1868-183">Connect the ports as shown below in Figure 5 and set the Label column of the [Train Model][train-model] module to *income*.</span></span>

![Evaluating a Binary Classification Model](./media/evaluate-model-performance/5.png)

<span data-ttu-id="b1868-185">Figure 5.</span><span class="sxs-lookup"><span data-stu-id="b1868-185">Figure 5.</span></span> <span data-ttu-id="b1868-186">Evaluating a Binary Classification Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-186">Evaluating a Binary Classification Model.</span></span>

### <a name="inspecting-the-evaluation-results"></a><span data-ttu-id="b1868-187">Inspecting the Evaluation Results</span><span class="sxs-lookup"><span data-stu-id="b1868-187">Inspecting the Evaluation Results</span></span>
<span data-ttu-id="b1868-188">After running the experiment, you can click on the output port of the [Evaluate Model][evaluate-model] module and select *Visualize* to see the evaluation results (Figure 7).</span><span class="sxs-lookup"><span data-stu-id="b1868-188">After running the experiment, you can click on the output port of the [Evaluate Model][evaluate-model] module and select *Visualize* to see the evaluation results (Figure 7).</span></span> <span data-ttu-id="b1868-189">The evaluation metrics available for binary classification models are: *Accuracy*, *Precision*, *Recall*, *F1 Score*, and *AUC*.</span><span class="sxs-lookup"><span data-stu-id="b1868-189">The evaluation metrics available for binary classification models are: *Accuracy*, *Precision*, *Recall*, *F1 Score*, and *AUC*.</span></span> <span data-ttu-id="b1868-190">In addition, the module outputs a confusion matrix showing the number of true positives, false negatives, false positives, and true negatives, as well as *ROC*, *Precision/Recall*, and *Lift* curves.</span><span class="sxs-lookup"><span data-stu-id="b1868-190">In addition, the module outputs a confusion matrix showing the number of true positives, false negatives, false positives, and true negatives, as well as *ROC*, *Precision/Recall*, and *Lift* curves.</span></span>

<span data-ttu-id="b1868-191">Accuracy is simply the proportion of correctly classified instances.</span><span class="sxs-lookup"><span data-stu-id="b1868-191">Accuracy is simply the proportion of correctly classified instances.</span></span> <span data-ttu-id="b1868-192">It is usually the first metric you look at when evaluating a classifier.</span><span class="sxs-lookup"><span data-stu-id="b1868-192">It is usually the first metric you look at when evaluating a classifier.</span></span> <span data-ttu-id="b1868-193">However, when the test data is unbalanced (where most of the instances belong to one of the classes), or you are more interested in the performance on either one of the classes, accuracy doesn’t really capture the effectiveness of a classifier.</span><span class="sxs-lookup"><span data-stu-id="b1868-193">However, when the test data is unbalanced (where most of the instances belong to one of the classes), or you are more interested in the performance on either one of the classes, accuracy doesn’t really capture the effectiveness of a classifier.</span></span> <span data-ttu-id="b1868-194">In the income level classification scenario, assume you are testing on some data where 99% of the instances represent people who earn less than or equal to 50K per year.</span><span class="sxs-lookup"><span data-stu-id="b1868-194">In the income level classification scenario, assume you are testing on some data where 99% of the instances represent people who earn less than or equal to 50K per year.</span></span> <span data-ttu-id="b1868-195">It is possible to achieve a 0.99 accuracy by predicting the class “<=50K” for all instances.</span><span class="sxs-lookup"><span data-stu-id="b1868-195">It is possible to achieve a 0.99 accuracy by predicting the class “<=50K” for all instances.</span></span> <span data-ttu-id="b1868-196">The classifier in this case appears to be doing a good job overall, but in reality, it fails to classify any of the high-income individuals (the 1%) correctly.</span><span class="sxs-lookup"><span data-stu-id="b1868-196">The classifier in this case appears to be doing a good job overall, but in reality, it fails to classify any of the high-income individuals (the 1%) correctly.</span></span>

<span data-ttu-id="b1868-197">For that reason, it is helpful to compute additional metrics that capture more specific aspects of the evaluation.</span><span class="sxs-lookup"><span data-stu-id="b1868-197">For that reason, it is helpful to compute additional metrics that capture more specific aspects of the evaluation.</span></span> <span data-ttu-id="b1868-198">Before going into the details of such metrics, it is important to understand the confusion matrix of a binary classification evaluation.</span><span class="sxs-lookup"><span data-stu-id="b1868-198">Before going into the details of such metrics, it is important to understand the confusion matrix of a binary classification evaluation.</span></span> <span data-ttu-id="b1868-199">The class labels in the training set can take on only 2 possible values, which we usually refer to as positive or negative.</span><span class="sxs-lookup"><span data-stu-id="b1868-199">The class labels in the training set can take on only 2 possible values, which we usually refer to as positive or negative.</span></span> <span data-ttu-id="b1868-200">The positive and negative instances that a classifier predicts correctly are called true positives (TP) and true negatives (TN), respectively.</span><span class="sxs-lookup"><span data-stu-id="b1868-200">The positive and negative instances that a classifier predicts correctly are called true positives (TP) and true negatives (TN), respectively.</span></span> <span data-ttu-id="b1868-201">Similarly, the incorrectly classified instances are called false positives (FP) and false negatives (FN).</span><span class="sxs-lookup"><span data-stu-id="b1868-201">Similarly, the incorrectly classified instances are called false positives (FP) and false negatives (FN).</span></span> <span data-ttu-id="b1868-202">The confusion matrix is simply a table showing the number of instances that fall under each of these 4 categories.</span><span class="sxs-lookup"><span data-stu-id="b1868-202">The confusion matrix is simply a table showing the number of instances that fall under each of these 4 categories.</span></span> <span data-ttu-id="b1868-203">Azure Machine Learning automatically decides which of the two classes in the dataset is the positive class.</span><span class="sxs-lookup"><span data-stu-id="b1868-203">Azure Machine Learning automatically decides which of the two classes in the dataset is the positive class.</span></span> <span data-ttu-id="b1868-204">If the class labels are Boolean or integers, then the ‘true’ or ‘1’ labeled instances are assigned the positive class.</span><span class="sxs-lookup"><span data-stu-id="b1868-204">If the class labels are Boolean or integers, then the ‘true’ or ‘1’ labeled instances are assigned the positive class.</span></span> <span data-ttu-id="b1868-205">If the labels are strings, as in the case of the income dataset, the labels are sorted alphabetically and the first level is chosen to be the negative class while the second level is the positive class.</span><span class="sxs-lookup"><span data-stu-id="b1868-205">If the labels are strings, as in the case of the income dataset, the labels are sorted alphabetically and the first level is chosen to be the negative class while the second level is the positive class.</span></span>

![Binary Classification Confusion Matrix](./media/evaluate-model-performance/6a.png)

<span data-ttu-id="b1868-207">Figure 6.</span><span class="sxs-lookup"><span data-stu-id="b1868-207">Figure 6.</span></span> <span data-ttu-id="b1868-208">Binary Classification Confusion Matrix.</span><span class="sxs-lookup"><span data-stu-id="b1868-208">Binary Classification Confusion Matrix.</span></span>

<span data-ttu-id="b1868-209">Going back to the income classification problem, we would want to ask several evaluation questions that help us understand the performance of the classifier used.</span><span class="sxs-lookup"><span data-stu-id="b1868-209">Going back to the income classification problem, we would want to ask several evaluation questions that help us understand the performance of the classifier used.</span></span> <span data-ttu-id="b1868-210">A very natural question is: ‘Out of the individuals whom the model predicted to be earning >50K (TP+FP), how many were classified correctly (TP)?’</span><span class="sxs-lookup"><span data-stu-id="b1868-210">A very natural question is: ‘Out of the individuals whom the model predicted to be earning >50K (TP+FP), how many were classified correctly (TP)?’</span></span> <span data-ttu-id="b1868-211">This question can be answered by looking at the **Precision** of the model, which is the proportion of positives that are classified correctly: TP/(TP+FP).</span><span class="sxs-lookup"><span data-stu-id="b1868-211">This question can be answered by looking at the **Precision** of the model, which is the proportion of positives that are classified correctly: TP/(TP+FP).</span></span> <span data-ttu-id="b1868-212">Another common question is “Out of all the high earning employees with income >50k (TP+FN), how many did the classifier classify correctly (TP)”.</span><span class="sxs-lookup"><span data-stu-id="b1868-212">Another common question is “Out of all the high earning employees with income >50k (TP+FN), how many did the classifier classify correctly (TP)”.</span></span> <span data-ttu-id="b1868-213">This is actually the **Recall**, or the true positive rate: TP/(TP+FN) of the classifier.</span><span class="sxs-lookup"><span data-stu-id="b1868-213">This is actually the **Recall**, or the true positive rate: TP/(TP+FN) of the classifier.</span></span> <span data-ttu-id="b1868-214">You might notice that there is an obvious trade-off between precision and recall.</span><span class="sxs-lookup"><span data-stu-id="b1868-214">You might notice that there is an obvious trade-off between precision and recall.</span></span> <span data-ttu-id="b1868-215">For example, given a relatively balanced dataset, a classifier that predicts mostly positive instances, would have a high recall, but a rather low precision as many of the negative instances would be misclassified resulting in a large number of false positives.</span><span class="sxs-lookup"><span data-stu-id="b1868-215">For example, given a relatively balanced dataset, a classifier that predicts mostly positive instances, would have a high recall, but a rather low precision as many of the negative instances would be misclassified resulting in a large number of false positives.</span></span> <span data-ttu-id="b1868-216">To see a plot of how these two metrics vary, you can click on the ‘PRECISION/RECALL’ curve in the evaluation result output page (top left part of Figure 7).</span><span class="sxs-lookup"><span data-stu-id="b1868-216">To see a plot of how these two metrics vary, you can click on the ‘PRECISION/RECALL’ curve in the evaluation result output page (top left part of Figure 7).</span></span>

![Binary Classification Evaluation Results](./media/evaluate-model-performance/7.png)

<span data-ttu-id="b1868-218">Figure 7.</span><span class="sxs-lookup"><span data-stu-id="b1868-218">Figure 7.</span></span> <span data-ttu-id="b1868-219">Binary Classification Evaluation Results.</span><span class="sxs-lookup"><span data-stu-id="b1868-219">Binary Classification Evaluation Results.</span></span>

<span data-ttu-id="b1868-220">Another related metric that is often used is the **F1 Score**, which takes both precision and recall into consideration.</span><span class="sxs-lookup"><span data-stu-id="b1868-220">Another related metric that is often used is the **F1 Score**, which takes both precision and recall into consideration.</span></span> <span data-ttu-id="b1868-221">It is the harmonic mean of these 2 metrics and is computed as such: F1 = 2 (precision x recall) / (precision + recall).</span><span class="sxs-lookup"><span data-stu-id="b1868-221">It is the harmonic mean of these 2 metrics and is computed as such: F1 = 2 (precision x recall) / (precision + recall).</span></span> <span data-ttu-id="b1868-222">The F1 score is a good way to summarize the evaluation in a single number, but it’s always a good practice to look at both precision and recall together to better understand how a classifier behaves.</span><span class="sxs-lookup"><span data-stu-id="b1868-222">The F1 score is a good way to summarize the evaluation in a single number, but it’s always a good practice to look at both precision and recall together to better understand how a classifier behaves.</span></span>

<span data-ttu-id="b1868-223">In addition, one can inspect the true positive rate vs. the false positive rate in the **Receiver Operating Characteristic (ROC)** curve and the corresponding **Area Under the Curve (AUC)** value.</span><span class="sxs-lookup"><span data-stu-id="b1868-223">In addition, one can inspect the true positive rate vs. the false positive rate in the **Receiver Operating Characteristic (ROC)** curve and the corresponding **Area Under the Curve (AUC)** value.</span></span> <span data-ttu-id="b1868-224">The closer this curve is to the upper left corner, the better the classifier’s performance is (that is maximizing the true positive rate while minimizing the false positive rate).</span><span class="sxs-lookup"><span data-stu-id="b1868-224">The closer this curve is to the upper left corner, the better the classifier’s performance is (that is maximizing the true positive rate while minimizing the false positive rate).</span></span> <span data-ttu-id="b1868-225">Curves that are close to the diagonal of the plot, result from classifiers that tend to make predictions that are close to random guessing.</span><span class="sxs-lookup"><span data-stu-id="b1868-225">Curves that are close to the diagonal of the plot, result from classifiers that tend to make predictions that are close to random guessing.</span></span>

### <a name="using-cross-validation"></a><span data-ttu-id="b1868-226">Using Cross Validation</span><span class="sxs-lookup"><span data-stu-id="b1868-226">Using Cross Validation</span></span>
<span data-ttu-id="b1868-227">As in the regression example, we can perform cross validation to repeatedly train, score and evaluate different subsets of the data automatically.</span><span class="sxs-lookup"><span data-stu-id="b1868-227">As in the regression example, we can perform cross validation to repeatedly train, score and evaluate different subsets of the data automatically.</span></span> <span data-ttu-id="b1868-228">Similarly, we can use the [Cross-Validate Model][cross-validate-model] module, an untrained logistic regression model, and a dataset.</span><span class="sxs-lookup"><span data-stu-id="b1868-228">Similarly, we can use the [Cross-Validate Model][cross-validate-model] module, an untrained logistic regression model, and a dataset.</span></span> <span data-ttu-id="b1868-229">The label column must be set to *income* in the [Cross-Validate Model][cross-validate-model] module’s properties.</span><span class="sxs-lookup"><span data-stu-id="b1868-229">The label column must be set to *income* in the [Cross-Validate Model][cross-validate-model] module’s properties.</span></span> <span data-ttu-id="b1868-230">After running the experiment and clicking on the right output port of the [Cross-Validate Model][cross-validate-model] module, we can see the binary classification metric values for each fold, in addition to the mean and standard deviation of each.</span><span class="sxs-lookup"><span data-stu-id="b1868-230">After running the experiment and clicking on the right output port of the [Cross-Validate Model][cross-validate-model] module, we can see the binary classification metric values for each fold, in addition to the mean and standard deviation of each.</span></span> 

![Cross-Validating a Binary Classification Model](./media/evaluate-model-performance/8.png)

<span data-ttu-id="b1868-232">Figure 8.</span><span class="sxs-lookup"><span data-stu-id="b1868-232">Figure 8.</span></span> <span data-ttu-id="b1868-233">Cross-Validating a Binary Classification Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-233">Cross-Validating a Binary Classification Model.</span></span>

![Cross-Validation Results of a Binary Classifier](./media/evaluate-model-performance/9.png)

<span data-ttu-id="b1868-235">Figure 9.</span><span class="sxs-lookup"><span data-stu-id="b1868-235">Figure 9.</span></span> <span data-ttu-id="b1868-236">Cross-Validation Results of a Binary Classifier.</span><span class="sxs-lookup"><span data-stu-id="b1868-236">Cross-Validation Results of a Binary Classifier.</span></span>

## <a name="evaluating-a-multiclass-classification-model"></a><span data-ttu-id="b1868-237">Evaluating a Multiclass Classification Model</span><span class="sxs-lookup"><span data-stu-id="b1868-237">Evaluating a Multiclass Classification Model</span></span>
<span data-ttu-id="b1868-238">In this experiment we will use the popular [Iris](http://archive.ics.uci.edu/ml/datasets/Iris "Iris") dataset which contains instances of 3 different types (classes) of the iris plant.</span><span class="sxs-lookup"><span data-stu-id="b1868-238">In this experiment we will use the popular [Iris](http://archive.ics.uci.edu/ml/datasets/Iris "Iris") dataset which contains instances of 3 different types (classes) of the iris plant.</span></span> <span data-ttu-id="b1868-239">There are 4 feature values (sepal length/width and petal length/width) for each instance.</span><span class="sxs-lookup"><span data-stu-id="b1868-239">There are 4 feature values (sepal length/width and petal length/width) for each instance.</span></span> <span data-ttu-id="b1868-240">In the previous experiments we trained and tested the models using the same datasets.</span><span class="sxs-lookup"><span data-stu-id="b1868-240">In the previous experiments we trained and tested the models using the same datasets.</span></span> <span data-ttu-id="b1868-241">Here, we will use the [Split Data][split] module to create 2 subsets of the data, train on the first, and score and evaluate on the second.</span><span class="sxs-lookup"><span data-stu-id="b1868-241">Here, we will use the [Split Data][split] module to create 2 subsets of the data, train on the first, and score and evaluate on the second.</span></span> <span data-ttu-id="b1868-242">The Iris dataset is publicly available on the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.html), and can be downloaded using an [Import Data][import-data] module.</span><span class="sxs-lookup"><span data-stu-id="b1868-242">The Iris dataset is publicly available on the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.html), and can be downloaded using an [Import Data][import-data] module.</span></span>

### <a name="creating-the-experiment"></a><span data-ttu-id="b1868-243">Creating the Experiment</span><span class="sxs-lookup"><span data-stu-id="b1868-243">Creating the Experiment</span></span>
<span data-ttu-id="b1868-244">Add the following modules to your workspace in Azure Machine Learning Studio:</span><span class="sxs-lookup"><span data-stu-id="b1868-244">Add the following modules to your workspace in Azure Machine Learning Studio:</span></span>

* <span data-ttu-id="b1868-245">[Import Data][import-data]</span><span class="sxs-lookup"><span data-stu-id="b1868-245">[Import Data][import-data]</span></span>
* <span data-ttu-id="b1868-246">[Multiclass Decision Forest][multiclass-decision-forest]</span><span class="sxs-lookup"><span data-stu-id="b1868-246">[Multiclass Decision Forest][multiclass-decision-forest]</span></span>
* <span data-ttu-id="b1868-247">[Split Data][split]</span><span class="sxs-lookup"><span data-stu-id="b1868-247">[Split Data][split]</span></span>
* <span data-ttu-id="b1868-248">[Train Model][train-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-248">[Train Model][train-model]</span></span>
* <span data-ttu-id="b1868-249">[Score Model][score-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-249">[Score Model][score-model]</span></span>
* <span data-ttu-id="b1868-250">[Evaluate Model][evaluate-model]</span><span class="sxs-lookup"><span data-stu-id="b1868-250">[Evaluate Model][evaluate-model]</span></span>

<span data-ttu-id="b1868-251">Connect the ports as shown below in Figure 10.</span><span class="sxs-lookup"><span data-stu-id="b1868-251">Connect the ports as shown below in Figure 10.</span></span>

<span data-ttu-id="b1868-252">Set the Label column index of the [Train Model][train-model] module to 5.</span><span class="sxs-lookup"><span data-stu-id="b1868-252">Set the Label column index of the [Train Model][train-model] module to 5.</span></span> <span data-ttu-id="b1868-253">The dataset has no header row but we know that the class labels are in the fifth column.</span><span class="sxs-lookup"><span data-stu-id="b1868-253">The dataset has no header row but we know that the class labels are in the fifth column.</span></span>

<span data-ttu-id="b1868-254">Click on the [Import Data][import-data] module and set the *Data source* property to *Web URL via HTTP*, and the *URL* to http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data.</span><span class="sxs-lookup"><span data-stu-id="b1868-254">Click on the [Import Data][import-data] module and set the *Data source* property to *Web URL via HTTP*, and the *URL* to http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data.</span></span>

<span data-ttu-id="b1868-255">Set the fraction of instances to be used for training in the [Split Data][split] module (0.7 for example).</span><span class="sxs-lookup"><span data-stu-id="b1868-255">Set the fraction of instances to be used for training in the [Split Data][split] module (0.7 for example).</span></span>

![Evaluating a Multiclass Classifier](./media/evaluate-model-performance/10.png)

<span data-ttu-id="b1868-257">Figure 10.</span><span class="sxs-lookup"><span data-stu-id="b1868-257">Figure 10.</span></span> <span data-ttu-id="b1868-258">Evaluating a Multiclass Classifier</span><span class="sxs-lookup"><span data-stu-id="b1868-258">Evaluating a Multiclass Classifier</span></span>

### <a name="inspecting-the-evaluation-results"></a><span data-ttu-id="b1868-259">Inspecting the Evaluation Results</span><span class="sxs-lookup"><span data-stu-id="b1868-259">Inspecting the Evaluation Results</span></span>
<span data-ttu-id="b1868-260">Run the experiment and click on the output port of [Evaluate Model][evaluate-model].</span><span class="sxs-lookup"><span data-stu-id="b1868-260">Run the experiment and click on the output port of [Evaluate Model][evaluate-model].</span></span> <span data-ttu-id="b1868-261">The evaluation results are presented in the form of a confusion matrix, in this case.</span><span class="sxs-lookup"><span data-stu-id="b1868-261">The evaluation results are presented in the form of a confusion matrix, in this case.</span></span> <span data-ttu-id="b1868-262">The matrix shows the actual vs. predicted instances for all 3 classes.</span><span class="sxs-lookup"><span data-stu-id="b1868-262">The matrix shows the actual vs. predicted instances for all 3 classes.</span></span>

![Multiclass Classification Evaluation Results](./media/evaluate-model-performance/11.png)

<span data-ttu-id="b1868-264">Figure 11.</span><span class="sxs-lookup"><span data-stu-id="b1868-264">Figure 11.</span></span> <span data-ttu-id="b1868-265">Multiclass Classification Evaluation Results.</span><span class="sxs-lookup"><span data-stu-id="b1868-265">Multiclass Classification Evaluation Results.</span></span>

### <a name="using-cross-validation"></a><span data-ttu-id="b1868-266">Using Cross Validation</span><span class="sxs-lookup"><span data-stu-id="b1868-266">Using Cross Validation</span></span>
<span data-ttu-id="b1868-267">As mentioned earlier, you can perform repeated training, scoring and evaluations automatically using the [Cross-Validate Model][cross-validate-model] module.</span><span class="sxs-lookup"><span data-stu-id="b1868-267">As mentioned earlier, you can perform repeated training, scoring and evaluations automatically using the [Cross-Validate Model][cross-validate-model] module.</span></span> <span data-ttu-id="b1868-268">You would need a dataset, an untrained model, and a [Cross-Validate Model][cross-validate-model] module (see figure below).</span><span class="sxs-lookup"><span data-stu-id="b1868-268">You would need a dataset, an untrained model, and a [Cross-Validate Model][cross-validate-model] module (see figure below).</span></span> <span data-ttu-id="b1868-269">Again you need to set the label column of the [Cross-Validate Model][cross-validate-model] module (column index 5 in this case).</span><span class="sxs-lookup"><span data-stu-id="b1868-269">Again you need to set the label column of the [Cross-Validate Model][cross-validate-model] module (column index 5 in this case).</span></span> <span data-ttu-id="b1868-270">After running the experiment and clicking the right output port of the [Cross-Validate Model][cross-validate-model], you can inspect the metric values for each fold as well as the mean and standard deviation.</span><span class="sxs-lookup"><span data-stu-id="b1868-270">After running the experiment and clicking the right output port of the [Cross-Validate Model][cross-validate-model], you can inspect the metric values for each fold as well as the mean and standard deviation.</span></span> <span data-ttu-id="b1868-271">The metrics displayed here are the similar to the ones discussed in the binary classification case.</span><span class="sxs-lookup"><span data-stu-id="b1868-271">The metrics displayed here are the similar to the ones discussed in the binary classification case.</span></span> <span data-ttu-id="b1868-272">However, note that in multiclass classification, computing the true positives/negatives and false positives/negatives is done by counting on a per-class basis, as there is no overall positive or negative class.</span><span class="sxs-lookup"><span data-stu-id="b1868-272">However, note that in multiclass classification, computing the true positives/negatives and false positives/negatives is done by counting on a per-class basis, as there is no overall positive or negative class.</span></span> <span data-ttu-id="b1868-273">For example, when computing the precision or recall of the ‘Iris-setosa’ class, it is assumed that this is the positive class and all others as negative.</span><span class="sxs-lookup"><span data-stu-id="b1868-273">For example, when computing the precision or recall of the ‘Iris-setosa’ class, it is assumed that this is the positive class and all others as negative.</span></span>

![Cross-Validating a Multiclass Classification Model](./media/evaluate-model-performance/12.png)

<span data-ttu-id="b1868-275">Figure 12.</span><span class="sxs-lookup"><span data-stu-id="b1868-275">Figure 12.</span></span> <span data-ttu-id="b1868-276">Cross-Validating a Multiclass Classification Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-276">Cross-Validating a Multiclass Classification Model.</span></span>

![Cross-Validation Results of a Multiclass Classification Model](./media/evaluate-model-performance/13.png)

<span data-ttu-id="b1868-278">Figure 13.</span><span class="sxs-lookup"><span data-stu-id="b1868-278">Figure 13.</span></span> <span data-ttu-id="b1868-279">Cross-Validation Results of a Multiclass Classification Model.</span><span class="sxs-lookup"><span data-stu-id="b1868-279">Cross-Validation Results of a Multiclass Classification Model.</span></span>

<!-- Module References -->
[cross-validate-model]: https://msdn.microsoft.com/library/azure/75fb875d-6b86-4d46-8bcc-74261ade5826/
[evaluate-model]: https://msdn.microsoft.com/library/azure/927d65ac-3b50-4694-9903-20f6c1672089/
[linear-regression]: https://msdn.microsoft.com/library/azure/31960a6f-789b-4cf7-88d6-2e1152c0bd1a/
[multiclass-decision-forest]: https://msdn.microsoft.com/library/azure/5e70108d-2e44-45d9-86e8-94f37c68fe86/
[import-data]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/
[score-model]: https://msdn.microsoft.com/library/azure/401b4f92-e724-4d5a-be81-d5b0ff9bdb33/
[split]: https://msdn.microsoft.com/library/azure/70530644-c97a-4ab6-85f7-88bf30a8be5f/
[train-model]: https://msdn.microsoft.com/library/azure/5cc7053e-aa30-450d-96c0-dae4be720977/
[two-class-logistic-regression]: https://msdn.microsoft.com/library/azure/b0fd7660-eeed-43c5-9487-20d9cc79ed5d/

