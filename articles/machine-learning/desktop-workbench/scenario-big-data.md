---
title: Server workload forecasting on terabytes of data - Azure | Microsoft Docs
description: How to train a machine learning model on big data by using Azure Machine Learning Workbench.
services: machine-learning
documentationcenter: ''
author: daden
manager: mithal
editor: daden
ms.assetid: ''
ms.reviewer: garyericson, jasonwhowell, mldocs
ms.service: machine-learning
ms.component: core
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 09/15/2017
ms.author: daden
ms.openlocfilehash: c1587beb3acb21fe9b2f6fcb6a9e91f32f26923e
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44870156"
---
# <a name="server-workload-forecasting-on-terabytes-of-data"></a><span data-ttu-id="d4c3c-103">Server workload forecasting on terabytes of data</span><span class="sxs-lookup"><span data-stu-id="d4c3c-103">Server workload forecasting on terabytes of data</span></span>

<span data-ttu-id="d4c3c-104">This article covers how data scientists can use Azure Machine Learning Workbench to develop solutions that require the use of big data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-104">This article covers how data scientists can use Azure Machine Learning Workbench to develop solutions that require the use of big data.</span></span> <span data-ttu-id="d4c3c-105">You can start from a sample of a large data set, iterate through data preparation, feature engineering, and machine learning, and then extend the process to the entire large data set.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-105">You can start from a sample of a large data set, iterate through data preparation, feature engineering, and machine learning, and then extend the process to the entire large data set.</span></span> 

<span data-ttu-id="d4c3c-106">You'll learn about the following key capabilities of Machine Learning Workbench:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-106">You'll learn about the following key capabilities of Machine Learning Workbench:</span></span>
* <span data-ttu-id="d4c3c-107">Easy switching between compute targets.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-107">Easy switching between compute targets.</span></span> <span data-ttu-id="d4c3c-108">You can set up different compute targets and use them in experimentation.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-108">You can set up different compute targets and use them in experimentation.</span></span> <span data-ttu-id="d4c3c-109">In this example, you use an Ubuntu DSVM and an Azure HDInsight cluster as the compute targets.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-109">In this example, you use an Ubuntu DSVM and an Azure HDInsight cluster as the compute targets.</span></span> <span data-ttu-id="d4c3c-110">You can also configure the compute targets, depending on the availability of resources.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-110">You can also configure the compute targets, depending on the availability of resources.</span></span> <span data-ttu-id="d4c3c-111">In particular, after scaling out the Spark cluster with more worker nodes, you can use the resources through Machine Learning Workbench to speed up experiment runs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-111">In particular, after scaling out the Spark cluster with more worker nodes, you can use the resources through Machine Learning Workbench to speed up experiment runs.</span></span>
* <span data-ttu-id="d4c3c-112">Run history tracking.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-112">Run history tracking.</span></span> <span data-ttu-id="d4c3c-113">You can use Machine Learning Workbench to track the performance of machine learning models and other metrics of interest.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-113">You can use Machine Learning Workbench to track the performance of machine learning models and other metrics of interest.</span></span>
* <span data-ttu-id="d4c3c-114">Operationalization of the machine learning model.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-114">Operationalization of the machine learning model.</span></span> <span data-ttu-id="d4c3c-115">You can use the built-in tools within Machine Learning Workbench to deploy the trained machine learning model as a web service on Azure Container Service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-115">You can use the built-in tools within Machine Learning Workbench to deploy the trained machine learning model as a web service on Azure Container Service.</span></span> <span data-ttu-id="d4c3c-116">You can also use the web service to get mini-batch predictions through REST API calls.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-116">You can also use the web service to get mini-batch predictions through REST API calls.</span></span> 
* <span data-ttu-id="d4c3c-117">Support for terabytes data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-117">Support for terabytes data.</span></span>

> [!NOTE]
> <span data-ttu-id="d4c3c-118">For code samples and other materials related to this example, see [GitHub](https://github.com/Azure/MachineLearningSamples-BigData).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-118">For code samples and other materials related to this example, see [GitHub](https://github.com/Azure/MachineLearningSamples-BigData).</span></span>
> 

## <a name="use-case-overview"></a><span data-ttu-id="d4c3c-119">Use case overview</span><span class="sxs-lookup"><span data-stu-id="d4c3c-119">Use case overview</span></span>

<span data-ttu-id="d4c3c-120">Forecasting the workload on servers is a common business need for technology companies that manage their own infrastructure.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-120">Forecasting the workload on servers is a common business need for technology companies that manage their own infrastructure.</span></span> <span data-ttu-id="d4c3c-121">To reduce infrastructure cost, services running on under-used servers should be grouped together to run on a smaller number of machines.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-121">To reduce infrastructure cost, services running on under-used servers should be grouped together to run on a smaller number of machines.</span></span> <span data-ttu-id="d4c3c-122">Services running on overused servers should be given more machines to run.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-122">Services running on overused servers should be given more machines to run.</span></span> 

<span data-ttu-id="d4c3c-123">In this scenario, you focus on workload prediction for each machine (or server).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-123">In this scenario, you focus on workload prediction for each machine (or server).</span></span> <span data-ttu-id="d4c3c-124">In particular, you use the session data on each server to predict the workload class of the server in future.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-124">In particular, you use the session data on each server to predict the workload class of the server in future.</span></span> <span data-ttu-id="d4c3c-125">You classify the load of each server into low, medium, and high classes by using the Random Forest Classifier in [Apache Spark ML](https://spark.apache.org/docs/2.1.1/ml-guide.html).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-125">You classify the load of each server into low, medium, and high classes by using the Random Forest Classifier in [Apache Spark ML](https://spark.apache.org/docs/2.1.1/ml-guide.html).</span></span> <span data-ttu-id="d4c3c-126">The machine learning techniques and workflow in this example can be easily extended to other similar problems.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-126">The machine learning techniques and workflow in this example can be easily extended to other similar problems.</span></span> 


## <a name="prerequisites"></a><span data-ttu-id="d4c3c-127">Prerequisites</span><span class="sxs-lookup"><span data-stu-id="d4c3c-127">Prerequisites</span></span>

<span data-ttu-id="d4c3c-128">The prerequisites to run this example are as follows:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-128">The prerequisites to run this example are as follows:</span></span>

* <span data-ttu-id="d4c3c-129">An [Azure account](https://azure.microsoft.com/free/) (free trials are available).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-129">An [Azure account](https://azure.microsoft.com/free/) (free trials are available).</span></span>
* <span data-ttu-id="d4c3c-130">An installed copy of [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-130">An installed copy of [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md).</span></span> <span data-ttu-id="d4c3c-131">To install the program and create a workspace, see the [quickstart installation guide](../service/quickstart-installation.md).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-131">To install the program and create a workspace, see the [quickstart installation guide](../service/quickstart-installation.md).</span></span> <span data-ttu-id="d4c3c-132">If you have multiple subscriptions, you can [set the desired subscription to be the current active subscription](https://docs.microsoft.com/cli/azure/account?view=azure-cli-latest#az-account-set).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-132">If you have multiple subscriptions, you can [set the desired subscription to be the current active subscription](https://docs.microsoft.com/cli/azure/account?view=azure-cli-latest#az-account-set).</span></span>
* <span data-ttu-id="d4c3c-133">Windows 10 (the instructions in this example are generally the same for macOS systems).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-133">Windows 10 (the instructions in this example are generally the same for macOS systems).</span></span>
* <span data-ttu-id="d4c3c-134">A Data Science Virtual Machine (DSVM) for Linux (Ubuntu), preferably in East US region where the data locates.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-134">A Data Science Virtual Machine (DSVM) for Linux (Ubuntu), preferably in East US region where the data locates.</span></span> <span data-ttu-id="d4c3c-135">You can provision an Ubuntu DSVM by following [these instructions](https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/dsvm-ubuntu-intro).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-135">You can provision an Ubuntu DSVM by following [these instructions](https://docs.microsoft.com/azure/machine-learning/data-science-virtual-machine/dsvm-ubuntu-intro).</span></span> <span data-ttu-id="d4c3c-136">You can also see [this quickstart](https://ms.portal.azure.com/#create/microsoft-ads.linux-data-science-vm-ubuntulinuxdsvmubuntu).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-136">You can also see [this quickstart](https://ms.portal.azure.com/#create/microsoft-ads.linux-data-science-vm-ubuntulinuxdsvmubuntu).</span></span> <span data-ttu-id="d4c3c-137">We recommend using a virtual machine with at least 8 cores and 32 GB of memory.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-137">We recommend using a virtual machine with at least 8 cores and 32 GB of memory.</span></span> 

<span data-ttu-id="d4c3c-138">Follow the [instruction](../service/known-issues-and-troubleshooting-guide.md#remove-vm-execution-error-no-tty-present) to enable password-less sudoer access on the VM for AML Workbench.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-138">Follow the [instruction](../service/known-issues-and-troubleshooting-guide.md#remove-vm-execution-error-no-tty-present) to enable password-less sudoer access on the VM for AML Workbench.</span></span>  <span data-ttu-id="d4c3c-139">You can choose to use [SSH key-based authentication for creating and using the VM in AML Workbench](experimentation-service-configuration.md#using-ssh-key-based-authentication-for-creating-and-using-compute-targets).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-139">You can choose to use [SSH key-based authentication for creating and using the VM in AML Workbench](experimentation-service-configuration.md#using-ssh-key-based-authentication-for-creating-and-using-compute-targets).</span></span> <span data-ttu-id="d4c3c-140">In this example, we use password to access the VM.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-140">In this example, we use password to access the VM.</span></span>  <span data-ttu-id="d4c3c-141">Save the following table with the DSVM info for later steps:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-141">Save the following table with the DSVM info for later steps:</span></span>

 <span data-ttu-id="d4c3c-142">Field name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-142">Field name</span></span>| <span data-ttu-id="d4c3c-143">Value</span><span class="sxs-lookup"><span data-stu-id="d4c3c-143">Value</span></span> |  
 |------------|------|
<span data-ttu-id="d4c3c-144">DSVM IP address</span><span class="sxs-lookup"><span data-stu-id="d4c3c-144">DSVM IP address</span></span> | <span data-ttu-id="d4c3c-145">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-145">xxx</span></span>|
 <span data-ttu-id="d4c3c-146">User name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-146">User name</span></span>  | <span data-ttu-id="d4c3c-147">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-147">xxx</span></span>|
 <span data-ttu-id="d4c3c-148">Password</span><span class="sxs-lookup"><span data-stu-id="d4c3c-148">Password</span></span>   | <span data-ttu-id="d4c3c-149">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-149">xxx</span></span>|


 <span data-ttu-id="d4c3c-150">You can choose to use any VM with [Docker Engine](https://docs.docker.com/engine/) installed.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-150">You can choose to use any VM with [Docker Engine](https://docs.docker.com/engine/) installed.</span></span>

* <span data-ttu-id="d4c3c-151">An HDInsight Spark Cluster, with Hortonworks Data Platform version 3.6 and Spark version 2.1.x, preferably in East US region where the data locates.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-151">An HDInsight Spark Cluster, with Hortonworks Data Platform version 3.6 and Spark version 2.1.x, preferably in East US region where the data locates.</span></span> <span data-ttu-id="d4c3c-152">Visit [Create an Apache Spark cluster in Azure HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-provision-linux-clusters) for details about how to create HDInsight clusters.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-152">Visit [Create an Apache Spark cluster in Azure HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-provision-linux-clusters) for details about how to create HDInsight clusters.</span></span> <span data-ttu-id="d4c3c-153">We recommend using a three-worker cluster, with each worker having 16 cores and 112 GB of memory.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-153">We recommend using a three-worker cluster, with each worker having 16 cores and 112 GB of memory.</span></span> <span data-ttu-id="d4c3c-154">Or you can just choose VM type `D12 V2` for head node, and `D14 V2` for the worker node.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-154">Or you can just choose VM type `D12 V2` for head node, and `D14 V2` for the worker node.</span></span> <span data-ttu-id="d4c3c-155">The deployment of the cluster takes about 20 minutes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-155">The deployment of the cluster takes about 20 minutes.</span></span> <span data-ttu-id="d4c3c-156">You need the cluster name, SSH user name, and password to try out this example.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-156">You need the cluster name, SSH user name, and password to try out this example.</span></span> <span data-ttu-id="d4c3c-157">Save the following table with the Azure HDInsight cluster info for later steps:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-157">Save the following table with the Azure HDInsight cluster info for later steps:</span></span>

 <span data-ttu-id="d4c3c-158">Field name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-158">Field name</span></span>| <span data-ttu-id="d4c3c-159">Value</span><span class="sxs-lookup"><span data-stu-id="d4c3c-159">Value</span></span> |  
 |------------|------|
 <span data-ttu-id="d4c3c-160">Cluster name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-160">Cluster name</span></span>| <span data-ttu-id="d4c3c-161">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-161">xxx</span></span>|
 <span data-ttu-id="d4c3c-162">User name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-162">User name</span></span>  | <span data-ttu-id="d4c3c-163">xxx (sshuser by default)</span><span class="sxs-lookup"><span data-stu-id="d4c3c-163">xxx (sshuser by default)</span></span>|
 <span data-ttu-id="d4c3c-164">Password</span><span class="sxs-lookup"><span data-stu-id="d4c3c-164">Password</span></span>   | <span data-ttu-id="d4c3c-165">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-165">xxx</span></span>|


* <span data-ttu-id="d4c3c-166">An Azure Storage account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-166">An Azure Storage account.</span></span> <span data-ttu-id="d4c3c-167">You can follow [these instructions](https://docs.microsoft.com/azure/storage/common/storage-create-storage-account) to create one.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-167">You can follow [these instructions](https://docs.microsoft.com/azure/storage/common/storage-create-storage-account) to create one.</span></span> <span data-ttu-id="d4c3c-168">Also, create two private blob containers with the names `fullmodel` and `onemonthmodel` in this storage account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-168">Also, create two private blob containers with the names `fullmodel` and `onemonthmodel` in this storage account.</span></span> <span data-ttu-id="d4c3c-169">The storage account is used to save intermediate compute results and machine learning models.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-169">The storage account is used to save intermediate compute results and machine learning models.</span></span> <span data-ttu-id="d4c3c-170">You need the storage account name and access key to try out this example.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-170">You need the storage account name and access key to try out this example.</span></span> <span data-ttu-id="d4c3c-171">Save the following table with the Azure storage account info for later steps:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-171">Save the following table with the Azure storage account info for later steps:</span></span>

 <span data-ttu-id="d4c3c-172">Field name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-172">Field name</span></span>| <span data-ttu-id="d4c3c-173">Value</span><span class="sxs-lookup"><span data-stu-id="d4c3c-173">Value</span></span> |  
 |------------|------|
 <span data-ttu-id="d4c3c-174">Storage account name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-174">Storage account name</span></span>| <span data-ttu-id="d4c3c-175">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-175">xxx</span></span>|
 <span data-ttu-id="d4c3c-176">Access key</span><span class="sxs-lookup"><span data-stu-id="d4c3c-176">Access key</span></span>  | <span data-ttu-id="d4c3c-177">xxx</span><span class="sxs-lookup"><span data-stu-id="d4c3c-177">xxx</span></span>|


<span data-ttu-id="d4c3c-178">The Ubuntu DSVM and the Azure HDInsight cluster created in the prerequisite list are compute targets.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-178">The Ubuntu DSVM and the Azure HDInsight cluster created in the prerequisite list are compute targets.</span></span> <span data-ttu-id="d4c3c-179">Compute targets are the compute resource in the context of Machine Learning Workbench, which might be different from the computer where Workbench runs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-179">Compute targets are the compute resource in the context of Machine Learning Workbench, which might be different from the computer where Workbench runs.</span></span>   

## <a name="create-a-new-workbench-project"></a><span data-ttu-id="d4c3c-180">Create a new Workbench project</span><span class="sxs-lookup"><span data-stu-id="d4c3c-180">Create a new Workbench project</span></span>

<span data-ttu-id="d4c3c-181">Create a new project by using this example as a template:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-181">Create a new project by using this example as a template:</span></span>
1.  <span data-ttu-id="d4c3c-182">Open Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-182">Open Machine Learning Workbench.</span></span>
2.  <span data-ttu-id="d4c3c-183">On the **Projects** page, select the **+** sign, and select **New Project**.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-183">On the **Projects** page, select the **+** sign, and select **New Project**.</span></span>
3.  <span data-ttu-id="d4c3c-184">In the **Create New Project** pane, fill in the information for your new project.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-184">In the **Create New Project** pane, fill in the information for your new project.</span></span>
4.  <span data-ttu-id="d4c3c-185">In the **Search Project Templates** search box, type **Workload Forecasting on Terabytes Data**, and select the template.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-185">In the **Search Project Templates** search box, type **Workload Forecasting on Terabytes Data**, and select the template.</span></span>
5.  <span data-ttu-id="d4c3c-186">Select **Create**.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-186">Select **Create**.</span></span>

<span data-ttu-id="d4c3c-187">You can create a Workbench project with a pre-created git repository by following [this instruction](./tutorial-classifying-iris-part-1.md).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-187">You can create a Workbench project with a pre-created git repository by following [this instruction](./tutorial-classifying-iris-part-1.md).</span></span>  
<span data-ttu-id="d4c3c-188">Run `git status` to inspect the status of the files for version tracking.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-188">Run `git status` to inspect the status of the files for version tracking.</span></span>

## <a name="data-description"></a><span data-ttu-id="d4c3c-189">Data description</span><span class="sxs-lookup"><span data-stu-id="d4c3c-189">Data description</span></span>

<span data-ttu-id="d4c3c-190">The data used in this example is synthesized server workload data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-190">The data used in this example is synthesized server workload data.</span></span> <span data-ttu-id="d4c3c-191">It is hosted in an Azure Blob storage account that's publically accessible in East US region.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-191">It is hosted in an Azure Blob storage account that's publically accessible in East US region.</span></span> <span data-ttu-id="d4c3c-192">The specific storage account info can be found in the `dataFile` field of [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json) in the format of "wasb://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>".</span><span class="sxs-lookup"><span data-stu-id="d4c3c-192">The specific storage account info can be found in the `dataFile` field of [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json) in the format of "wasb://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>".</span></span> <span data-ttu-id="d4c3c-193">You can use the data directly from the Blob storage.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-193">You can use the data directly from the Blob storage.</span></span> <span data-ttu-id="d4c3c-194">If the storage is used by many users simultaneously, you can use [azcopy](https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-linux) to download the data into your own storage for better experimentation experience.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-194">If the storage is used by many users simultaneously, you can use [azcopy](https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-linux) to download the data into your own storage for better experimentation experience.</span></span> 

<span data-ttu-id="d4c3c-195">The total data size is approximately 1 TB.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-195">The total data size is approximately 1 TB.</span></span> <span data-ttu-id="d4c3c-196">Each file is about 1-3 GB, and is in CSV file format, without header.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-196">Each file is about 1-3 GB, and is in CSV file format, without header.</span></span> <span data-ttu-id="d4c3c-197">Each row of data represents the load of a transaction on a particular server.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-197">Each row of data represents the load of a transaction on a particular server.</span></span> <span data-ttu-id="d4c3c-198">The detailed information of the data schema is as follows:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-198">The detailed information of the data schema is as follows:</span></span>

<span data-ttu-id="d4c3c-199">Column number</span><span class="sxs-lookup"><span data-stu-id="d4c3c-199">Column number</span></span> | <span data-ttu-id="d4c3c-200">Field name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-200">Field name</span></span>| <span data-ttu-id="d4c3c-201">Type</span><span class="sxs-lookup"><span data-stu-id="d4c3c-201">Type</span></span> | <span data-ttu-id="d4c3c-202">Description</span><span class="sxs-lookup"><span data-stu-id="d4c3c-202">Description</span></span> |  
|------------|------|-------------|---------------|
<span data-ttu-id="d4c3c-203">1</span><span class="sxs-lookup"><span data-stu-id="d4c3c-203">1</span></span>  | `SessionStart` | <span data-ttu-id="d4c3c-204">Datetime</span><span class="sxs-lookup"><span data-stu-id="d4c3c-204">Datetime</span></span> |    <span data-ttu-id="d4c3c-205">Session start time</span><span class="sxs-lookup"><span data-stu-id="d4c3c-205">Session start time</span></span>
<span data-ttu-id="d4c3c-206">2</span><span class="sxs-lookup"><span data-stu-id="d4c3c-206">2</span></span>  |`SessionEnd`    | <span data-ttu-id="d4c3c-207">Datetime</span><span class="sxs-lookup"><span data-stu-id="d4c3c-207">Datetime</span></span> | <span data-ttu-id="d4c3c-208">Session end time</span><span class="sxs-lookup"><span data-stu-id="d4c3c-208">Session end time</span></span>
<span data-ttu-id="d4c3c-209">3</span><span class="sxs-lookup"><span data-stu-id="d4c3c-209">3</span></span> |`ConcurrentConnectionCounts` | <span data-ttu-id="d4c3c-210">Integer</span><span class="sxs-lookup"><span data-stu-id="d4c3c-210">Integer</span></span> | <span data-ttu-id="d4c3c-211">Number of concurrent connections</span><span class="sxs-lookup"><span data-stu-id="d4c3c-211">Number of concurrent connections</span></span>
<span data-ttu-id="d4c3c-212">4</span><span class="sxs-lookup"><span data-stu-id="d4c3c-212">4</span></span> | `MbytesTransferred` | <span data-ttu-id="d4c3c-213">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-213">Double</span></span> | <span data-ttu-id="d4c3c-214">Normalized data transferred in megabytes</span><span class="sxs-lookup"><span data-stu-id="d4c3c-214">Normalized data transferred in megabytes</span></span>
<span data-ttu-id="d4c3c-215">5</span><span class="sxs-lookup"><span data-stu-id="d4c3c-215">5</span></span> | `ServiceGrade` | <span data-ttu-id="d4c3c-216">Integer</span><span class="sxs-lookup"><span data-stu-id="d4c3c-216">Integer</span></span> |  <span data-ttu-id="d4c3c-217">Service grade for the session</span><span class="sxs-lookup"><span data-stu-id="d4c3c-217">Service grade for the session</span></span>
<span data-ttu-id="d4c3c-218">6</span><span class="sxs-lookup"><span data-stu-id="d4c3c-218">6</span></span> | `HTTP1` | <span data-ttu-id="d4c3c-219">Integer</span><span class="sxs-lookup"><span data-stu-id="d4c3c-219">Integer</span></span>|  <span data-ttu-id="d4c3c-220">Session uses HTTP1 or HTTP2</span><span class="sxs-lookup"><span data-stu-id="d4c3c-220">Session uses HTTP1 or HTTP2</span></span>
<span data-ttu-id="d4c3c-221">7</span><span class="sxs-lookup"><span data-stu-id="d4c3c-221">7</span></span> |`ServerType` | <span data-ttu-id="d4c3c-222">Integer</span><span class="sxs-lookup"><span data-stu-id="d4c3c-222">Integer</span></span>   |<span data-ttu-id="d4c3c-223">Server type</span><span class="sxs-lookup"><span data-stu-id="d4c3c-223">Server type</span></span>
<span data-ttu-id="d4c3c-224">8</span><span class="sxs-lookup"><span data-stu-id="d4c3c-224">8</span></span> |`SubService_1_Load` | <span data-ttu-id="d4c3c-225">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-225">Double</span></span> |   <span data-ttu-id="d4c3c-226">Subservice 1 load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-226">Subservice 1 load</span></span>
<span data-ttu-id="d4c3c-227">9</span><span class="sxs-lookup"><span data-stu-id="d4c3c-227">9</span></span> | `SubService_2_Load` | <span data-ttu-id="d4c3c-228">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-228">Double</span></span> |  <span data-ttu-id="d4c3c-229">Subservice 2 load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-229">Subservice 2 load</span></span>
<span data-ttu-id="d4c3c-230">10</span><span class="sxs-lookup"><span data-stu-id="d4c3c-230">10</span></span> | `SubService_3_Load` | <span data-ttu-id="d4c3c-231">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-231">Double</span></span> |     <span data-ttu-id="d4c3c-232">Subservice 3 load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-232">Subservice 3 load</span></span>
<span data-ttu-id="d4c3c-233">11</span><span class="sxs-lookup"><span data-stu-id="d4c3c-233">11</span></span> |`SubService_4_Load` | <span data-ttu-id="d4c3c-234">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-234">Double</span></span> |  <span data-ttu-id="d4c3c-235">Subservice 4 load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-235">Subservice 4 load</span></span>
<span data-ttu-id="d4c3c-236">12</span><span class="sxs-lookup"><span data-stu-id="d4c3c-236">12</span></span> | `SubService_5_Load`| <span data-ttu-id="d4c3c-237">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-237">Double</span></span> |      <span data-ttu-id="d4c3c-238">Subservice 5 load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-238">Subservice 5 load</span></span>
<span data-ttu-id="d4c3c-239">13</span><span class="sxs-lookup"><span data-stu-id="d4c3c-239">13</span></span> |`SecureBytes_Load`  | <span data-ttu-id="d4c3c-240">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-240">Double</span></span> | <span data-ttu-id="d4c3c-241">Secure bytes load</span><span class="sxs-lookup"><span data-stu-id="d4c3c-241">Secure bytes load</span></span>
<span data-ttu-id="d4c3c-242">14</span><span class="sxs-lookup"><span data-stu-id="d4c3c-242">14</span></span> |`TotalLoad` | <span data-ttu-id="d4c3c-243">Double</span><span class="sxs-lookup"><span data-stu-id="d4c3c-243">Double</span></span> | <span data-ttu-id="d4c3c-244">Total load on server</span><span class="sxs-lookup"><span data-stu-id="d4c3c-244">Total load on server</span></span>
<span data-ttu-id="d4c3c-245">15</span><span class="sxs-lookup"><span data-stu-id="d4c3c-245">15</span></span> |`ClientIP` | <span data-ttu-id="d4c3c-246">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-246">String</span></span>|    <span data-ttu-id="d4c3c-247">Client IP address</span><span class="sxs-lookup"><span data-stu-id="d4c3c-247">Client IP address</span></span>
<span data-ttu-id="d4c3c-248">16</span><span class="sxs-lookup"><span data-stu-id="d4c3c-248">16</span></span> |`ServerIP` | <span data-ttu-id="d4c3c-249">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-249">String</span></span>|    <span data-ttu-id="d4c3c-250">Server IP address</span><span class="sxs-lookup"><span data-stu-id="d4c3c-250">Server IP address</span></span>



<span data-ttu-id="d4c3c-251">Note that the expected data types are listed in the preceding table.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-251">Note that the expected data types are listed in the preceding table.</span></span> <span data-ttu-id="d4c3c-252">Due to missing values and dirty-data problems, there is no guarantee that the data types actually are as expected.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-252">Due to missing values and dirty-data problems, there is no guarantee that the data types actually are as expected.</span></span> <span data-ttu-id="d4c3c-253">Data processing should take this into consideration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-253">Data processing should take this into consideration.</span></span> 


## <a name="scenario-structure"></a><span data-ttu-id="d4c3c-254">Scenario structure</span><span class="sxs-lookup"><span data-stu-id="d4c3c-254">Scenario structure</span></span>

<span data-ttu-id="d4c3c-255">The files in this example are organized as follows.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-255">The files in this example are organized as follows.</span></span>

| <span data-ttu-id="d4c3c-256">File name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-256">File name</span></span> | <span data-ttu-id="d4c3c-257">Type</span><span class="sxs-lookup"><span data-stu-id="d4c3c-257">Type</span></span> | <span data-ttu-id="d4c3c-258">Description</span><span class="sxs-lookup"><span data-stu-id="d4c3c-258">Description</span></span> |
|-----------|------|-------------|
| `Code` | <span data-ttu-id="d4c3c-259">Folder</span><span class="sxs-lookup"><span data-stu-id="d4c3c-259">Folder</span></span> | <span data-ttu-id="d4c3c-260">The  folder contains all the code in the example.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-260">The  folder contains all the code in the example.</span></span> |
| `Config` | <span data-ttu-id="d4c3c-261">Folder</span><span class="sxs-lookup"><span data-stu-id="d4c3c-261">Folder</span></span> | <span data-ttu-id="d4c3c-262">The  folder contains the configuration files.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-262">The  folder contains the configuration files.</span></span> |
| `Image` | <span data-ttu-id="d4c3c-263">Folder</span><span class="sxs-lookup"><span data-stu-id="d4c3c-263">Folder</span></span> | <span data-ttu-id="d4c3c-264">The folder used to save images for the README file.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-264">The folder used to save images for the README file.</span></span> |
| `Model` | <span data-ttu-id="d4c3c-265">Folder</span><span class="sxs-lookup"><span data-stu-id="d4c3c-265">Folder</span></span> | <span data-ttu-id="d4c3c-266">The folder used to save model files downloaded from Blob storage.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-266">The folder used to save model files downloaded from Blob storage.</span></span> |
| `Code/etl.py` | <span data-ttu-id="d4c3c-267">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-267">Python file</span></span> | <span data-ttu-id="d4c3c-268">The Python file used for data preparation and feature engineering.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-268">The Python file used for data preparation and feature engineering.</span></span> |
| `Code/train.py` | <span data-ttu-id="d4c3c-269">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-269">Python file</span></span> | <span data-ttu-id="d4c3c-270">The Python file used to train a three-class multi-classfication model.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-270">The Python file used to train a three-class multi-classfication model.</span></span>  |
| `Code/webservice.py` | <span data-ttu-id="d4c3c-271">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-271">Python file</span></span> | <span data-ttu-id="d4c3c-272">The Python file used for operationalization.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-272">The Python file used for operationalization.</span></span>  |
| `Code/scoring_webservice.py` | <span data-ttu-id="d4c3c-273">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-273">Python file</span></span> |  <span data-ttu-id="d4c3c-274">The Python file used for data transformation and calling the web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-274">The Python file used for data transformation and calling the web service.</span></span> |
| `Code/O16Npreprocessing.py` | <span data-ttu-id="d4c3c-275">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-275">Python file</span></span> | <span data-ttu-id="d4c3c-276">The Python file used to preprocess the data for scoring_webservice.py.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-276">The Python file used to preprocess the data for scoring_webservice.py.</span></span>  |
| `Code/util.py` | <span data-ttu-id="d4c3c-277">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-277">Python file</span></span> | <span data-ttu-id="d4c3c-278">The Python file that contains code for reading and writing Azure blobs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-278">The Python file that contains code for reading and writing Azure blobs.</span></span>  
| `Config/storageconfig.json` | <span data-ttu-id="d4c3c-279">JSON file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-279">JSON file</span></span> | <span data-ttu-id="d4c3c-280">The configuration file for the Azure blob container that stores the intermediate results and model for processing and training on one-month data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-280">The configuration file for the Azure blob container that stores the intermediate results and model for processing and training on one-month data.</span></span> |
| `Config/fulldata_storageconfig.json` | <span data-ttu-id="d4c3c-281">Json file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-281">Json file</span></span> | <span data-ttu-id="d4c3c-282">The configuration file for the Azure blob container that stores the intermediate results and model for processing and training on a full data set.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-282">The configuration file for the Azure blob container that stores the intermediate results and model for processing and training on a full data set.</span></span>|
| `Config/webservice.json` | <span data-ttu-id="d4c3c-283">JSON file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-283">JSON file</span></span> | <span data-ttu-id="d4c3c-284">The configuration file for scoring_webservice.py.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-284">The configuration file for scoring_webservice.py.</span></span>|
| `Config/conda_dependencies.yml` | <span data-ttu-id="d4c3c-285">YAML file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-285">YAML file</span></span> | <span data-ttu-id="d4c3c-286">The Conda dependency file.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-286">The Conda dependency file.</span></span> |
| `Config/conda_dependencies_webservice.yml` | <span data-ttu-id="d4c3c-287">YAML file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-287">YAML file</span></span> | <span data-ttu-id="d4c3c-288">The Conda dependency file for the web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-288">The Conda dependency file for the web service.</span></span>|
| `Config/dsvm_spark_dependencies.yml` | <span data-ttu-id="d4c3c-289">YAML file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-289">YAML file</span></span> | <span data-ttu-id="d4c3c-290">The Spark dependency file for Ubuntu DSVM.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-290">The Spark dependency file for Ubuntu DSVM.</span></span> |
| `Config/hdi_spark_dependencies.yml` | <span data-ttu-id="d4c3c-291">YAML file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-291">YAML file</span></span> | <span data-ttu-id="d4c3c-292">The Spark dependency file for the HDInsight Spark cluster.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-292">The Spark dependency file for the HDInsight Spark cluster.</span></span> |
| `README.md` | <span data-ttu-id="d4c3c-293">Markdown file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-293">Markdown file</span></span> | <span data-ttu-id="d4c3c-294">The README markdown file.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-294">The README markdown file.</span></span> |
| `Code/download_model.py` | <span data-ttu-id="d4c3c-295">Python file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-295">Python file</span></span> | <span data-ttu-id="d4c3c-296">The Python file used to download the  model files from the Azure blob to a local disk.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-296">The Python file used to download the  model files from the Azure blob to a local disk.</span></span> |
| `Docs/DownloadModelsFromBlob.md` | <span data-ttu-id="d4c3c-297">Markdown file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-297">Markdown file</span></span> | <span data-ttu-id="d4c3c-298">The markdown file that contains instructions for how to run `Code/download_model.py`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-298">The markdown file that contains instructions for how to run `Code/download_model.py`.</span></span> |



### <a name="data-flow"></a><span data-ttu-id="d4c3c-299">Data flow</span><span class="sxs-lookup"><span data-stu-id="d4c3c-299">Data flow</span></span>

<span data-ttu-id="d4c3c-300">The code in [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py)  loads data from the publicly accessible container (`dataFile` field of [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json)).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-300">The code in [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py)  loads data from the publicly accessible container (`dataFile` field of [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldata_storageconfig.json)).</span></span> <span data-ttu-id="d4c3c-301">It includes data preparation and feature engineering, and saves the intermediate compute results and models to your own private container.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-301">It includes data preparation and feature engineering, and saves the intermediate compute results and models to your own private container.</span></span> <span data-ttu-id="d4c3c-302">The code in [`Code/train.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/train.py) loads the intermediate compute results from the private container, trains the multi-class classification model, and writes the trained machine learning model to the private container.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-302">The code in [`Code/train.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/train.py) loads the intermediate compute results from the private container, trains the multi-class classification model, and writes the trained machine learning model to the private container.</span></span> 

<span data-ttu-id="d4c3c-303">You should use one container for experimentation on the one-month data set, and another one for experimentation on the full data set.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-303">You should use one container for experimentation on the one-month data set, and another one for experimentation on the full data set.</span></span> <span data-ttu-id="d4c3c-304">Because the data and models are saved as Parquet file, each file is actually a folder in the container, containing multiple blobs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-304">Because the data and models are saved as Parquet file, each file is actually a folder in the container, containing multiple blobs.</span></span> <span data-ttu-id="d4c3c-305">The resulting container looks as follows:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-305">The resulting container looks as follows:</span></span>

| <span data-ttu-id="d4c3c-306">Blob prefix name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-306">Blob prefix name</span></span> | <span data-ttu-id="d4c3c-307">Type</span><span class="sxs-lookup"><span data-stu-id="d4c3c-307">Type</span></span> | <span data-ttu-id="d4c3c-308">Description</span><span class="sxs-lookup"><span data-stu-id="d4c3c-308">Description</span></span> |
|-----------|------|-------------|
| <span data-ttu-id="d4c3c-309">featureScaleModel</span><span class="sxs-lookup"><span data-stu-id="d4c3c-309">featureScaleModel</span></span> | <span data-ttu-id="d4c3c-310">Parquet</span><span class="sxs-lookup"><span data-stu-id="d4c3c-310">Parquet</span></span> | <span data-ttu-id="d4c3c-311">Standard scaler model for numeric features.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-311">Standard scaler model for numeric features.</span></span> |
| <span data-ttu-id="d4c3c-312">stringIndexModel</span><span class="sxs-lookup"><span data-stu-id="d4c3c-312">stringIndexModel</span></span> | <span data-ttu-id="d4c3c-313">Parquet</span><span class="sxs-lookup"><span data-stu-id="d4c3c-313">Parquet</span></span> | <span data-ttu-id="d4c3c-314">String indexer model for non-numeric features.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-314">String indexer model for non-numeric features.</span></span>|
| <span data-ttu-id="d4c3c-315">oneHotEncoderModel</span><span class="sxs-lookup"><span data-stu-id="d4c3c-315">oneHotEncoderModel</span></span>|<span data-ttu-id="d4c3c-316">Parquet</span><span class="sxs-lookup"><span data-stu-id="d4c3c-316">Parquet</span></span> | <span data-ttu-id="d4c3c-317">One-hot encoder model for categorical features.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-317">One-hot encoder model for categorical features.</span></span> |
| <span data-ttu-id="d4c3c-318">mlModel</span><span class="sxs-lookup"><span data-stu-id="d4c3c-318">mlModel</span></span> | <span data-ttu-id="d4c3c-319">Parquet</span><span class="sxs-lookup"><span data-stu-id="d4c3c-319">Parquet</span></span> | <span data-ttu-id="d4c3c-320">Trained machine learning model.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-320">Trained machine learning model.</span></span> |
| <span data-ttu-id="d4c3c-321">info</span><span class="sxs-lookup"><span data-stu-id="d4c3c-321">info</span></span>| <span data-ttu-id="d4c3c-322">Python pickle file</span><span class="sxs-lookup"><span data-stu-id="d4c3c-322">Python pickle file</span></span> | <span data-ttu-id="d4c3c-323">Information about the transformed data, including training start, training end, duration, the timestamp for train-test splitting, and columns for indexing and one-hot encoding.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-323">Information about the transformed data, including training start, training end, duration, the timestamp for train-test splitting, and columns for indexing and one-hot encoding.</span></span>

<span data-ttu-id="d4c3c-324">All the files and blobs in the preceding table are used for operationalization.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-324">All the files and blobs in the preceding table are used for operationalization.</span></span>


### <a name="model-development"></a><span data-ttu-id="d4c3c-325">Model development</span><span class="sxs-lookup"><span data-stu-id="d4c3c-325">Model development</span></span>

#### <a name="architecture-diagram"></a><span data-ttu-id="d4c3c-326">Architecture diagram</span><span class="sxs-lookup"><span data-stu-id="d4c3c-326">Architecture diagram</span></span>


<span data-ttu-id="d4c3c-327">The following diagram shows the end-to-end workflow of using Machine Learning Workbench to develop the model: ![architecture](media/scenario-big-data/architecture.PNG)</span><span class="sxs-lookup"><span data-stu-id="d4c3c-327">The following diagram shows the end-to-end workflow of using Machine Learning Workbench to develop the model: ![architecture](media/scenario-big-data/architecture.PNG)</span></span>

<span data-ttu-id="d4c3c-328">In the following sections, we show the model development by using the remote compute target functionality in Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-328">In the following sections, we show the model development by using the remote compute target functionality in Machine Learning Workbench.</span></span> <span data-ttu-id="d4c3c-329">We first load a small amount of sample data, and run the script [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) on an Ubuntu DSVM for fast iteration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-329">We first load a small amount of sample data, and run the script [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) on an Ubuntu DSVM for fast iteration.</span></span> <span data-ttu-id="d4c3c-330">We can further limit the work we do in  [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) by passing an extra argument for faster iteration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-330">We can further limit the work we do in  [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) by passing an extra argument for faster iteration.</span></span> <span data-ttu-id="d4c3c-331">In the end, we use an HDInsight cluster to train with full data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-331">In the end, we use an HDInsight cluster to train with full data.</span></span>     

<span data-ttu-id="d4c3c-332">The  [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) file loads and prepares the data, and performs feature engineering.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-332">The  [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py) file loads and prepares the data, and performs feature engineering.</span></span> <span data-ttu-id="d4c3c-333">It accepts two arguments:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-333">It accepts two arguments:</span></span>
* <span data-ttu-id="d4c3c-334">A configuration file for the Blob storage container, for storing the intermediate compute results and models.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-334">A configuration file for the Blob storage container, for storing the intermediate compute results and models.</span></span>
* <span data-ttu-id="d4c3c-335">A debug config argument for faster iteration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-335">A debug config argument for faster iteration.</span></span>

<span data-ttu-id="d4c3c-336">The first argument, `configFilename`, is a local configuration file where you store the Blob storage information, and specify where to load the data.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-336">The first argument, `configFilename`, is a local configuration file where you store the Blob storage information, and specify where to load the data.</span></span> <span data-ttu-id="d4c3c-337">By default, it is  [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/storageconfig.json), and it is going to be used in the one-month data run.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-337">By default, it is  [`Config/storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/storageconfig.json), and it is going to be used in the one-month data run.</span></span> <span data-ttu-id="d4c3c-338">We also include [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json), which you need to use on the full data set run.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-338">We also include [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json), which you need to use on the full data set run.</span></span> <span data-ttu-id="d4c3c-339">The content in the configuration is as follows:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-339">The content in the configuration is as follows:</span></span> 

| <span data-ttu-id="d4c3c-340">Field</span><span class="sxs-lookup"><span data-stu-id="d4c3c-340">Field</span></span> | <span data-ttu-id="d4c3c-341">Type</span><span class="sxs-lookup"><span data-stu-id="d4c3c-341">Type</span></span> | <span data-ttu-id="d4c3c-342">Description</span><span class="sxs-lookup"><span data-stu-id="d4c3c-342">Description</span></span> |
|-----------|------|-------------|
| <span data-ttu-id="d4c3c-343">storageAccount</span><span class="sxs-lookup"><span data-stu-id="d4c3c-343">storageAccount</span></span> | <span data-ttu-id="d4c3c-344">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-344">String</span></span> | <span data-ttu-id="d4c3c-345">Azure Storage account name</span><span class="sxs-lookup"><span data-stu-id="d4c3c-345">Azure Storage account name</span></span> |
| <span data-ttu-id="d4c3c-346">storageContainer</span><span class="sxs-lookup"><span data-stu-id="d4c3c-346">storageContainer</span></span> | <span data-ttu-id="d4c3c-347">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-347">String</span></span> | <span data-ttu-id="d4c3c-348">Container in Azure Storage account to store intermediate results</span><span class="sxs-lookup"><span data-stu-id="d4c3c-348">Container in Azure Storage account to store intermediate results</span></span> |
| <span data-ttu-id="d4c3c-349">storageKey</span><span class="sxs-lookup"><span data-stu-id="d4c3c-349">storageKey</span></span> | <span data-ttu-id="d4c3c-350">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-350">String</span></span> |<span data-ttu-id="d4c3c-351">Azure Storage account access key</span><span class="sxs-lookup"><span data-stu-id="d4c3c-351">Azure Storage account access key</span></span> |
| <span data-ttu-id="d4c3c-352">dataFile</span><span class="sxs-lookup"><span data-stu-id="d4c3c-352">dataFile</span></span>|<span data-ttu-id="d4c3c-353">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-353">String</span></span> | <span data-ttu-id="d4c3c-354">Data source files</span><span class="sxs-lookup"><span data-stu-id="d4c3c-354">Data source files</span></span>  |
| <span data-ttu-id="d4c3c-355">duration</span><span class="sxs-lookup"><span data-stu-id="d4c3c-355">duration</span></span>| <span data-ttu-id="d4c3c-356">String</span><span class="sxs-lookup"><span data-stu-id="d4c3c-356">String</span></span> | <span data-ttu-id="d4c3c-357">Duration of data in the data source files</span><span class="sxs-lookup"><span data-stu-id="d4c3c-357">Duration of data in the data source files</span></span>|

<span data-ttu-id="d4c3c-358">Modify both `Config/storageconfig.json` and `Config/fulldata_storageconfig.json` to configure the storage account, storage key, and the blob container to store the intermediate results.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-358">Modify both `Config/storageconfig.json` and `Config/fulldata_storageconfig.json` to configure the storage account, storage key, and the blob container to store the intermediate results.</span></span> <span data-ttu-id="d4c3c-359">By default, the blob container for the one-month data run is `onemonthmodel`, and the blob container for full data set run is `fullmodel`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-359">By default, the blob container for the one-month data run is `onemonthmodel`, and the blob container for full data set run is `fullmodel`.</span></span> <span data-ttu-id="d4c3c-360">Make sure you create these two containers in your storage account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-360">Make sure you create these two containers in your storage account.</span></span> <span data-ttu-id="d4c3c-361">The `dataFile` field in [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json) configures what data is loaded in [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-361">The `dataFile` field in [`Config/fulldata_storageconfig.json`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Config/fulldatastorageconfig.json) configures what data is loaded in [`Code/etl.py`](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Code/etl.py).</span></span> <span data-ttu-id="d4c3c-362">The `duration` field configures the range the data includes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-362">The `duration` field configures the range the data includes.</span></span> <span data-ttu-id="d4c3c-363">If the duration is set to ONE_MONTH, the data loaded should be just one .csv file among the seven files of the data for June-2016.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-363">If the duration is set to ONE_MONTH, the data loaded should be just one .csv file among the seven files of the data for June-2016.</span></span> <span data-ttu-id="d4c3c-364">If the duration is FULL, the full data set (1 TB) is loaded.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-364">If the duration is FULL, the full data set (1 TB) is loaded.</span></span> <span data-ttu-id="d4c3c-365">You don't need to change `dataFile` and `duration` in these two configuration files.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-365">You don't need to change `dataFile` and `duration` in these two configuration files.</span></span>

<span data-ttu-id="d4c3c-366">The second argument is DEBUG.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-366">The second argument is DEBUG.</span></span> <span data-ttu-id="d4c3c-367">Setting it to FILTER_IP enables a faster iteration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-367">Setting it to FILTER_IP enables a faster iteration.</span></span> <span data-ttu-id="d4c3c-368">Use of this parameter is helpful when you want to debug your script.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-368">Use of this parameter is helpful when you want to debug your script.</span></span>

> [!NOTE]
> <span data-ttu-id="d4c3c-369">In all of the following commands, replace any argument variable with its actual value.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-369">In all of the following commands, replace any argument variable with its actual value.</span></span>
> 


#### <a name="model-development-on-the-docker-of-ubuntu-dsvm"></a><span data-ttu-id="d4c3c-370">Model development on the Docker of Ubuntu DSVM</span><span class="sxs-lookup"><span data-stu-id="d4c3c-370">Model development on the Docker of Ubuntu DSVM</span></span>

#####  <a name="1-set-up-the-compute-target"></a><span data-ttu-id="d4c3c-371">1. Set up the compute target</span><span class="sxs-lookup"><span data-stu-id="d4c3c-371">1. Set up the compute target</span></span>

<span data-ttu-id="d4c3c-372">Start the command line from Machine Learning Workbench by selecting **File** > **Open Command Prompt**.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-372">Start the command line from Machine Learning Workbench by selecting **File** > **Open Command Prompt**.</span></span> <span data-ttu-id="d4c3c-373">Then run:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-373">Then run:</span></span> 

```az ml computetarget attach remotedocker --name dockerdsvm --address $DSVMIPaddress  --username $user --password $password ```

<span data-ttu-id="d4c3c-374">The following two files are created in the aml_config folder of your project:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-374">The following two files are created in the aml_config folder of your project:</span></span>

-  <span data-ttu-id="d4c3c-375">dockerdsvm.compute: This file contains the connection and configuration information for a remote execution target.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-375">dockerdsvm.compute: This file contains the connection and configuration information for a remote execution target.</span></span>
-  <span data-ttu-id="d4c3c-376">dockerdsvm.runconfig: This file is a set of run options used within the Workbench application.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-376">dockerdsvm.runconfig: This file is a set of run options used within the Workbench application.</span></span>

<span data-ttu-id="d4c3c-377">Browse to dockerdsvm.runconfig, and change the configuration of these fields to the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-377">Browse to dockerdsvm.runconfig, and change the configuration of these fields to the following:</span></span>

    PrepareEnvironment: true 
    CondaDependenciesFile: Config/conda_dependencies.yml 
    SparkDependenciesFile: Config/dsvm_spark_dependencies.yml

<span data-ttu-id="d4c3c-378">Prepare the project environment by running:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-378">Prepare the project environment by running:</span></span>

```az ml experiment prepare -c dockerdsvm```


<span data-ttu-id="d4c3c-379">With `PrepareEnvironment` set to true, Machine Learning Workbench creates the runtime environment whenever you submit a job.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-379">With `PrepareEnvironment` set to true, Machine Learning Workbench creates the runtime environment whenever you submit a job.</span></span> <span data-ttu-id="d4c3c-380">`Config/conda_dependencies.yml` and `Config/dsvm_spark_dependencies.yml` contain the customization of the runtime environment.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-380">`Config/conda_dependencies.yml` and `Config/dsvm_spark_dependencies.yml` contain the customization of the runtime environment.</span></span> <span data-ttu-id="d4c3c-381">You can always modify the Conda dependencies, Spark configuration, and Spark dependencies by editing these two YMAL files.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-381">You can always modify the Conda dependencies, Spark configuration, and Spark dependencies by editing these two YMAL files.</span></span> <span data-ttu-id="d4c3c-382">For this example, we added `azure-storage` and `azure-ml-api-sdk` as extra Python packages in  `Config/conda_dependencies.yml`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-382">For this example, we added `azure-storage` and `azure-ml-api-sdk` as extra Python packages in  `Config/conda_dependencies.yml`.</span></span> <span data-ttu-id="d4c3c-383">We also added `spark.default.parallelism`, `spark.executor.instances`, and `spark.executor.cores` in `Config/dsvm_spark_dependencies.yml`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-383">We also added `spark.default.parallelism`, `spark.executor.instances`, and `spark.executor.cores` in `Config/dsvm_spark_dependencies.yml`.</span></span> 

#####  <a name="2-data-preparation-and-feature-engineering-on-dsvm-docker"></a><span data-ttu-id="d4c3c-384">2. Data preparation and feature engineering on DSVM Docker</span><span class="sxs-lookup"><span data-stu-id="d4c3c-384">2. Data preparation and feature engineering on DSVM Docker</span></span>

<span data-ttu-id="d4c3c-385">Run the script `etl.py` on DSVM Docker.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-385">Run the script `etl.py` on DSVM Docker.</span></span> <span data-ttu-id="d4c3c-386">Use a debug parameter that filters the loaded data with specific server IP addresses:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-386">Use a debug parameter that filters the loaded data with specific server IP addresses:</span></span>

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/etl.py ./Config/storageconfig.json FILTER_IP```

<span data-ttu-id="d4c3c-387">Browse to the side panel, and select **Run** to see the run history of `etl.py`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-387">Browse to the side panel, and select **Run** to see the run history of `etl.py`.</span></span> <span data-ttu-id="d4c3c-388">Notice that the run time is about two minutes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-388">Notice that the run time is about two minutes.</span></span> <span data-ttu-id="d4c3c-389">If you plan to change your code to include new features, providing FILTER_IP as the second argument provides a faster iteration.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-389">If you plan to change your code to include new features, providing FILTER_IP as the second argument provides a faster iteration.</span></span> <span data-ttu-id="d4c3c-390">You might need to run this step multiple times when dealing with your own machine learning problems, to explore the data set or create new features.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-390">You might need to run this step multiple times when dealing with your own machine learning problems, to explore the data set or create new features.</span></span> 

<span data-ttu-id="d4c3c-391">With the customized restriction on what data to load, and further filtering of what data to process, you can speed up the iteration process in your model development.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-391">With the customized restriction on what data to load, and further filtering of what data to process, you can speed up the iteration process in your model development.</span></span> <span data-ttu-id="d4c3c-392">As you experiment, you should periodically save the changes in your code to the Git repository.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-392">As you experiment, you should periodically save the changes in your code to the Git repository.</span></span> <span data-ttu-id="d4c3c-393">Note that we used the following code in `etl.py` to enable the access to the private container:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-393">Note that we used the following code in `etl.py` to enable the access to the private container:</span></span>

```python
def attach_storage_container(spark, account, key):
    config = spark._sc._jsc.hadoopConfiguration()
    setting = "fs.azure.account.key." + account + ".blob.core.windows.net"
    if not config.get(setting):
        config.set(setting, key)

# attach the blob storage to the spark cluster or VM so that the storage can be accessed by the cluster or VM        
attach_storage_container(spark, storageAccount, storageKey)
```


<span data-ttu-id="d4c3c-394">Next, run the script `etl.py` on DSVM Docker without the debug parameter FILTER_IP:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-394">Next, run the script `etl.py` on DSVM Docker without the debug parameter FILTER_IP:</span></span>

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/etl.py ./Config/storageconfig.json FALSE```

<span data-ttu-id="d4c3c-395">Browse to the side panel, and select **Run** to see the run history of `etl.py`.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-395">Browse to the side panel, and select **Run** to see the run history of `etl.py`.</span></span> <span data-ttu-id="d4c3c-396">Notice that the run time is about four minutes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-396">Notice that the run time is about four minutes.</span></span> <span data-ttu-id="d4c3c-397">The processed result of this step is saved into the container, and is loaded for training in train.py.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-397">The processed result of this step is saved into the container, and is loaded for training in train.py.</span></span> <span data-ttu-id="d4c3c-398">In addition, the string indexers, encoder pipelines, and standard scalers are saved to the private container.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-398">In addition, the string indexers, encoder pipelines, and standard scalers are saved to the private container.</span></span> <span data-ttu-id="d4c3c-399">These are used in operationalization.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-399">These are used in operationalization.</span></span> 


##### <a name="3-model-training-on-dsvm-docker"></a><span data-ttu-id="d4c3c-400">3. Model training on DSVM Docker</span><span class="sxs-lookup"><span data-stu-id="d4c3c-400">3. Model training on DSVM Docker</span></span>

<span data-ttu-id="d4c3c-401">Run the script `train.py` on DSVM Docker:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-401">Run the script `train.py` on DSVM Docker:</span></span>

```az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/train.py ./Config/storageconfig.json```

<span data-ttu-id="d4c3c-402">This step loads the intermediate compute results from the run of `etl.py`, and  trains a machine learning model.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-402">This step loads the intermediate compute results from the run of `etl.py`, and  trains a machine learning model.</span></span> <span data-ttu-id="d4c3c-403">This step takes about two minutes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-403">This step takes about two minutes.</span></span>

<span data-ttu-id="d4c3c-404">When you have successfully finished the experimentation on the small data, you can continue to run the experimentation on the full data set.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-404">When you have successfully finished the experimentation on the small data, you can continue to run the experimentation on the full data set.</span></span> <span data-ttu-id="d4c3c-405">You can start by using the same code, and then experiment with argument and compute target changes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-405">You can start by using the same code, and then experiment with argument and compute target changes.</span></span>  

####  <a name="model-development-on-the-hdinsight-cluster"></a><span data-ttu-id="d4c3c-406">Model development on the HDInsight cluster</span><span class="sxs-lookup"><span data-stu-id="d4c3c-406">Model development on the HDInsight cluster</span></span>

##### <a name="1-create-the-compute-target-in-machine-learning-workbench-for-the-hdinsight-cluster"></a><span data-ttu-id="d4c3c-407">1. Create the compute target in Machine Learning Workbench for the HDInsight cluster</span><span class="sxs-lookup"><span data-stu-id="d4c3c-407">1. Create the compute target in Machine Learning Workbench for the HDInsight cluster</span></span>

```az ml computetarget attach cluster --name myhdi --address $clustername-ssh.azurehdinsight.net --username $username --password $password```

<span data-ttu-id="d4c3c-408">The following two files are created in the aml_config folder:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-408">The following two files are created in the aml_config folder:</span></span>
    
-  <span data-ttu-id="d4c3c-409">myhdi.compute: This file contains connection and configuration information for a remote execution target.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-409">myhdi.compute: This file contains connection and configuration information for a remote execution target.</span></span>
-  <span data-ttu-id="d4c3c-410">myhdi.runconfig: This file is set of run options used within the Workbench application.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-410">myhdi.runconfig: This file is set of run options used within the Workbench application.</span></span>


<span data-ttu-id="d4c3c-411">Browse to myhdi.runconfig, and change the configuration of these fields to the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-411">Browse to myhdi.runconfig, and change the configuration of these fields to the following:</span></span>

    PrepareEnvironment: true 
    CondaDependenciesFile: Config/conda_dependencies.yml 
    SparkDependenciesFile: Config/hdi_spark_dependencies.yml

<span data-ttu-id="d4c3c-412">Prepare the project environment by running:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-412">Prepare the project environment by running:</span></span>

```az ml experiment prepare -c myhdi```

<span data-ttu-id="d4c3c-413">This step can take up to seven minutes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-413">This step can take up to seven minutes.</span></span>

##### <a name="2-data-preparation-and-feature-engineering-on-hdinsight-cluster"></a><span data-ttu-id="d4c3c-414">2. Data preparation and feature engineering on HDInsight cluster</span><span class="sxs-lookup"><span data-stu-id="d4c3c-414">2. Data preparation and feature engineering on HDInsight cluster</span></span>

<span data-ttu-id="d4c3c-415">Run the script `etl.py`, with full data on the HDInsight cluster:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-415">Run the script `etl.py`, with full data on the HDInsight cluster:</span></span>

```az ml experiment submit -a -t myhdi -c myhdi ./Code/etl.py Config/fulldata_storageconfig.json FALSE```

<span data-ttu-id="d4c3c-416">Because this job lasts for a relatively long time (approximately two hours), you can use `-a` to disable output streaming.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-416">Because this job lasts for a relatively long time (approximately two hours), you can use `-a` to disable output streaming.</span></span> <span data-ttu-id="d4c3c-417">When the job is done, in **Run History**, you can view the driver and controller logs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-417">When the job is done, in **Run History**, you can view the driver and controller logs.</span></span> <span data-ttu-id="d4c3c-418">If you have a larger cluster, you can always reconfigure the configurations in `Config/hdi_spark_dependencies.yml` to use more instances or cores.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-418">If you have a larger cluster, you can always reconfigure the configurations in `Config/hdi_spark_dependencies.yml` to use more instances or cores.</span></span> <span data-ttu-id="d4c3c-419">For example, if you have a four-worker-node cluster, you can increase the value of `spark.executor.instances` from 5 to 7.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-419">For example, if you have a four-worker-node cluster, you can increase the value of `spark.executor.instances` from 5 to 7.</span></span> <span data-ttu-id="d4c3c-420">You can see the output of this step in the **fullmodel** container in your storage account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-420">You can see the output of this step in the **fullmodel** container in your storage account.</span></span> 


##### <a name="3-model-training-on-hdinsight-cluster"></a><span data-ttu-id="d4c3c-421">3. Model training on HDInsight cluster</span><span class="sxs-lookup"><span data-stu-id="d4c3c-421">3. Model training on HDInsight cluster</span></span>

<span data-ttu-id="d4c3c-422">Run the script  `train.py` on HDInsight cluster:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-422">Run the script  `train.py` on HDInsight cluster:</span></span>

```az ml experiment submit -a -t myhdi -c myhdi ./Code/train.py Config/fulldata_storageconfig.json```

<span data-ttu-id="d4c3c-423">Because this job lasts for a relatively long timeapproximately 30 minutes), you can use `-a` to disable output streaming.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-423">Because this job lasts for a relatively long timeapproximately 30 minutes), you can use `-a` to disable output streaming.</span></span>

#### <a name="run-history-exploration"></a><span data-ttu-id="d4c3c-424">Run history exploration</span><span class="sxs-lookup"><span data-stu-id="d4c3c-424">Run history exploration</span></span>

<span data-ttu-id="d4c3c-425">Run history is a feature that enables tracking of your experimentation in Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-425">Run history is a feature that enables tracking of your experimentation in Machine Learning Workbench.</span></span> <span data-ttu-id="d4c3c-426">By default, it tracks the duration of the experimentation.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-426">By default, it tracks the duration of the experimentation.</span></span> <span data-ttu-id="d4c3c-427">In our specific example, when we move to the full data set for `Code/etl.py` in the experimentation, we notice that duration significantly increases.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-427">In our specific example, when we move to the full data set for `Code/etl.py` in the experimentation, we notice that duration significantly increases.</span></span> <span data-ttu-id="d4c3c-428">You can also log specific metrics for tracking purposes.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-428">You can also log specific metrics for tracking purposes.</span></span> <span data-ttu-id="d4c3c-429">To enable metric tracking, add the following lines of code to the head of your Python file:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-429">To enable metric tracking, add the following lines of code to the head of your Python file:</span></span>
```python
# import logger
from azureml.logging import get_azureml_logger

# initialize logger
run_logger = get_azureml_logger()
```
<span data-ttu-id="d4c3c-430">Here is an example to track a specific metric:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-430">Here is an example to track a specific metric:</span></span>

```python
run_logger.log("Test Accuracy", testAccuracy)
```

<span data-ttu-id="d4c3c-431">On the right sidebar of the Workbench, browse to **Runs** to see the run history for each Python file.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-431">On the right sidebar of the Workbench, browse to **Runs** to see the run history for each Python file.</span></span> <span data-ttu-id="d4c3c-432">You can also go to your GitHub repository.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-432">You can also go to your GitHub repository.</span></span> <span data-ttu-id="d4c3c-433">A new branch, with the name starting with "AMLHistory," is created to track the change you made to your script in each run.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-433">A new branch, with the name starting with "AMLHistory," is created to track the change you made to your script in each run.</span></span> 


### <a name="operationalize-the-model"></a><span data-ttu-id="d4c3c-434">Operationalize the model</span><span class="sxs-lookup"><span data-stu-id="d4c3c-434">Operationalize the model</span></span>

<span data-ttu-id="d4c3c-435">In this section, you operationalize the model you created in the previous steps as a web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-435">In this section, you operationalize the model you created in the previous steps as a web service.</span></span> <span data-ttu-id="d4c3c-436">You also learn how to use the web service to predict workload.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-436">You also learn how to use the web service to predict workload.</span></span> <span data-ttu-id="d4c3c-437">Use Machine Language operationalization command-line interfaces (CLIs) to package the code and dependencies as Docker images, and to publish the model as a containerized web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-437">Use Machine Language operationalization command-line interfaces (CLIs) to package the code and dependencies as Docker images, and to publish the model as a containerized web service.</span></span>

<span data-ttu-id="d4c3c-438">You can use the command-line prompt in Machine Learning Workbench to run the CLIs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-438">You can use the command-line prompt in Machine Learning Workbench to run the CLIs.</span></span>  <span data-ttu-id="d4c3c-439">You can also run the CLIs on Ubuntu Linux by following the [installation guide](./deployment-setup-configuration.md#using-the-cli).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-439">You can also run the CLIs on Ubuntu Linux by following the [installation guide](./deployment-setup-configuration.md#using-the-cli).</span></span> 

> [!NOTE]
> <span data-ttu-id="d4c3c-440">In all the following commands, replace any argument variable with its actual value.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-440">In all the following commands, replace any argument variable with its actual value.</span></span> <span data-ttu-id="d4c3c-441">It takes about 40 minutes to finish this section.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-441">It takes about 40 minutes to finish this section.</span></span>
> 

<span data-ttu-id="d4c3c-442">Choose a unique string as the environment for operationalization.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-442">Choose a unique string as the environment for operationalization.</span></span> <span data-ttu-id="d4c3c-443">Here, we use the string "[unique]" to represent the string you choose.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-443">Here, we use the string "[unique]" to represent the string you choose.</span></span>

1. <span data-ttu-id="d4c3c-444">Create the environment for operationalization, and create the resource group.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-444">Create the environment for operationalization, and create the resource group.</span></span>

        az ml env setup -c -n [unique] --location eastus2 --cluster -z 5 --yes

   <span data-ttu-id="d4c3c-445">Note that you can use Container Service as the environment by using  `--cluster` in the `az ml env setup` command.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-445">Note that you can use Container Service as the environment by using  `--cluster` in the `az ml env setup` command.</span></span> <span data-ttu-id="d4c3c-446">You can operationalize the machine learning model on [Azure Container Service](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-intro-kubernetes).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-446">You can operationalize the machine learning model on [Azure Container Service](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-intro-kubernetes).</span></span> <span data-ttu-id="d4c3c-447">It uses [Kubernetes](https://kubernetes.io/) for automating deployment, scaling, and management of containerized applications.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-447">It uses [Kubernetes](https://kubernetes.io/) for automating deployment, scaling, and management of containerized applications.</span></span> <span data-ttu-id="d4c3c-448">This command takes about 20 minutes to run.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-448">This command takes about 20 minutes to run.</span></span> <span data-ttu-id="d4c3c-449">Use the following to check if the deployment has finished successfully:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-449">Use the following to check if the deployment has finished successfully:</span></span> 

        az ml env show -g [unique]rg -n [unique]

   <span data-ttu-id="d4c3c-450">Set the deployment environment as the one you just created by running the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-450">Set the deployment environment as the one you just created by running the following:</span></span>

        az ml env set -g [unique]rg -n [unique]

2. <span data-ttu-id="d4c3c-451">Create and use a model management account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-451">Create and use a model management account.</span></span> <span data-ttu-id="d4c3c-452">To create a model management account, run the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-452">To create a model management account, run the following:</span></span>

        az ml account modelmanagement create --location  eastus2 -n [unique]acc -g [unique]rg --sku-instances 4 --sku-name S3 

   <span data-ttu-id="d4c3c-453">Use the model management for operationalization by running the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-453">Use the model management for operationalization by running the following:</span></span>

        az ml account modelmanagement set  -n [unique]acc -g [unique]rg  

   <span data-ttu-id="d4c3c-454">A model management account is used to manage the models and web services.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-454">A model management account is used to manage the models and web services.</span></span> <span data-ttu-id="d4c3c-455">From the Azure portal, you can see a new model management account has been created.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-455">From the Azure portal, you can see a new model management account has been created.</span></span> <span data-ttu-id="d4c3c-456">You can see the models, manifests, Docker images, and services that are created by using this model management account.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-456">You can see the models, manifests, Docker images, and services that are created by using this model management account.</span></span>

3. <span data-ttu-id="d4c3c-457">Download and register the models.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-457">Download and register the models.</span></span>

   <span data-ttu-id="d4c3c-458">Download the models in the **fullmodel** container to your local machine in the directory of code.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-458">Download the models in the **fullmodel** container to your local machine in the directory of code.</span></span> <span data-ttu-id="d4c3c-459">Do not download the parquet data file with the name "vmlSource.parquet."</span><span class="sxs-lookup"><span data-stu-id="d4c3c-459">Do not download the parquet data file with the name "vmlSource.parquet."</span></span> <span data-ttu-id="d4c3c-460">It's not a model file; it's an intermediate compute result.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-460">It's not a model file; it's an intermediate compute result.</span></span> <span data-ttu-id="d4c3c-461">You can also reuse the model files included in the Git repository.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-461">You can also reuse the model files included in the Git repository.</span></span> <span data-ttu-id="d4c3c-462">For more information, see [GitHub](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Docs/DownloadModelsFromBlob.md).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-462">For more information, see [GitHub](https://github.com/Azure/MachineLearningSamples-BigData/blob/master/Docs/DownloadModelsFromBlob.md).</span></span> 

   <span data-ttu-id="d4c3c-463">Go to the `Model` folder in the CLI, and register the models as follows:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-463">Go to the `Model` folder in the CLI, and register the models as follows:</span></span>

        az ml model register -m  mlModel -n vmlModel -t fullmodel
        az ml model register -m  featureScaleModel -n featureScaleModel -t fullmodel
        az ml model register -m  oneHotEncoderModel -n  oneHotEncoderModel -t fullmodel
        az ml model register -m  stringIndexModel -n stringIndexModel -t fullmodel
        az ml model register -m  info -n info -t fullmodel

   <span data-ttu-id="d4c3c-464">The output of each command gives a model ID, which is needed in the next step.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-464">The output of each command gives a model ID, which is needed in the next step.</span></span> <span data-ttu-id="d4c3c-465">Save them in a text file for future use.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-465">Save them in a text file for future use.</span></span>

4. <span data-ttu-id="d4c3c-466">Create a manifest for the web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-466">Create a manifest for the web service.</span></span>

   <span data-ttu-id="d4c3c-467">A manifest includes the code for the web service, all the machine learning models, and runtime environment dependencies.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-467">A manifest includes the code for the web service, all the machine learning models, and runtime environment dependencies.</span></span> <span data-ttu-id="d4c3c-468">Go to the `Code` folder in the CLI, and run the following command:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-468">Go to the `Code` folder in the CLI, and run the following command:</span></span>

        az ml manifest create -n $webserviceName -f webservice.py -r spark-py -c ../Config/conda_dependencies_webservice.yml -i $modelID1 -i $modelID2 -i $modelID3 -i $modelID4 -i $modelID5

   <span data-ttu-id="d4c3c-469">The output gives a manifest ID for the next step.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-469">The output gives a manifest ID for the next step.</span></span> 

   <span data-ttu-id="d4c3c-470">Stay in the `Code` directory, and you can test webservice.py by running the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-470">Stay in the `Code` directory, and you can test webservice.py by running the following:</span></span> 

        az ml experiment submit -t dockerdsvm -c dockerdsvm webservice.py

5. <span data-ttu-id="d4c3c-471">Create a Docker image.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-471">Create a Docker image.</span></span> 

        az ml image create -n [unique]image --manifest-id $manifestID

   <span data-ttu-id="d4c3c-472">The output gives an image ID for the next step, This docker image is used in Container Service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-472">The output gives an image ID for the next step, This docker image is used in Container Service.</span></span> 

6. <span data-ttu-id="d4c3c-473">Deploy the web service to the Container Service cluster.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-473">Deploy the web service to the Container Service cluster.</span></span>

        az ml service create realtime -n [unique] --image-id $imageID --cpu 0.5 --memory 2G

   <span data-ttu-id="d4c3c-474">The output gives a service ID.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-474">The output gives a service ID.</span></span> <span data-ttu-id="d4c3c-475">You need to use it to get the authorization key and service URL.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-475">You need to use it to get the authorization key and service URL.</span></span>

7. <span data-ttu-id="d4c3c-476">Call the web service in Python code to score in mini-batches.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-476">Call the web service in Python code to score in mini-batches.</span></span>

   <span data-ttu-id="d4c3c-477">Use the following command to get the authorization key:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-477">Use the following command to get the authorization key:</span></span>

         az ml service keys realtime -i $ServiceID 

   <span data-ttu-id="d4c3c-478">Use the following command to get the service scoring URL:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-478">Use the following command to get the service scoring URL:</span></span>

        az ml service usage realtime -i $ServiceID

   <span data-ttu-id="d4c3c-479">Modify the content in `./Config/webservice.json` with the right service scoring URL and authorization key.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-479">Modify the content in `./Config/webservice.json` with the right service scoring URL and authorization key.</span></span> <span data-ttu-id="d4c3c-480">Keep the "Bearer" in the original file, and replace the "xxx" part.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-480">Keep the "Bearer" in the original file, and replace the "xxx" part.</span></span> 
   
   <span data-ttu-id="d4c3c-481">Go to the root directory of your project, and test the web service for mini-batch scoring by using the following:</span><span class="sxs-lookup"><span data-stu-id="d4c3c-481">Go to the root directory of your project, and test the web service for mini-batch scoring by using the following:</span></span>

        az ml experiment submit -t dockerdsvm -c dockerdsvm ./Code/scoring_webservice.py ./Config/webservice.json

8. <span data-ttu-id="d4c3c-482">Scale the web service.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-482">Scale the web service.</span></span> 

   <span data-ttu-id="d4c3c-483">For more information, see [How to scale operationalization on your Azure Container Service cluster](how-to-scale-clusters.md).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-483">For more information, see [How to scale operationalization on your Azure Container Service cluster](how-to-scale-clusters.md).</span></span>
 

## <a name="next-steps"></a><span data-ttu-id="d4c3c-484">Next steps</span><span class="sxs-lookup"><span data-stu-id="d4c3c-484">Next steps</span></span>

<span data-ttu-id="d4c3c-485">This example highlights how to use Machine Learning Workbench to train a machine learning model on big data, and operationalize the trained model.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-485">This example highlights how to use Machine Learning Workbench to train a machine learning model on big data, and operationalize the trained model.</span></span> <span data-ttu-id="d4c3c-486">In particular, you learned how to configure and use different compute targets, and run the history of tracking metrics and use different runs.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-486">In particular, you learned how to configure and use different compute targets, and run the history of tracking metrics and use different runs.</span></span>

<span data-ttu-id="d4c3c-487">You can extend the code to explore cross-validation and hyper-parameter tuning.</span><span class="sxs-lookup"><span data-stu-id="d4c3c-487">You can extend the code to explore cross-validation and hyper-parameter tuning.</span></span> <span data-ttu-id="d4c3c-488">To learn more about cross-validation and hyper-parameter tuning, see [this GitHub resource](https://github.com/Azure/MachineLearningSamples-DistributedHyperParameterTuning).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-488">To learn more about cross-validation and hyper-parameter tuning, see [this GitHub resource](https://github.com/Azure/MachineLearningSamples-DistributedHyperParameterTuning).</span></span>  

<span data-ttu-id="d4c3c-489">To learn more about time-series forecasting, see [this GitHub resource](https://github.com/Azure/MachineLearningSamples-EnergyDemandTimeSeriesForecasting).</span><span class="sxs-lookup"><span data-stu-id="d4c3c-489">To learn more about time-series forecasting, see [this GitHub resource](https://github.com/Azure/MachineLearningSamples-EnergyDemandTimeSeriesForecasting).</span></span>
