---
title: Image Classification using CNTK inside Azure Machine Learning Workbench | Microsoft Docs
description: Train, evaluate, and deploy a custom image classification model using Azure ML Workbench.
services: machine-learning
documentationcenter: ''
author: PatrickBue
ms.author: pabuehle
manager: mwinkle
ms.reviewer: marhamil, mldocs, garyericson, jasonwhowell
ms.service: machine-learning
ms.component: core
ms.workload: data-services
ms.topic: article
ms.date: 10/17/2017
ms.openlocfilehash: 48c21638fe5756e6527288ed0fdc73dd9e331afd
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44968950"
---
# <a name="image-classification-using-azure-machine-learning-workbench"></a><span data-ttu-id="2158f-103">Image classification using Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="2158f-103">Image classification using Azure Machine Learning Workbench</span></span>

<span data-ttu-id="2158f-104">Image classification approaches can be used to solve a large number of Computer Vision problems.</span><span class="sxs-lookup"><span data-stu-id="2158f-104">Image classification approaches can be used to solve a large number of Computer Vision problems.</span></span>
<span data-ttu-id="2158f-105">These include building models, which answer questions such as: *Is an OBJECT present in the image?* where OBJECT could for example be *dog*, *car*, or *ship*.</span><span class="sxs-lookup"><span data-stu-id="2158f-105">These include building models, which answer questions such as: *Is an OBJECT present in the image?* where OBJECT could for example be *dog*, *car*, or *ship*.</span></span> <span data-ttu-id="2158f-106">Or more complex questions like: *What class of eye disease severity is evinced by this patient's retinal scan?*.</span><span class="sxs-lookup"><span data-stu-id="2158f-106">Or more complex questions like: *What class of eye disease severity is evinced by this patient's retinal scan?*.</span></span>

<span data-ttu-id="2158f-107">This tutorial addresses solving such problems.</span><span class="sxs-lookup"><span data-stu-id="2158f-107">This tutorial addresses solving such problems.</span></span> <span data-ttu-id="2158f-108">We show how to train, evaluate, and deploy your own image classification model using the  [Microsoft Cognitive Toolkit (CNTK) ](https://docs.microsoft.com/cognitive-toolkit/) for deep learning.</span><span class="sxs-lookup"><span data-stu-id="2158f-108">We show how to train, evaluate, and deploy your own image classification model using the  [Microsoft Cognitive Toolkit (CNTK) ](https://docs.microsoft.com/cognitive-toolkit/) for deep learning.</span></span>
<span data-ttu-id="2158f-109">Example images are provided, but the reader can also bring their own dataset and train their own custom models.</span><span class="sxs-lookup"><span data-stu-id="2158f-109">Example images are provided, but the reader can also bring their own dataset and train their own custom models.</span></span>

<span data-ttu-id="2158f-110">Computer Vision solutions traditionally required expert knowledge to manually identify and implement so-called *features*, which highlight desired information in images.</span><span class="sxs-lookup"><span data-stu-id="2158f-110">Computer Vision solutions traditionally required expert knowledge to manually identify and implement so-called *features*, which highlight desired information in images.</span></span>
<span data-ttu-id="2158f-111">This manual approach changed in 2012 with the famous [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] Deep Learning paper, and at present, Deep Neural Networks (DNN) are used to automatically find these features.</span><span class="sxs-lookup"><span data-stu-id="2158f-111">This manual approach changed in 2012 with the famous [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] Deep Learning paper, and at present, Deep Neural Networks (DNN) are used to automatically find these features.</span></span>
<span data-ttu-id="2158f-112">DNNs led to a huge improvement in the field not just for image classification, but also for other Computer Vision problems such as object detection and image similarity.</span><span class="sxs-lookup"><span data-stu-id="2158f-112">DNNs led to a huge improvement in the field not just for image classification, but also for other Computer Vision problems such as object detection and image similarity.</span></span>


## <a name="link-to-the-gallery-github-repository"></a><span data-ttu-id="2158f-113">Link to the gallery GitHub repository</span><span class="sxs-lookup"><span data-stu-id="2158f-113">Link to the gallery GitHub repository</span></span>
[https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK](https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK)

## <a name="overview"></a><span data-ttu-id="2158f-114">Overview</span><span class="sxs-lookup"><span data-stu-id="2158f-114">Overview</span></span>

<span data-ttu-id="2158f-115">This tutorial is split into three parts:</span><span class="sxs-lookup"><span data-stu-id="2158f-115">This tutorial is split into three parts:</span></span>

- <span data-ttu-id="2158f-116">Part 1 shows how to train, evaluate, and deploy an image classification system using a pre-trained DNN as featurizer and training an SVM on its output.</span><span class="sxs-lookup"><span data-stu-id="2158f-116">Part 1 shows how to train, evaluate, and deploy an image classification system using a pre-trained DNN as featurizer and training an SVM on its output.</span></span>
- <span data-ttu-id="2158f-117">Part 2 then shows how to improve accuracy by, for example, refining the DNN rather than using it as a fixed featurizer.</span><span class="sxs-lookup"><span data-stu-id="2158f-117">Part 2 then shows how to improve accuracy by, for example, refining the DNN rather than using it as a fixed featurizer.</span></span>
- <span data-ttu-id="2158f-118">Part 3 covers how to use your own dataset instead of the provided example images, and if needed, how to produce your own dataset by scraping images from the net.</span><span class="sxs-lookup"><span data-stu-id="2158f-118">Part 3 covers how to use your own dataset instead of the provided example images, and if needed, how to produce your own dataset by scraping images from the net.</span></span>

<span data-ttu-id="2158f-119">While previous experience with machine learning and CNTK is not required, it is helpful for understanding the underlying principles.</span><span class="sxs-lookup"><span data-stu-id="2158f-119">While previous experience with machine learning and CNTK is not required, it is helpful for understanding the underlying principles.</span></span> <span data-ttu-id="2158f-120">Accuracy numbers, training time, etc. reported in the tutorial are only for reference, and the actual values when running the code almost certainly differ.</span><span class="sxs-lookup"><span data-stu-id="2158f-120">Accuracy numbers, training time, etc. reported in the tutorial are only for reference, and the actual values when running the code almost certainly differ.</span></span>


## <a name="prerequisites"></a><span data-ttu-id="2158f-121">Prerequisites</span><span class="sxs-lookup"><span data-stu-id="2158f-121">Prerequisites</span></span>

<span data-ttu-id="2158f-122">The prerequisites to run this example are as follows:</span><span class="sxs-lookup"><span data-stu-id="2158f-122">The prerequisites to run this example are as follows:</span></span>

1. <span data-ttu-id="2158f-123">An [Azure account](https://azure.microsoft.com/free/) (free trials are available).</span><span class="sxs-lookup"><span data-stu-id="2158f-123">An [Azure account](https://azure.microsoft.com/free/) (free trials are available).</span></span>
2. <span data-ttu-id="2158f-124">The [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) following the [quick start installation guide](../service/quickstart-installation.md) to install the program and create a workspace.</span><span class="sxs-lookup"><span data-stu-id="2158f-124">The [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) following the [quick start installation guide](../service/quickstart-installation.md) to install the program and create a workspace.</span></span>  
3. <span data-ttu-id="2158f-125">A Windows machine.</span><span class="sxs-lookup"><span data-stu-id="2158f-125">A Windows machine.</span></span> <span data-ttu-id="2158f-126">Windows OS is necessary since the Workbench supports only Windows and MacOS, while Microsoft's Cognitive Toolkit (which we use as deep learning library) only supports Windows and Linux.</span><span class="sxs-lookup"><span data-stu-id="2158f-126">Windows OS is necessary since the Workbench supports only Windows and MacOS, while Microsoft's Cognitive Toolkit (which we use as deep learning library) only supports Windows and Linux.</span></span>
4. <span data-ttu-id="2158f-127">A dedicated GPU is not required to execute the SVM training in part 1, however it is needed for refining of the DNN described in part 2.</span><span class="sxs-lookup"><span data-stu-id="2158f-127">A dedicated GPU is not required to execute the SVM training in part 1, however it is needed for refining of the DNN described in part 2.</span></span> <span data-ttu-id="2158f-128">If you lack a strong GPU, want to train on multiple GPUs, or do not have a Windows machine, then consider using Azure's Deep Learning Virtual Machine with Windows operating system.</span><span class="sxs-lookup"><span data-stu-id="2158f-128">If you lack a strong GPU, want to train on multiple GPUs, or do not have a Windows machine, then consider using Azure's Deep Learning Virtual Machine with Windows operating system.</span></span> <span data-ttu-id="2158f-129">See [here](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) for a 1-click deployment guide.</span><span class="sxs-lookup"><span data-stu-id="2158f-129">See [here](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) for a 1-click deployment guide.</span></span> <span data-ttu-id="2158f-130">Once deployed, connect to the VM via a remote desktop connection, install Workbench there, and execute the code locally from the VM.</span><span class="sxs-lookup"><span data-stu-id="2158f-130">Once deployed, connect to the VM via a remote desktop connection, install Workbench there, and execute the code locally from the VM.</span></span>
5. <span data-ttu-id="2158f-131">Various Python libraries such as OpenCV need to be installed.</span><span class="sxs-lookup"><span data-stu-id="2158f-131">Various Python libraries such as OpenCV need to be installed.</span></span> <span data-ttu-id="2158f-132">Click *Open Command Prompt* from the *File* menu in the Workbench and run the following commands to install these dependencies:</span><span class="sxs-lookup"><span data-stu-id="2158f-132">Click *Open Command Prompt* from the *File* menu in the Workbench and run the following commands to install these dependencies:</span></span>  
    - `pip install https://cntk.ai/PythonWheel/GPU/cntk-2.2-cp35-cp35m-win_amd64.whl`  
    - <span data-ttu-id="2158f-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` after downloading the OpenCV wheel from http://www.lfd.uci.edu/~gohlke/pythonlibs/ (the exact filename and version can change)</span><span class="sxs-lookup"><span data-stu-id="2158f-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` after downloading the OpenCV wheel from http://www.lfd.uci.edu/~gohlke/pythonlibs/ (the exact filename and version can change)</span></span>
    - `conda install pillow`
    - `pip install -U numpy`
    - `pip install bqplot`
    - `jupyter nbextension enable --py --sys-prefix bqplot`
    - `jupyter nbextension enable --py widgetsnbextension`

### <a name="troubleshooting--known-bugs"></a><span data-ttu-id="2158f-134">Troubleshooting / Known bugs</span><span class="sxs-lookup"><span data-stu-id="2158f-134">Troubleshooting / Known bugs</span></span>
- <span data-ttu-id="2158f-135">A GPU is needed for part 2, and otherwise the error "Batch normalization training on CPU is not yet implemented" is thrown when trying to refine the DNN.</span><span class="sxs-lookup"><span data-stu-id="2158f-135">A GPU is needed for part 2, and otherwise the error "Batch normalization training on CPU is not yet implemented" is thrown when trying to refine the DNN.</span></span>
- <span data-ttu-id="2158f-136">Out-of-memory errors during DNN training can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="2158f-136">Out-of-memory errors during DNN training can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span></span>
- <span data-ttu-id="2158f-137">The code was tested using CNTK 2.2, and should run also on older (up to v2.0) and newer versions without any or only minor changes.</span><span class="sxs-lookup"><span data-stu-id="2158f-137">The code was tested using CNTK 2.2, and should run also on older (up to v2.0) and newer versions without any or only minor changes.</span></span>
- <span data-ttu-id="2158f-138">At the time of writing, the Azure Machine Learning Workbench had problems showing notebooks larger than 5 Mbytes.</span><span class="sxs-lookup"><span data-stu-id="2158f-138">At the time of writing, the Azure Machine Learning Workbench had problems showing notebooks larger than 5 Mbytes.</span></span> <span data-ttu-id="2158f-139">Notebooks of this large size can happen if the notebook is saved with all cell output displayed.</span><span class="sxs-lookup"><span data-stu-id="2158f-139">Notebooks of this large size can happen if the notebook is saved with all cell output displayed.</span></span> <span data-ttu-id="2158f-140">If you encounter this error, then open the command prompt from the File menu inside the Workbench, execute `jupyter notebook`, open the notebook, clear all output, and save the notebook.</span><span class="sxs-lookup"><span data-stu-id="2158f-140">If you encounter this error, then open the command prompt from the File menu inside the Workbench, execute `jupyter notebook`, open the notebook, clear all output, and save the notebook.</span></span> <span data-ttu-id="2158f-141">After performing these steps, the notebook will open properly inside the Azure Machine Learning Workbench again.</span><span class="sxs-lookup"><span data-stu-id="2158f-141">After performing these steps, the notebook will open properly inside the Azure Machine Learning Workbench again.</span></span>
- <span data-ttu-id="2158f-142">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span><span class="sxs-lookup"><span data-stu-id="2158f-142">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span></span> <span data-ttu-id="2158f-143">All notebooks need to be executed with kernel set to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local").</span><span class="sxs-lookup"><span data-stu-id="2158f-143">All notebooks need to be executed with kernel set to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local").</span></span>

    
## <a name="create-a-new-workbench-project"></a><span data-ttu-id="2158f-144">Create a new workbench project</span><span class="sxs-lookup"><span data-stu-id="2158f-144">Create a new workbench project</span></span>

<span data-ttu-id="2158f-145">To create a new project using this example as a template:</span><span class="sxs-lookup"><span data-stu-id="2158f-145">To create a new project using this example as a template:</span></span>
1.  <span data-ttu-id="2158f-146">Open Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="2158f-146">Open Azure Machine Learning Workbench.</span></span>
2.  <span data-ttu-id="2158f-147">On the **Projects** page, click the **+** sign and select **New Project**.</span><span class="sxs-lookup"><span data-stu-id="2158f-147">On the **Projects** page, click the **+** sign and select **New Project**.</span></span>
3.  <span data-ttu-id="2158f-148">In the **Create New Project** pane, fill in the information for your new project.</span><span class="sxs-lookup"><span data-stu-id="2158f-148">In the **Create New Project** pane, fill in the information for your new project.</span></span>
4.  <span data-ttu-id="2158f-149">In the **Search Project Templates** search box, type "Image classification" and select the template.</span><span class="sxs-lookup"><span data-stu-id="2158f-149">In the **Search Project Templates** search box, type "Image classification" and select the template.</span></span>
5.  <span data-ttu-id="2158f-150">Click **Create**.</span><span class="sxs-lookup"><span data-stu-id="2158f-150">Click **Create**.</span></span>

<span data-ttu-id="2158f-151">Performing these steps creates the project structure shown below.</span><span class="sxs-lookup"><span data-stu-id="2158f-151">Performing these steps creates the project structure shown below.</span></span> <span data-ttu-id="2158f-152">The project directory is restricted to be less than 25 Mbytes since the Azure Machine Learning Workbench creates a copy of this folder after each run (to enable run history).</span><span class="sxs-lookup"><span data-stu-id="2158f-152">The project directory is restricted to be less than 25 Mbytes since the Azure Machine Learning Workbench creates a copy of this folder after each run (to enable run history).</span></span> <span data-ttu-id="2158f-153">Hence, all image and temporary files are saved to and from the directory *~/Desktop/imgClassificationUsingCntk_data* (referred to as *DATA_DIR* in this document).</span><span class="sxs-lookup"><span data-stu-id="2158f-153">Hence, all image and temporary files are saved to and from the directory *~/Desktop/imgClassificationUsingCntk_data* (referred to as *DATA_DIR* in this document).</span></span>

  <span data-ttu-id="2158f-154">Folder</span><span class="sxs-lookup"><span data-stu-id="2158f-154">Folder</span></span>| <span data-ttu-id="2158f-155">Description</span><span class="sxs-lookup"><span data-stu-id="2158f-155">Description</span></span>
  ---|---
  <span data-ttu-id="2158f-156">aml_config/</span><span class="sxs-lookup"><span data-stu-id="2158f-156">aml_config/</span></span>|                           <span data-ttu-id="2158f-157">Directory containing the Azure Machine Learning Workbench configuration files</span><span class="sxs-lookup"><span data-stu-id="2158f-157">Directory containing the Azure Machine Learning Workbench configuration files</span></span>
  <span data-ttu-id="2158f-158">libraries/</span><span class="sxs-lookup"><span data-stu-id="2158f-158">libraries/</span></span>|                              <span data-ttu-id="2158f-159">Directory containing all Python and Jupyter helper functions</span><span class="sxs-lookup"><span data-stu-id="2158f-159">Directory containing all Python and Jupyter helper functions</span></span>
  <span data-ttu-id="2158f-160">notebooks/</span><span class="sxs-lookup"><span data-stu-id="2158f-160">notebooks/</span></span>|                              <span data-ttu-id="2158f-161">Directory containing all notebooks</span><span class="sxs-lookup"><span data-stu-id="2158f-161">Directory containing all notebooks</span></span>
  <span data-ttu-id="2158f-162">resources/</span><span class="sxs-lookup"><span data-stu-id="2158f-162">resources/</span></span>|                              <span data-ttu-id="2158f-163">Directory containing all resources (for example url of fashion images)</span><span class="sxs-lookup"><span data-stu-id="2158f-163">Directory containing all resources (for example url of fashion images)</span></span>
  <span data-ttu-id="2158f-164">scripts/</span><span class="sxs-lookup"><span data-stu-id="2158f-164">scripts/</span></span>|                              <span data-ttu-id="2158f-165">Directory containing all scripts</span><span class="sxs-lookup"><span data-stu-id="2158f-165">Directory containing all scripts</span></span>
  <span data-ttu-id="2158f-166">PARAMETERS.py</span><span class="sxs-lookup"><span data-stu-id="2158f-166">PARAMETERS.py</span></span>|                       <span data-ttu-id="2158f-167">Python script specifying all parameters</span><span class="sxs-lookup"><span data-stu-id="2158f-167">Python script specifying all parameters</span></span>
  <span data-ttu-id="2158f-168">readme.md</span><span class="sxs-lookup"><span data-stu-id="2158f-168">readme.md</span></span>|                           <span data-ttu-id="2158f-169">This readme document</span><span class="sxs-lookup"><span data-stu-id="2158f-169">This readme document</span></span>


## <a name="data-description"></a><span data-ttu-id="2158f-170">Data description</span><span class="sxs-lookup"><span data-stu-id="2158f-170">Data description</span></span>

<span data-ttu-id="2158f-171">This tutorial uses as running example an upper body clothing texture dataset consisting of up to 428 images.</span><span class="sxs-lookup"><span data-stu-id="2158f-171">This tutorial uses as running example an upper body clothing texture dataset consisting of up to 428 images.</span></span> <span data-ttu-id="2158f-172">Each image is annotated as one of three different textures (dotted, striped, leopard).</span><span class="sxs-lookup"><span data-stu-id="2158f-172">Each image is annotated as one of three different textures (dotted, striped, leopard).</span></span> <span data-ttu-id="2158f-173">We kept the number of images small so that this tutorial can be executed quickly.</span><span class="sxs-lookup"><span data-stu-id="2158f-173">We kept the number of images small so that this tutorial can be executed quickly.</span></span> <span data-ttu-id="2158f-174">However, the code is well-tested and works with tens of thousands of images or more.</span><span class="sxs-lookup"><span data-stu-id="2158f-174">However, the code is well-tested and works with tens of thousands of images or more.</span></span> <span data-ttu-id="2158f-175">All images were scraped using Bing Image Search and hand-annotated as is explained in [Part 3](#using-a-custom-dataset).</span><span class="sxs-lookup"><span data-stu-id="2158f-175">All images were scraped using Bing Image Search and hand-annotated as is explained in [Part 3](#using-a-custom-dataset).</span></span> <span data-ttu-id="2158f-176">The image URLs with their respective attributes are listed in the */resources/fashionTextureUrls.tsv* file.</span><span class="sxs-lookup"><span data-stu-id="2158f-176">The image URLs with their respective attributes are listed in the */resources/fashionTextureUrls.tsv* file.</span></span>

<span data-ttu-id="2158f-177">The script `0_downloadData.py` downloads all images to the *DATA_DIR/images/fashionTexture/* directory.</span><span class="sxs-lookup"><span data-stu-id="2158f-177">The script `0_downloadData.py` downloads all images to the *DATA_DIR/images/fashionTexture/* directory.</span></span> <span data-ttu-id="2158f-178">Some of the 428 URLs are likely broken.</span><span class="sxs-lookup"><span data-stu-id="2158f-178">Some of the 428 URLs are likely broken.</span></span> <span data-ttu-id="2158f-179">This is not an issue, and just means that we have slightly fewer images for training and testing.</span><span class="sxs-lookup"><span data-stu-id="2158f-179">This is not an issue, and just means that we have slightly fewer images for training and testing.</span></span> <span data-ttu-id="2158f-180">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span><span class="sxs-lookup"><span data-stu-id="2158f-180">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span></span>

<span data-ttu-id="2158f-181">The following figure shows examples for the attributes dotted (left), striped (middle), and leopard (right).</span><span class="sxs-lookup"><span data-stu-id="2158f-181">The following figure shows examples for the attributes dotted (left), striped (middle), and leopard (right).</span></span> <span data-ttu-id="2158f-182">Annotations were done according to the upper body clothing item.</span><span class="sxs-lookup"><span data-stu-id="2158f-182">Annotations were done according to the upper body clothing item.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/examples_all.jpg"  alt="alt text" width="700">
</p>


## <a name="part-1---model-training-and-evaluation"></a><span data-ttu-id="2158f-183">Part 1 - Model training and evaluation</span><span class="sxs-lookup"><span data-stu-id="2158f-183">Part 1 - Model training and evaluation</span></span>

<span data-ttu-id="2158f-184">In the first part of this tutorial, we are training a system that uses, but does not modify, a pre-trained deep neural network.</span><span class="sxs-lookup"><span data-stu-id="2158f-184">In the first part of this tutorial, we are training a system that uses, but does not modify, a pre-trained deep neural network.</span></span> <span data-ttu-id="2158f-185">This pre-trained DNN is used as a featurizer, and a linear SVM is trained to predict the attribute (dotted, striped, or leopard) of a given image.</span><span class="sxs-lookup"><span data-stu-id="2158f-185">This pre-trained DNN is used as a featurizer, and a linear SVM is trained to predict the attribute (dotted, striped, or leopard) of a given image.</span></span>

<span data-ttu-id="2158f-186">We now described this approach in detail, step-by-step, and show which scripts need to be executed.</span><span class="sxs-lookup"><span data-stu-id="2158f-186">We now described this approach in detail, step-by-step, and show which scripts need to be executed.</span></span> <span data-ttu-id="2158f-187">We recommend after each step to inspect which files are written and where they are written to.</span><span class="sxs-lookup"><span data-stu-id="2158f-187">We recommend after each step to inspect which files are written and where they are written to.</span></span>

<span data-ttu-id="2158f-188">All important parameters are specified, and a short explanation provided, in a single place: the  `PARAMETERS.py` file.</span><span class="sxs-lookup"><span data-stu-id="2158f-188">All important parameters are specified, and a short explanation provided, in a single place: the  `PARAMETERS.py` file.</span></span>




### <a name="step-1-data-preparation"></a><span data-ttu-id="2158f-189">Step 1: Data preparation</span><span class="sxs-lookup"><span data-stu-id="2158f-189">Step 1: Data preparation</span></span>
`Script: 1_prepareData.py. Notebook: showImages.ipynb`

<span data-ttu-id="2158f-190">The notebook `showImages.ipynb` can be used to visualize the images, and to correct their annotation as needed.</span><span class="sxs-lookup"><span data-stu-id="2158f-190">The notebook `showImages.ipynb` can be used to visualize the images, and to correct their annotation as needed.</span></span> <span data-ttu-id="2158f-191">To run the notebook, open it in Azure Machine Learning Workbench, click on "Start Notebook Server" if this option is shown, change to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local"), and then execute all cells in the notebook.</span><span class="sxs-lookup"><span data-stu-id="2158f-191">To run the notebook, open it in Azure Machine Learning Workbench, click on "Start Notebook Server" if this option is shown, change to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local"), and then execute all cells in the notebook.</span></span> <span data-ttu-id="2158f-192">See the troubleshooting section in this document if you get an error complaining that the notebook is too large to be displayed.</span><span class="sxs-lookup"><span data-stu-id="2158f-192">See the troubleshooting section in this document if you get an error complaining that the notebook is too large to be displayed.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showImages.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="2158f-193">Now execute the script named `1_prepareData.py`, which assigns all images to either the training set or the test set.</span><span class="sxs-lookup"><span data-stu-id="2158f-193">Now execute the script named `1_prepareData.py`, which assigns all images to either the training set or the test set.</span></span> <span data-ttu-id="2158f-194">This assignment is mutually exclusive - no training image is also used for testing or vice versa.</span><span class="sxs-lookup"><span data-stu-id="2158f-194">This assignment is mutually exclusive - no training image is also used for testing or vice versa.</span></span> <span data-ttu-id="2158f-195">By default, a random 75% of the images from each attribute class are assigned to training, and the remaining 25% are assigned to testing.</span><span class="sxs-lookup"><span data-stu-id="2158f-195">By default, a random 75% of the images from each attribute class are assigned to training, and the remaining 25% are assigned to testing.</span></span> <span data-ttu-id="2158f-196">All data generated by the script are saved in the *DATA_DIR/proc/fashionTexture/* folder.</span><span class="sxs-lookup"><span data-stu-id="2158f-196">All data generated by the script are saved in the *DATA_DIR/proc/fashionTexture/* folder.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_1_white.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-2-refining-the-deep-neural-network"></a><span data-ttu-id="2158f-197">Step 2: Refining the Deep Neural Network</span><span class="sxs-lookup"><span data-stu-id="2158f-197">Step 2: Refining the Deep Neural Network</span></span>
`Script: 2_refineDNN.py`

<span data-ttu-id="2158f-198">As we explained in part 1 of this tutorial, the pre-trained DNN is kept fixed (that is, it is not refined).</span><span class="sxs-lookup"><span data-stu-id="2158f-198">As we explained in part 1 of this tutorial, the pre-trained DNN is kept fixed (that is, it is not refined).</span></span> <span data-ttu-id="2158f-199">However, the script named `2_refineDNN.py` is still executed in part 1, as it loads a pre-trained [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [2] model and modifies it, for example, to allow for higher input image resolution.</span><span class="sxs-lookup"><span data-stu-id="2158f-199">However, the script named `2_refineDNN.py` is still executed in part 1, as it loads a pre-trained [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [2] model and modifies it, for example, to allow for higher input image resolution.</span></span> <span data-ttu-id="2158f-200">This step is fast (seconds) and does not require a GPU.</span><span class="sxs-lookup"><span data-stu-id="2158f-200">This step is fast (seconds) and does not require a GPU.</span></span>

<span data-ttu-id="2158f-201">In part 2 of the tutorial, a modification to the PARAMETERS.py file causes the `2_refineDNN.py` script to also refine the pre-trained DNN.</span><span class="sxs-lookup"><span data-stu-id="2158f-201">In part 2 of the tutorial, a modification to the PARAMETERS.py file causes the `2_refineDNN.py` script to also refine the pre-trained DNN.</span></span> <span data-ttu-id="2158f-202">By default, we run 45 training epochs during refinement.</span><span class="sxs-lookup"><span data-stu-id="2158f-202">By default, we run 45 training epochs during refinement.</span></span>

<span data-ttu-id="2158f-203">In both cases, the final model is then written to the file *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span><span class="sxs-lookup"><span data-stu-id="2158f-203">In both cases, the final model is then written to the file *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span></span>

### <a name="step-3-evaluate-dnn-for-all-images"></a><span data-ttu-id="2158f-204">Step 3: Evaluate DNN for all images</span><span class="sxs-lookup"><span data-stu-id="2158f-204">Step 3: Evaluate DNN for all images</span></span>
`Script: 3_runDNN.py`

<span data-ttu-id="2158f-205">We can now use the (possibly refined) DNN from the last step to featurize our images.</span><span class="sxs-lookup"><span data-stu-id="2158f-205">We can now use the (possibly refined) DNN from the last step to featurize our images.</span></span> <span data-ttu-id="2158f-206">Given an image as input to the DNN, the output is the 512-floats vector from the penultimate layer of the model.</span><span class="sxs-lookup"><span data-stu-id="2158f-206">Given an image as input to the DNN, the output is the 512-floats vector from the penultimate layer of the model.</span></span> <span data-ttu-id="2158f-207">This vector is much smaller dimensional than the image itself.</span><span class="sxs-lookup"><span data-stu-id="2158f-207">This vector is much smaller dimensional than the image itself.</span></span> <span data-ttu-id="2158f-208">Nevertheless, it should contain (and even highlight) all information in the image relevant to recognize the image's attribute, that is, if the clothing item has a dotted, striped, or leopard texture.</span><span class="sxs-lookup"><span data-stu-id="2158f-208">Nevertheless, it should contain (and even highlight) all information in the image relevant to recognize the image's attribute, that is, if the clothing item has a dotted, striped, or leopard texture.</span></span>

<span data-ttu-id="2158f-209">All of the DNN image representations are saved to the file *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span><span class="sxs-lookup"><span data-stu-id="2158f-209">All of the DNN image representations are saved to the file *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_4_white.jpg" alt="alt text" width="700"/>
</p>


### <a name="step-4-support-vector-machine-training"></a><span data-ttu-id="2158f-210">Step 4: Support Vector Machine training</span><span class="sxs-lookup"><span data-stu-id="2158f-210">Step 4: Support Vector Machine training</span></span>
`Script: 4_trainSVM.py`

<span data-ttu-id="2158f-211">The 512-floats representations computed in the last step are now used to train an SVM classifier: given an image as input, the SVM outputs a score for each attribute to be present.</span><span class="sxs-lookup"><span data-stu-id="2158f-211">The 512-floats representations computed in the last step are now used to train an SVM classifier: given an image as input, the SVM outputs a score for each attribute to be present.</span></span> <span data-ttu-id="2158f-212">In our example dataset, this means a score for 'striped', for 'dotted', and for 'leopard'.</span><span class="sxs-lookup"><span data-stu-id="2158f-212">In our example dataset, this means a score for 'striped', for 'dotted', and for 'leopard'.</span></span>

<span data-ttu-id="2158f-213">Script `4_trainSVM.py` loads the training images, trains an SVM for different values of the regularization (slack) parameter C, and keeps the SVM with highest accuracy.</span><span class="sxs-lookup"><span data-stu-id="2158f-213">Script `4_trainSVM.py` loads the training images, trains an SVM for different values of the regularization (slack) parameter C, and keeps the SVM with highest accuracy.</span></span> <span data-ttu-id="2158f-214">The classification accuracy is printed on the console and plotted in the Workbench.</span><span class="sxs-lookup"><span data-stu-id="2158f-214">The classification accuracy is printed on the console and plotted in the Workbench.</span></span> <span data-ttu-id="2158f-215">For the provided texture data these values should be around 100% and 88% respectively.</span><span class="sxs-lookup"><span data-stu-id="2158f-215">For the provided texture data these values should be around 100% and 88% respectively.</span></span> <span data-ttu-id="2158f-216">Finally, the trained SVM is written to the file *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span><span class="sxs-lookup"><span data-stu-id="2158f-216">Finally, the trained SVM is written to the file *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/vienna_svm_log_zoom.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-5-evaluation-and-visualization"></a><span data-ttu-id="2158f-217">Step 5: Evaluation and visualization</span><span class="sxs-lookup"><span data-stu-id="2158f-217">Step 5: Evaluation and visualization</span></span>
`Script: 5_evaluate.py. Notebook: showResults.ipynb`

<span data-ttu-id="2158f-218">The accuracy of the trained image classifier can be measured using the script `5_evaluate.py`.</span><span class="sxs-lookup"><span data-stu-id="2158f-218">The accuracy of the trained image classifier can be measured using the script `5_evaluate.py`.</span></span> <span data-ttu-id="2158f-219">The script scores all test images using the trained SVM classifier, assigns each image the attribute with the highest score, and compares the predicted attributes with the ground truth annotations.</span><span class="sxs-lookup"><span data-stu-id="2158f-219">The script scores all test images using the trained SVM classifier, assigns each image the attribute with the highest score, and compares the predicted attributes with the ground truth annotations.</span></span>

<span data-ttu-id="2158f-220">The output of script `5_evaluate.py` is shown below.</span><span class="sxs-lookup"><span data-stu-id="2158f-220">The output of script `5_evaluate.py` is shown below.</span></span> <span data-ttu-id="2158f-221">The classification accuracy of each individual class is computed, as well as the accuracy for the full test set ('overall accuracy'), and the average over the individual accuracies ('overall class-averaged accuracy').</span><span class="sxs-lookup"><span data-stu-id="2158f-221">The classification accuracy of each individual class is computed, as well as the accuracy for the full test set ('overall accuracy'), and the average over the individual accuracies ('overall class-averaged accuracy').</span></span> <span data-ttu-id="2158f-222">100% corresponds to the best possible accuracy and 0% to the worst.</span><span class="sxs-lookup"><span data-stu-id="2158f-222">100% corresponds to the best possible accuracy and 0% to the worst.</span></span> <span data-ttu-id="2158f-223">Random guessing would on average produce a class-averaged accuracy of 1 over the number of attributes: in our case, this accuracy would be 33.33%.</span><span class="sxs-lookup"><span data-stu-id="2158f-223">Random guessing would on average produce a class-averaged accuracy of 1 over the number of attributes: in our case, this accuracy would be 33.33%.</span></span> <span data-ttu-id="2158f-224">These results improve significantly when using a higher input resolution such as `rf_inputResoluton = 1000`, however at the expense of longer DNN computation times.</span><span class="sxs-lookup"><span data-stu-id="2158f-224">These results improve significantly when using a higher input resolution such as `rf_inputResoluton = 1000`, however at the expense of longer DNN computation times.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_6_white.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="2158f-225">In addition to accuracy, the ROC curve is plotted with respective area-under-curve (left); and the confusion matrix is shown (right):</span><span class="sxs-lookup"><span data-stu-id="2158f-225">In addition to accuracy, the ROC curve is plotted with respective area-under-curve (left); and the confusion matrix is shown (right):</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="2158f-226">Finally, the notebook `showResults.py` is provided to scroll through the test images and visualize their respective classification scores.</span><span class="sxs-lookup"><span data-stu-id="2158f-226">Finally, the notebook `showResults.py` is provided to scroll through the test images and visualize their respective classification scores.</span></span> <span data-ttu-id="2158f-227">As explained in step1, every notebook in this sample needs to use the local project kernel with name "PROJECTNAME local":</span><span class="sxs-lookup"><span data-stu-id="2158f-227">As explained in step1, every notebook in this sample needs to use the local project kernel with name "PROJECTNAME local":</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showResults.jpg" alt="alt text" width="700"/>
</p>





### <a name="step-6-deployment"></a><span data-ttu-id="2158f-228">Step 6: Deployment</span><span class="sxs-lookup"><span data-stu-id="2158f-228">Step 6: Deployment</span></span>
`Scripts: 6_callWebservice.py, deploymain.py. Notebook: deploy.ipynb`

<span data-ttu-id="2158f-229">The trained system can now be published as a REST API.</span><span class="sxs-lookup"><span data-stu-id="2158f-229">The trained system can now be published as a REST API.</span></span> <span data-ttu-id="2158f-230">Deployment is explained in the notebook `deploy.ipynb`, and based on functionality within the Azure Machine Learning Workbench (remember to set as kernel the local project kernel with name "PROJECTNAME local").</span><span class="sxs-lookup"><span data-stu-id="2158f-230">Deployment is explained in the notebook `deploy.ipynb`, and based on functionality within the Azure Machine Learning Workbench (remember to set as kernel the local project kernel with name "PROJECTNAME local").</span></span> <span data-ttu-id="2158f-231">See also the excellent deployment section of the [IRIS tutorial](tutorial-classifying-iris-part-3.md) for more deployment related information.</span><span class="sxs-lookup"><span data-stu-id="2158f-231">See also the excellent deployment section of the [IRIS tutorial](tutorial-classifying-iris-part-3.md) for more deployment related information.</span></span>

<span data-ttu-id="2158f-232">Once deployed, the web service can be called using the script `6_callWebservice.py`.</span><span class="sxs-lookup"><span data-stu-id="2158f-232">Once deployed, the web service can be called using the script `6_callWebservice.py`.</span></span> <span data-ttu-id="2158f-233">Note that the IP address (either local or on the cloud) of the web service needs to be set first in the script.</span><span class="sxs-lookup"><span data-stu-id="2158f-233">Note that the IP address (either local or on the cloud) of the web service needs to be set first in the script.</span></span> <span data-ttu-id="2158f-234">The notebook `deploy.ipynb` explains how to find this IP address.</span><span class="sxs-lookup"><span data-stu-id="2158f-234">The notebook `deploy.ipynb` explains how to find this IP address.</span></span>








## <a name="part-2---accuracy-improvements"></a><span data-ttu-id="2158f-235">Part 2 - Accuracy improvements</span><span class="sxs-lookup"><span data-stu-id="2158f-235">Part 2 - Accuracy improvements</span></span>

<span data-ttu-id="2158f-236">In part 1, we showed how to classify an image by training a linear Support Vector Machine on the 512-floats output of a Deep Neural Network.</span><span class="sxs-lookup"><span data-stu-id="2158f-236">In part 1, we showed how to classify an image by training a linear Support Vector Machine on the 512-floats output of a Deep Neural Network.</span></span> <span data-ttu-id="2158f-237">This DNN was pre-trained on millions of images, and the penultimate layer returned as feature vector.</span><span class="sxs-lookup"><span data-stu-id="2158f-237">This DNN was pre-trained on millions of images, and the penultimate layer returned as feature vector.</span></span> <span data-ttu-id="2158f-238">This approach is fast since the DNN is used as-is, but nevertheless often gives good results.</span><span class="sxs-lookup"><span data-stu-id="2158f-238">This approach is fast since the DNN is used as-is, but nevertheless often gives good results.</span></span>

<span data-ttu-id="2158f-239">We now present several ways to improve the accuracy of the model from part 1.</span><span class="sxs-lookup"><span data-stu-id="2158f-239">We now present several ways to improve the accuracy of the model from part 1.</span></span> <span data-ttu-id="2158f-240">Most notably we refine the DNN rather than keeping it fixed.</span><span class="sxs-lookup"><span data-stu-id="2158f-240">Most notably we refine the DNN rather than keeping it fixed.</span></span>

### <a name="dnn-refinement"></a><span data-ttu-id="2158f-241">DNN refinement</span><span class="sxs-lookup"><span data-stu-id="2158f-241">DNN refinement</span></span>

<span data-ttu-id="2158f-242">Instead of an SVM, one can do the classification directly in the neural network.</span><span class="sxs-lookup"><span data-stu-id="2158f-242">Instead of an SVM, one can do the classification directly in the neural network.</span></span> <span data-ttu-id="2158f-243">This is achieved by adding a new last layer to the pre-trained DNN, which takes the 512-floats from the penultimate layer as input.</span><span class="sxs-lookup"><span data-stu-id="2158f-243">This is achieved by adding a new last layer to the pre-trained DNN, which takes the 512-floats from the penultimate layer as input.</span></span> <span data-ttu-id="2158f-244">The advantage of doing the classification in the DNN is that now the full network can be retrained using backpropagation.</span><span class="sxs-lookup"><span data-stu-id="2158f-244">The advantage of doing the classification in the DNN is that now the full network can be retrained using backpropagation.</span></span> <span data-ttu-id="2158f-245">This approach often leads to much better classification accuracies compared to using the pre-trained DNN as-is, however at the expense of much longer training time (even with GPU).</span><span class="sxs-lookup"><span data-stu-id="2158f-245">This approach often leads to much better classification accuracies compared to using the pre-trained DNN as-is, however at the expense of much longer training time (even with GPU).</span></span>

<span data-ttu-id="2158f-246">Training the Neural Network instead of an SVM is done by changing the variable `classifier` in `PARAMETERS.py` from `svm` to `dnn`.</span><span class="sxs-lookup"><span data-stu-id="2158f-246">Training the Neural Network instead of an SVM is done by changing the variable `classifier` in `PARAMETERS.py` from `svm` to `dnn`.</span></span> <span data-ttu-id="2158f-247">Then, as described in part 1, all the scripts except for data preparation (step 1) and SVM training (step 4) need to be executed again.</span><span class="sxs-lookup"><span data-stu-id="2158f-247">Then, as described in part 1, all the scripts except for data preparation (step 1) and SVM training (step 4) need to be executed again.</span></span> <span data-ttu-id="2158f-248">DNN refinement requires a GPU.</span><span class="sxs-lookup"><span data-stu-id="2158f-248">DNN refinement requires a GPU.</span></span> <span data-ttu-id="2158f-249">if no GPU was found or if the GPU is locked (for example by a previous CNTK run) then script `2_refineDNN.py` throws an error.</span><span class="sxs-lookup"><span data-stu-id="2158f-249">if no GPU was found or if the GPU is locked (for example by a previous CNTK run) then script `2_refineDNN.py` throws an error.</span></span> <span data-ttu-id="2158f-250">DNN training can throw out-of-memory error on some GPUs, which can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="2158f-250">DNN training can throw out-of-memory error on some GPUs, which can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span></span>

<span data-ttu-id="2158f-251">Once training completes, the refined model is saved to *DATA_DIR/proc/fashionTexture/cntk_refined.model*, and a plot drawn which shows how the training and test classification errors change during training.</span><span class="sxs-lookup"><span data-stu-id="2158f-251">Once training completes, the refined model is saved to *DATA_DIR/proc/fashionTexture/cntk_refined.model*, and a plot drawn which shows how the training and test classification errors change during training.</span></span> <span data-ttu-id="2158f-252">Note in that plot that the error on the training set is much smaller than on the test set.</span><span class="sxs-lookup"><span data-stu-id="2158f-252">Note in that plot that the error on the training set is much smaller than on the test set.</span></span> <span data-ttu-id="2158f-253">This so-called over-fitting behavior can be reduced, for example,  by using a higher value for the dropout rate `rf_dropoutRate`.</span><span class="sxs-lookup"><span data-stu-id="2158f-253">This so-called over-fitting behavior can be reduced, for example,  by using a higher value for the dropout rate `rf_dropoutRate`.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_3_plot.png" alt="alt text" height="300"/>
</p>

<span data-ttu-id="2158f-254">As can be seen in the plot below, the accuracy using DNN refinement on the provided dataset is 92.35% versus the 88.92% before (part 1).</span><span class="sxs-lookup"><span data-stu-id="2158f-254">As can be seen in the plot below, the accuracy using DNN refinement on the provided dataset is 92.35% versus the 88.92% before (part 1).</span></span> <span data-ttu-id="2158f-255">In particular, the 'dotted' images improve significantly, with an ROC area-under-curve of 0.98 with refinement vs. 0.94 before.</span><span class="sxs-lookup"><span data-stu-id="2158f-255">In particular, the 'dotted' images improve significantly, with an ROC area-under-curve of 0.98 with refinement vs. 0.94 before.</span></span> <span data-ttu-id="2158f-256">We are using a small dataset, and hence the actual accuracies running the code are different.</span><span class="sxs-lookup"><span data-stu-id="2158f-256">We are using a small dataset, and hence the actual accuracies running the code are different.</span></span> <span data-ttu-id="2158f-257">This discrepancy is due to stochastic effects such as the random split of the images into training and testing sets.</span><span class="sxs-lookup"><span data-stu-id="2158f-257">This discrepancy is due to stochastic effects such as the random split of the images into training and testing sets.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat_dnn.jpg" alt="alt text" width="700"/>
</p>

### <a name="run-history-tracking"></a><span data-ttu-id="2158f-258">Run history tracking</span><span class="sxs-lookup"><span data-stu-id="2158f-258">Run history tracking</span></span>

<span data-ttu-id="2158f-259">The Azure Machine Learning Workbench stores the history of each run on Azure to allow comparison of two or more runs that are even weeks apart.</span><span class="sxs-lookup"><span data-stu-id="2158f-259">The Azure Machine Learning Workbench stores the history of each run on Azure to allow comparison of two or more runs that are even weeks apart.</span></span> <span data-ttu-id="2158f-260">This is explained in detail in the [Iris tutorial](tutorial-classifying-iris-part-2.md).</span><span class="sxs-lookup"><span data-stu-id="2158f-260">This is explained in detail in the [Iris tutorial](tutorial-classifying-iris-part-2.md).</span></span> <span data-ttu-id="2158f-261">It is also illustrated in the following screenshots where we compare two runs of the script `5_evaluate.py`, using either DNN refinement that is, `classifier = "dnn"`(run number 148) or SVM training that is, `classifier = "svm"` (run number 150).</span><span class="sxs-lookup"><span data-stu-id="2158f-261">It is also illustrated in the following screenshots where we compare two runs of the script `5_evaluate.py`, using either DNN refinement that is, `classifier = "dnn"`(run number 148) or SVM training that is, `classifier = "svm"` (run number 150).</span></span>

<span data-ttu-id="2158f-262">In the first screenshot, the DNN refinement leads to better accuracies than SVM training for all classes.</span><span class="sxs-lookup"><span data-stu-id="2158f-262">In the first screenshot, the DNN refinement leads to better accuracies than SVM training for all classes.</span></span> <span data-ttu-id="2158f-263">The second screenshot shows all metrics that are being tracked, including what the classifier was.</span><span class="sxs-lookup"><span data-stu-id="2158f-263">The second screenshot shows all metrics that are being tracked, including what the classifier was.</span></span> <span data-ttu-id="2158f-264">This tracking is done in the script `5_evaluate.py` by calling the Azure Machine Learning Workbench logger.</span><span class="sxs-lookup"><span data-stu-id="2158f-264">This tracking is done in the script `5_evaluate.py` by calling the Azure Machine Learning Workbench logger.</span></span> <span data-ttu-id="2158f-265">In addition, the script also saves the ROC curve and confusion matrix to the *outputs* folder.</span><span class="sxs-lookup"><span data-stu-id="2158f-265">In addition, the script also saves the ROC curve and confusion matrix to the *outputs* folder.</span></span> <span data-ttu-id="2158f-266">This *outputs* folder is special in that its content is also tracked by the Workbench history feature and hence the output files can be accessed at any time, regardless of whether local copies have been overwritten.</span><span class="sxs-lookup"><span data-stu-id="2158f-266">This *outputs* folder is special in that its content is also tracked by the Workbench history feature and hence the output files can be accessed at any time, regardless of whether local copies have been overwritten.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/>
</p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p>


### <a name="parameter-tuning"></a><span data-ttu-id="2158f-267">Parameter tuning</span><span class="sxs-lookup"><span data-stu-id="2158f-267">Parameter tuning</span></span>

<span data-ttu-id="2158f-268">As is true for most machine learning projects, getting good results for a new dataset requires careful parameter tuning as well as evaluating different design decisions.</span><span class="sxs-lookup"><span data-stu-id="2158f-268">As is true for most machine learning projects, getting good results for a new dataset requires careful parameter tuning as well as evaluating different design decisions.</span></span> <span data-ttu-id="2158f-269">To help with these tasks, all important parameters are specified, and a short explanation provided, in a single place: the `PARAMETERS.py` file.</span><span class="sxs-lookup"><span data-stu-id="2158f-269">To help with these tasks, all important parameters are specified, and a short explanation provided, in a single place: the `PARAMETERS.py` file.</span></span>

<span data-ttu-id="2158f-270">Some of the most promising avenues for improvements are:</span><span class="sxs-lookup"><span data-stu-id="2158f-270">Some of the most promising avenues for improvements are:</span></span>

- <span data-ttu-id="2158f-271">Data quality: Ensure the training and test sets have high quality.</span><span class="sxs-lookup"><span data-stu-id="2158f-271">Data quality: Ensure the training and test sets have high quality.</span></span> <span data-ttu-id="2158f-272">That is, the images are annotated correctly, ambiguous images removed (for example clothing items with both stripes and dots), and the attributes are mutually exclusive (that is, chosen such that each image belongs to exactly one attribute).</span><span class="sxs-lookup"><span data-stu-id="2158f-272">That is, the images are annotated correctly, ambiguous images removed (for example clothing items with both stripes and dots), and the attributes are mutually exclusive (that is, chosen such that each image belongs to exactly one attribute).</span></span>

- <span data-ttu-id="2158f-273">If the object-of-interest is small in the image then Image classification approaches are known not to work well.</span><span class="sxs-lookup"><span data-stu-id="2158f-273">If the object-of-interest is small in the image then Image classification approaches are known not to work well.</span></span> <span data-ttu-id="2158f-274">In such cases consider using an object detection approach as described in this [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span><span class="sxs-lookup"><span data-stu-id="2158f-274">In such cases consider using an object detection approach as described in this [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span></span>
- <span data-ttu-id="2158f-275">DNN refinement: The arguably most important parameter to get right is the learning rate `rf_lrPerMb`.</span><span class="sxs-lookup"><span data-stu-id="2158f-275">DNN refinement: The arguably most important parameter to get right is the learning rate `rf_lrPerMb`.</span></span> <span data-ttu-id="2158f-276">If the accuracy on the training set (first figure in part 2) is not close to 0-5%, most likely it is due to a wrong the learning rate.</span><span class="sxs-lookup"><span data-stu-id="2158f-276">If the accuracy on the training set (first figure in part 2) is not close to 0-5%, most likely it is due to a wrong the learning rate.</span></span> <span data-ttu-id="2158f-277">The other parameters starting with `rf_` are less important.</span><span class="sxs-lookup"><span data-stu-id="2158f-277">The other parameters starting with `rf_` are less important.</span></span> <span data-ttu-id="2158f-278">Typically, the training error should decrement exponentially and be close to 0% after training.</span><span class="sxs-lookup"><span data-stu-id="2158f-278">Typically, the training error should decrement exponentially and be close to 0% after training.</span></span>

- <span data-ttu-id="2158f-279">Input resolution: The default image resolution is 224x224 pixels.</span><span class="sxs-lookup"><span data-stu-id="2158f-279">Input resolution: The default image resolution is 224x224 pixels.</span></span> <span data-ttu-id="2158f-280">Using higher image resolution (parameter: `rf_inputResoluton`) of, for example, 448x448 or 896x896 pixels often significant improves accuracy but slows down DNN refinement.</span><span class="sxs-lookup"><span data-stu-id="2158f-280">Using higher image resolution (parameter: `rf_inputResoluton`) of, for example, 448x448 or 896x896 pixels often significant improves accuracy but slows down DNN refinement.</span></span> <span data-ttu-id="2158f-281">**Using higher image resolution is nearly free lunch and almost always boosts accuracy**.</span><span class="sxs-lookup"><span data-stu-id="2158f-281">**Using higher image resolution is nearly free lunch and almost always boosts accuracy**.</span></span>

- <span data-ttu-id="2158f-282">DNN over-fitting: Avoid a large gap between the training and test accuracy during DNN refinement (first figure in part 2).</span><span class="sxs-lookup"><span data-stu-id="2158f-282">DNN over-fitting: Avoid a large gap between the training and test accuracy during DNN refinement (first figure in part 2).</span></span> <span data-ttu-id="2158f-283">This gap can be reduced using dropout rates `rf_dropoutRate` of 0.5 or more, and by increasing the regularizer weight `rf_l2RegWeight`.</span><span class="sxs-lookup"><span data-stu-id="2158f-283">This gap can be reduced using dropout rates `rf_dropoutRate` of 0.5 or more, and by increasing the regularizer weight `rf_l2RegWeight`.</span></span> <span data-ttu-id="2158f-284">Using a high dropout rate can be especially helpful if the DNN input image resolution is high.</span><span class="sxs-lookup"><span data-stu-id="2158f-284">Using a high dropout rate can be especially helpful if the DNN input image resolution is high.</span></span>

- <span data-ttu-id="2158f-285">Try using deeper DNNs by changing `rf_pretrainedModelFilename` from `ResNet_18.model` to either `ResNet_34.model` or `ResNet_50.model`.</span><span class="sxs-lookup"><span data-stu-id="2158f-285">Try using deeper DNNs by changing `rf_pretrainedModelFilename` from `ResNet_18.model` to either `ResNet_34.model` or `ResNet_50.model`.</span></span> <span data-ttu-id="2158f-286">The Resnet-50 model is not only deeper, but its output of the penultimate layer is of size 2048 floats (vs. 512 floats of the ResNet-18 and ResNet-34 models).</span><span class="sxs-lookup"><span data-stu-id="2158f-286">The Resnet-50 model is not only deeper, but its output of the penultimate layer is of size 2048 floats (vs. 512 floats of the ResNet-18 and ResNet-34 models).</span></span> <span data-ttu-id="2158f-287">This increased dimension can be especially beneficial when training an SVM classifier.</span><span class="sxs-lookup"><span data-stu-id="2158f-287">This increased dimension can be especially beneficial when training an SVM classifier.</span></span>

## <a name="part-3---custom-dataset"></a><span data-ttu-id="2158f-288">Part 3 - Custom dataset</span><span class="sxs-lookup"><span data-stu-id="2158f-288">Part 3 - Custom dataset</span></span>

<span data-ttu-id="2158f-289">In part 1 and 2, we trained and evaluated an image classification model using the provided upper body clothing textures images.</span><span class="sxs-lookup"><span data-stu-id="2158f-289">In part 1 and 2, we trained and evaluated an image classification model using the provided upper body clothing textures images.</span></span> <span data-ttu-id="2158f-290">We now show how to use a custom user-provided dataset instead.</span><span class="sxs-lookup"><span data-stu-id="2158f-290">We now show how to use a custom user-provided dataset instead.</span></span> <span data-ttu-id="2158f-291">Or, if not available, how to generate and annotate such a dataset using Bing Image Search.</span><span class="sxs-lookup"><span data-stu-id="2158f-291">Or, if not available, how to generate and annotate such a dataset using Bing Image Search.</span></span>

### <a name="using-a-custom-dataset"></a><span data-ttu-id="2158f-292">Using a custom dataset</span><span class="sxs-lookup"><span data-stu-id="2158f-292">Using a custom dataset</span></span>

<span data-ttu-id="2158f-293">First, let's have a look at the folder structure for the clothing texture data.</span><span class="sxs-lookup"><span data-stu-id="2158f-293">First, let's have a look at the folder structure for the clothing texture data.</span></span> <span data-ttu-id="2158f-294">Note how all images for the different attributes are in the respective subfolders *dotted*, \*leopard, and *striped* at *DATA_DIR/images/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="2158f-294">Note how all images for the different attributes are in the respective subfolders *dotted*, \*leopard, and *striped* at *DATA_DIR/images/fashionTexture/*.</span></span> <span data-ttu-id="2158f-295">Note also how the image folder name also occurs in the `PARAMETERS.py` file:</span><span class="sxs-lookup"><span data-stu-id="2158f-295">Note also how the image folder name also occurs in the `PARAMETERS.py` file:</span></span>
```python
datasetName = "fashionTexture"
```

<span data-ttu-id="2158f-296">Using a custom dataset is as simple as reproducing this folder structure where all images are in subfolders according to their attribute, and to copy these subfolders to a new user-specified directory *DATA_DIR/images/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="2158f-296">Using a custom dataset is as simple as reproducing this folder structure where all images are in subfolders according to their attribute, and to copy these subfolders to a new user-specified directory *DATA_DIR/images/newDataSetName/*.</span></span> <span data-ttu-id="2158f-297">The only code change required is to set the `datasetName` variable to *newDataSetName*.</span><span class="sxs-lookup"><span data-stu-id="2158f-297">The only code change required is to set the `datasetName` variable to *newDataSetName*.</span></span> <span data-ttu-id="2158f-298">Scripts 1-5 can then be executed in order, and all intermediate files are written to *DATA_DIR/proc/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="2158f-298">Scripts 1-5 can then be executed in order, and all intermediate files are written to *DATA_DIR/proc/newDataSetName/*.</span></span> <span data-ttu-id="2158f-299">No other code changes are required.</span><span class="sxs-lookup"><span data-stu-id="2158f-299">No other code changes are required.</span></span>

<span data-ttu-id="2158f-300">It is important that each image can be assigned to exactly one attribute.</span><span class="sxs-lookup"><span data-stu-id="2158f-300">It is important that each image can be assigned to exactly one attribute.</span></span> <span data-ttu-id="2158f-301">For example, it would be wrong to have attributes for 'animal' and for 'leopard', since a 'leopard' image would also belong to 'animal'.</span><span class="sxs-lookup"><span data-stu-id="2158f-301">For example, it would be wrong to have attributes for 'animal' and for 'leopard', since a 'leopard' image would also belong to 'animal'.</span></span> <span data-ttu-id="2158f-302">Also, it is best to remove images that are ambiguous and hence difficult to annotate.</span><span class="sxs-lookup"><span data-stu-id="2158f-302">Also, it is best to remove images that are ambiguous and hence difficult to annotate.</span></span>



### <a name="image-scraping-and-annotation"></a><span data-ttu-id="2158f-303">Image scraping and annotation</span><span class="sxs-lookup"><span data-stu-id="2158f-303">Image scraping and annotation</span></span>

<span data-ttu-id="2158f-304">Collecting a sufficiently large number of annotated images for training and testing can be difficult.</span><span class="sxs-lookup"><span data-stu-id="2158f-304">Collecting a sufficiently large number of annotated images for training and testing can be difficult.</span></span> <span data-ttu-id="2158f-305">One way to overcome this problem is to scrape images from the Internet.</span><span class="sxs-lookup"><span data-stu-id="2158f-305">One way to overcome this problem is to scrape images from the Internet.</span></span> <span data-ttu-id="2158f-306">For example, see below the Bing Image Search results for the query *t-shirt striped*.</span><span class="sxs-lookup"><span data-stu-id="2158f-306">For example, see below the Bing Image Search results for the query *t-shirt striped*.</span></span> <span data-ttu-id="2158f-307">As expected, most images indeed are striped t-shirts.</span><span class="sxs-lookup"><span data-stu-id="2158f-307">As expected, most images indeed are striped t-shirts.</span></span> <span data-ttu-id="2158f-308">The few incorrect or ambiguous images (such as column 1, row 1; or column 3, row 2) can be identified and removed easily:</span><span class="sxs-lookup"><span data-stu-id="2158f-308">The few incorrect or ambiguous images (such as column 1, row 1; or column 3, row 2) can be identified and removed easily:</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/bing_search_striped.jpg" alt="alt text" width="600"/>
</p>

<span data-ttu-id="2158f-309">To generate a large and diverse dataset, multiple queries should be used.</span><span class="sxs-lookup"><span data-stu-id="2158f-309">To generate a large and diverse dataset, multiple queries should be used.</span></span> <span data-ttu-id="2158f-310">For example, 7\*3 = 21 queries can be synthesized automatically using all combinations of clothing items {blouse, hoodie, pullover, sweater, shirt, t-shirt, vest} and attributes {striped, dotted, leopard}.</span><span class="sxs-lookup"><span data-stu-id="2158f-310">For example, 7\*3 = 21 queries can be synthesized automatically using all combinations of clothing items {blouse, hoodie, pullover, sweater, shirt, t-shirt, vest} and attributes {striped, dotted, leopard}.</span></span> <span data-ttu-id="2158f-311">Downloading the top 50 images per query would then lead to a maximum of 21\*50=1050 images.</span><span class="sxs-lookup"><span data-stu-id="2158f-311">Downloading the top 50 images per query would then lead to a maximum of 21\*50=1050 images.</span></span>

<span data-ttu-id="2158f-312">Rather than manually downloading images from Bing Image Search, it is much easier to instead use the [Cognitive Services Bing Image Search API](https://www.microsoft.com/cognitive-services/bing-image-search-api) which returns a set of image URLs given a query string.</span><span class="sxs-lookup"><span data-stu-id="2158f-312">Rather than manually downloading images from Bing Image Search, it is much easier to instead use the [Cognitive Services Bing Image Search API](https://www.microsoft.com/cognitive-services/bing-image-search-api) which returns a set of image URLs given a query string.</span></span>

<span data-ttu-id="2158f-313">Some of the downloaded images are exact or near duplicates (for example, differ just by image resolution or jpg artifacts).</span><span class="sxs-lookup"><span data-stu-id="2158f-313">Some of the downloaded images are exact or near duplicates (for example, differ just by image resolution or jpg artifacts).</span></span> <span data-ttu-id="2158f-314">These duplicates should be removed so that the training and test split do not contain the same images.</span><span class="sxs-lookup"><span data-stu-id="2158f-314">These duplicates should be removed so that the training and test split do not contain the same images.</span></span> <span data-ttu-id="2158f-315">Removing duplicate images can be achieved using a hashing-based approach, which works in two steps: (i) first, the hash string is computed for all images; (ii) in a second pass over the images, only those images are kept with a hash string that has not yet been seen.</span><span class="sxs-lookup"><span data-stu-id="2158f-315">Removing duplicate images can be achieved using a hashing-based approach, which works in two steps: (i) first, the hash string is computed for all images; (ii) in a second pass over the images, only those images are kept with a hash string that has not yet been seen.</span></span> <span data-ttu-id="2158f-316">All other images are discarded.</span><span class="sxs-lookup"><span data-stu-id="2158f-316">All other images are discarded.</span></span> <span data-ttu-id="2158f-317">We found the `dhash` approach in the Python library `imagehash` and described in this [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) to perform well, with the parameter `hash_size` set to 16.</span><span class="sxs-lookup"><span data-stu-id="2158f-317">We found the `dhash` approach in the Python library `imagehash` and described in this [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) to perform well, with the parameter `hash_size` set to 16.</span></span> <span data-ttu-id="2158f-318">It is OK to incorrectly remove some non-duplicate images, as long as the majority of the real duplicates get removed.</span><span class="sxs-lookup"><span data-stu-id="2158f-318">It is OK to incorrectly remove some non-duplicate images, as long as the majority of the real duplicates get removed.</span></span>





## <a name="conclusion"></a><span data-ttu-id="2158f-319">Conclusion</span><span class="sxs-lookup"><span data-stu-id="2158f-319">Conclusion</span></span>

<span data-ttu-id="2158f-320">Some key highlights of this example are:</span><span class="sxs-lookup"><span data-stu-id="2158f-320">Some key highlights of this example are:</span></span>
- <span data-ttu-id="2158f-321">Code to train, evaluate, and deploy image classification models.</span><span class="sxs-lookup"><span data-stu-id="2158f-321">Code to train, evaluate, and deploy image classification models.</span></span>
- <span data-ttu-id="2158f-322">Demo images provided, but easily adaptable (one line change) to use own image dataset.</span><span class="sxs-lookup"><span data-stu-id="2158f-322">Demo images provided, but easily adaptable (one line change) to use own image dataset.</span></span>
- <span data-ttu-id="2158f-323">State-of-the-art expert features implemented to train high accuracy models based on Transfer Learning.</span><span class="sxs-lookup"><span data-stu-id="2158f-323">State-of-the-art expert features implemented to train high accuracy models based on Transfer Learning.</span></span>
- <span data-ttu-id="2158f-324">Interactive model development with Azure Machine Learning Workbench and Jupyter Notebook.</span><span class="sxs-lookup"><span data-stu-id="2158f-324">Interactive model development with Azure Machine Learning Workbench and Jupyter Notebook.</span></span>


## <a name="references"></a><span data-ttu-id="2158f-325">References</span><span class="sxs-lookup"><span data-stu-id="2158f-325">References</span></span>

<span data-ttu-id="2158f-326">[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</span><span class="sxs-lookup"><span data-stu-id="2158f-326">[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</span></span> <span data-ttu-id="2158f-327">NIPS 2012.</span><span class="sxs-lookup"><span data-stu-id="2158f-327">NIPS 2012.</span></span>  
<span data-ttu-id="2158f-328">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, [_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf).</span><span class="sxs-lookup"><span data-stu-id="2158f-328">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, [_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf).</span></span> <span data-ttu-id="2158f-329">CVPR 2016.</span><span class="sxs-lookup"><span data-stu-id="2158f-329">CVPR 2016.</span></span>
