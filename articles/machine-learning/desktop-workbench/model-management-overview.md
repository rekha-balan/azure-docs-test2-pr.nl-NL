---
title: Conceptual overview of Azure Machine Learning Model Management | Microsoft Docs
description: This document explains Model Management concepts for Azure Machine Learning.
services: machine-learning
author: hjerezmsft
ms.author: hjerez
ms.service: machine-learning
ms.component: core
ms.workload: data-services
ms.topic: article
ms.date: 09/20/2017
ms.openlocfilehash: 041f7147f171514d941555ff2f6144bac2062b06
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44857865"
---
# <a name="azure-machine-learning-model-management"></a><span data-ttu-id="ae639-103">Azure Machine Learning Model Management</span><span class="sxs-lookup"><span data-stu-id="ae639-103">Azure Machine Learning Model Management</span></span>

<span data-ttu-id="ae639-104">Azure Machine Learning Model Management enables you to manage and deploy machine-learning workflows and models.</span><span class="sxs-lookup"><span data-stu-id="ae639-104">Azure Machine Learning Model Management enables you to manage and deploy machine-learning workflows and models.</span></span> 

<span data-ttu-id="ae639-105">Model Management provides capabilities for:</span><span class="sxs-lookup"><span data-stu-id="ae639-105">Model Management provides capabilities for:</span></span>
- <span data-ttu-id="ae639-106">Model versioning</span><span class="sxs-lookup"><span data-stu-id="ae639-106">Model versioning</span></span>
- <span data-ttu-id="ae639-107">Tracking models in production</span><span class="sxs-lookup"><span data-stu-id="ae639-107">Tracking models in production</span></span>
- <span data-ttu-id="ae639-108">Deploying models to production through AzureML Compute Environment with [Azure Container Service](https://azure.microsoft.com/services/container-service/) and [Kubernetes](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-kubernetes-walkthrough)</span><span class="sxs-lookup"><span data-stu-id="ae639-108">Deploying models to production through AzureML Compute Environment with [Azure Container Service](https://azure.microsoft.com/services/container-service/) and [Kubernetes](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-kubernetes-walkthrough)</span></span>
- <span data-ttu-id="ae639-109">Creating Docker containers with the models and testing them locally</span><span class="sxs-lookup"><span data-stu-id="ae639-109">Creating Docker containers with the models and testing them locally</span></span>
- <span data-ttu-id="ae639-110">Automated model retraining</span><span class="sxs-lookup"><span data-stu-id="ae639-110">Automated model retraining</span></span>
- <span data-ttu-id="ae639-111">Capturing model telemetry for actionable insights.</span><span class="sxs-lookup"><span data-stu-id="ae639-111">Capturing model telemetry for actionable insights.</span></span> 

<span data-ttu-id="ae639-112">Azure Machine Learning Model Management provides a registry of model versions.</span><span class="sxs-lookup"><span data-stu-id="ae639-112">Azure Machine Learning Model Management provides a registry of model versions.</span></span> <span data-ttu-id="ae639-113">It also provides automated workflows for packaging and deploying Machine Learning containers as REST APIs.</span><span class="sxs-lookup"><span data-stu-id="ae639-113">It also provides automated workflows for packaging and deploying Machine Learning containers as REST APIs.</span></span> <span data-ttu-id="ae639-114">The models and their runtime dependencies are packaged in Linux-based Docker container with prediction API.</span><span class="sxs-lookup"><span data-stu-id="ae639-114">The models and their runtime dependencies are packaged in Linux-based Docker container with prediction API.</span></span> 

<span data-ttu-id="ae639-115">Azure Machine Learning Compute Environments help to set up and manage scalable clusters for hosting the models.</span><span class="sxs-lookup"><span data-stu-id="ae639-115">Azure Machine Learning Compute Environments help to set up and manage scalable clusters for hosting the models.</span></span> <span data-ttu-id="ae639-116">The compute environment is based on Azure Container Services.</span><span class="sxs-lookup"><span data-stu-id="ae639-116">The compute environment is based on Azure Container Services.</span></span> <span data-ttu-id="ae639-117">Azure Container Services provides automatic exposure of Machine Learning APIs as REST API endpoints with the following features:</span><span class="sxs-lookup"><span data-stu-id="ae639-117">Azure Container Services provides automatic exposure of Machine Learning APIs as REST API endpoints with the following features:</span></span>

- <span data-ttu-id="ae639-118">Authentication</span><span class="sxs-lookup"><span data-stu-id="ae639-118">Authentication</span></span>
- <span data-ttu-id="ae639-119">Load balancing</span><span class="sxs-lookup"><span data-stu-id="ae639-119">Load balancing</span></span>
- <span data-ttu-id="ae639-120">Automatic scale-out</span><span class="sxs-lookup"><span data-stu-id="ae639-120">Automatic scale-out</span></span>
- <span data-ttu-id="ae639-121">Encryption</span><span class="sxs-lookup"><span data-stu-id="ae639-121">Encryption</span></span>

<span data-ttu-id="ae639-122">Azure Machine Learning Model Management provides these capabilities through the CLI, API, and the Azure portal.</span><span class="sxs-lookup"><span data-stu-id="ae639-122">Azure Machine Learning Model Management provides these capabilities through the CLI, API, and the Azure portal.</span></span> 

<span data-ttu-id="ae639-123">Azure Machine Learning model management uses the following information:</span><span class="sxs-lookup"><span data-stu-id="ae639-123">Azure Machine Learning model management uses the following information:</span></span>

 - <span data-ttu-id="ae639-124">Model file or a directory with the model files</span><span class="sxs-lookup"><span data-stu-id="ae639-124">Model file or a directory with the model files</span></span>
 - <span data-ttu-id="ae639-125">User created Python file implementing a model scoring function</span><span class="sxs-lookup"><span data-stu-id="ae639-125">User created Python file implementing a model scoring function</span></span>
 - <span data-ttu-id="ae639-126">Conda dependency file listing runtime dependencies</span><span class="sxs-lookup"><span data-stu-id="ae639-126">Conda dependency file listing runtime dependencies</span></span>
 - <span data-ttu-id="ae639-127">Runtime environment choice, and</span><span class="sxs-lookup"><span data-stu-id="ae639-127">Runtime environment choice, and</span></span> 
 - <span data-ttu-id="ae639-128">Schema file for API parameters</span><span class="sxs-lookup"><span data-stu-id="ae639-128">Schema file for API parameters</span></span> 

<span data-ttu-id="ae639-129">This information is used when performing the following actions:</span><span class="sxs-lookup"><span data-stu-id="ae639-129">This information is used when performing the following actions:</span></span>

- <span data-ttu-id="ae639-130">Registering a model</span><span class="sxs-lookup"><span data-stu-id="ae639-130">Registering a model</span></span>
- <span data-ttu-id="ae639-131">Creating a manifest that is used when building a container</span><span class="sxs-lookup"><span data-stu-id="ae639-131">Creating a manifest that is used when building a container</span></span>
- <span data-ttu-id="ae639-132">Building a Docker container image</span><span class="sxs-lookup"><span data-stu-id="ae639-132">Building a Docker container image</span></span>
- <span data-ttu-id="ae639-133">Deploying a container to Azure Container Service</span><span class="sxs-lookup"><span data-stu-id="ae639-133">Deploying a container to Azure Container Service</span></span>
 
<span data-ttu-id="ae639-134">The following figure shows an overview of how models are registered and deployed into the cluster.</span><span class="sxs-lookup"><span data-stu-id="ae639-134">The following figure shows an overview of how models are registered and deployed into the cluster.</span></span> 

![](media/model-management-overview/modelmanagement.png)

## <a name="create-and-manage-models"></a><span data-ttu-id="ae639-135">Create and manage models</span><span class="sxs-lookup"><span data-stu-id="ae639-135">Create and manage models</span></span> 
<span data-ttu-id="ae639-136">You can register models with Azure Machine Learning Model Management for tracking model versions in production.</span><span class="sxs-lookup"><span data-stu-id="ae639-136">You can register models with Azure Machine Learning Model Management for tracking model versions in production.</span></span> <span data-ttu-id="ae639-137">For ease of reproducibility and governance, the service captures all dependencies and associated information.</span><span class="sxs-lookup"><span data-stu-id="ae639-137">For ease of reproducibility and governance, the service captures all dependencies and associated information.</span></span> <span data-ttu-id="ae639-138">For deeper insights into performance, you can capture model telemetry using the provided SDK.</span><span class="sxs-lookup"><span data-stu-id="ae639-138">For deeper insights into performance, you can capture model telemetry using the provided SDK.</span></span> <span data-ttu-id="ae639-139">Model telemetry is archived in user-provided storage.</span><span class="sxs-lookup"><span data-stu-id="ae639-139">Model telemetry is archived in user-provided storage.</span></span> <span data-ttu-id="ae639-140">The model telemetry can be used later for analyzing model performance, retraining, and gaining insights for your business.</span><span class="sxs-lookup"><span data-stu-id="ae639-140">The model telemetry can be used later for analyzing model performance, retraining, and gaining insights for your business.</span></span>

## <a name="create-and-manage-manifests"></a><span data-ttu-id="ae639-141">Create and manage manifests</span><span class="sxs-lookup"><span data-stu-id="ae639-141">Create and manage manifests</span></span> 
<span data-ttu-id="ae639-142">Models require additional artifacts to deploy into  production.</span><span class="sxs-lookup"><span data-stu-id="ae639-142">Models require additional artifacts to deploy into  production.</span></span> <span data-ttu-id="ae639-143">The system provides the capability to create a manifest that encompasses model, dependencies, inference script (aka scoring script), sample data, schema etc. This manifest acts as a recipe to create a Docker container image.</span><span class="sxs-lookup"><span data-stu-id="ae639-143">The system provides the capability to create a manifest that encompasses model, dependencies, inference script (aka scoring script), sample data, schema etc. This manifest acts as a recipe to create a Docker container image.</span></span> <span data-ttu-id="ae639-144">Enterprises can auto-generate manifest, create different versions, and manage their manifests.</span><span class="sxs-lookup"><span data-stu-id="ae639-144">Enterprises can auto-generate manifest, create different versions, and manage their manifests.</span></span> 

## <a name="create-and-manage-docker-container-images"></a><span data-ttu-id="ae639-145">Create and manage Docker container images</span><span class="sxs-lookup"><span data-stu-id="ae639-145">Create and manage Docker container images</span></span> 
<span data-ttu-id="ae639-146">You can use the manifest from the previous step to build Docker-based container images in their respective environments.</span><span class="sxs-lookup"><span data-stu-id="ae639-146">You can use the manifest from the previous step to build Docker-based container images in their respective environments.</span></span> <span data-ttu-id="ae639-147">The containerized, Docker-based images provide enterprises with the flexibility to run these images on the following compute environments:</span><span class="sxs-lookup"><span data-stu-id="ae639-147">The containerized, Docker-based images provide enterprises with the flexibility to run these images on the following compute environments:</span></span>

- [<span data-ttu-id="ae639-148">Kubernetes based Azure Container Service</span><span class="sxs-lookup"><span data-stu-id="ae639-148">Kubernetes based Azure Container Service</span></span>](https://docs.microsoft.com/azure/container-service/kubernetes/container-service-kubernetes-walkthrough)
- <span data-ttu-id="ae639-149">On-premises container services</span><span class="sxs-lookup"><span data-stu-id="ae639-149">On-premises container services</span></span>
- <span data-ttu-id="ae639-150">Development environments</span><span class="sxs-lookup"><span data-stu-id="ae639-150">Development environments</span></span>
- <span data-ttu-id="ae639-151">IoT devices</span><span class="sxs-lookup"><span data-stu-id="ae639-151">IoT devices</span></span>

<span data-ttu-id="ae639-152">These Docker-based containerized images are self-contained with all necessary dependencies required for generating predictions.</span><span class="sxs-lookup"><span data-stu-id="ae639-152">These Docker-based containerized images are self-contained with all necessary dependencies required for generating predictions.</span></span> 

## <a name="deploy-docker-container-images"></a><span data-ttu-id="ae639-153">Deploy Docker container images</span><span class="sxs-lookup"><span data-stu-id="ae639-153">Deploy Docker container images</span></span> 
<span data-ttu-id="ae639-154">With the Azure Machine Learning Model Management, you can deploy Docker-based container images with a single command to Azure Container Service managed by ML Compute Environment.</span><span class="sxs-lookup"><span data-stu-id="ae639-154">With the Azure Machine Learning Model Management, you can deploy Docker-based container images with a single command to Azure Container Service managed by ML Compute Environment.</span></span> <span data-ttu-id="ae639-155">These deployments are created with a front-end server that provides the following features:</span><span class="sxs-lookup"><span data-stu-id="ae639-155">These deployments are created with a front-end server that provides the following features:</span></span>

- <span data-ttu-id="ae639-156">Low latency predictions at scale</span><span class="sxs-lookup"><span data-stu-id="ae639-156">Low latency predictions at scale</span></span>
- <span data-ttu-id="ae639-157">Load balancing</span><span class="sxs-lookup"><span data-stu-id="ae639-157">Load balancing</span></span>
- <span data-ttu-id="ae639-158">Automatic scaling of ML endpoints</span><span class="sxs-lookup"><span data-stu-id="ae639-158">Automatic scaling of ML endpoints</span></span>
- <span data-ttu-id="ae639-159">API key authorization</span><span class="sxs-lookup"><span data-stu-id="ae639-159">API key authorization</span></span>
- <span data-ttu-id="ae639-160">API swagger document</span><span class="sxs-lookup"><span data-stu-id="ae639-160">API swagger document</span></span>

<span data-ttu-id="ae639-161">You can control the deployment scale and telemetry through the following configuration settings:</span><span class="sxs-lookup"><span data-stu-id="ae639-161">You can control the deployment scale and telemetry through the following configuration settings:</span></span>

- <span data-ttu-id="ae639-162">System logging and model telemetry for each web service level.</span><span class="sxs-lookup"><span data-stu-id="ae639-162">System logging and model telemetry for each web service level.</span></span> <span data-ttu-id="ae639-163">If enabled, all stdout logs are streamed to [Azure Application Insights](https://azure.microsoft.com/services/application-insights/).</span><span class="sxs-lookup"><span data-stu-id="ae639-163">If enabled, all stdout logs are streamed to [Azure Application Insights](https://azure.microsoft.com/services/application-insights/).</span></span> <span data-ttu-id="ae639-164">Model telemetry is archived in storage that you provide.</span><span class="sxs-lookup"><span data-stu-id="ae639-164">Model telemetry is archived in storage that you provide.</span></span> 
- <span data-ttu-id="ae639-165">Auto-scale and concurrency limits.</span><span class="sxs-lookup"><span data-stu-id="ae639-165">Auto-scale and concurrency limits.</span></span> <span data-ttu-id="ae639-166">These settings automatically increase the number of deployed containers based on the load within the existing cluster.</span><span class="sxs-lookup"><span data-stu-id="ae639-166">These settings automatically increase the number of deployed containers based on the load within the existing cluster.</span></span> <span data-ttu-id="ae639-167">They also control the throughput and consistency of prediction latency.</span><span class="sxs-lookup"><span data-stu-id="ae639-167">They also control the throughput and consistency of prediction latency.</span></span>

## <a name="consumption"></a><span data-ttu-id="ae639-168">Consumption</span><span class="sxs-lookup"><span data-stu-id="ae639-168">Consumption</span></span> 
<span data-ttu-id="ae639-169">Azure Machine Learning Model Management creates REST API for the deployed model along with the swagger document.</span><span class="sxs-lookup"><span data-stu-id="ae639-169">Azure Machine Learning Model Management creates REST API for the deployed model along with the swagger document.</span></span> <span data-ttu-id="ae639-170">You can consume deployed models by calling the REST APIs with API key and model inputs to get the predictions as part of the line-of-business applications.</span><span class="sxs-lookup"><span data-stu-id="ae639-170">You can consume deployed models by calling the REST APIs with API key and model inputs to get the predictions as part of the line-of-business applications.</span></span> <span data-ttu-id="ae639-171">The sample code is available in GitHub for languages Java, [Python](https://github.com/CortanaAnalyticsGallery-Int/digit-recognition-cnn-tf/blob/master/client.py), and C# for calling REST APIs.</span><span class="sxs-lookup"><span data-stu-id="ae639-171">The sample code is available in GitHub for languages Java, [Python](https://github.com/CortanaAnalyticsGallery-Int/digit-recognition-cnn-tf/blob/master/client.py), and C# for calling REST APIs.</span></span> <span data-ttu-id="ae639-172">The Azure Machine Learning Model Management CLI provides an easy way to work with these REST APIs.</span><span class="sxs-lookup"><span data-stu-id="ae639-172">The Azure Machine Learning Model Management CLI provides an easy way to work with these REST APIs.</span></span> <span data-ttu-id="ae639-173">You can consume the APIs using a single CLI command, a swagger-enabled applications, or using curl.</span><span class="sxs-lookup"><span data-stu-id="ae639-173">You can consume the APIs using a single CLI command, a swagger-enabled applications, or using curl.</span></span> 

## <a name="retraining"></a><span data-ttu-id="ae639-174">Retraining</span><span class="sxs-lookup"><span data-stu-id="ae639-174">Retraining</span></span> 
<span data-ttu-id="ae639-175">Azure Machine Learning Model Management provides APIs that you can use to retrain your models.</span><span class="sxs-lookup"><span data-stu-id="ae639-175">Azure Machine Learning Model Management provides APIs that you can use to retrain your models.</span></span> <span data-ttu-id="ae639-176">You can also use the APIs to update existing deployments with updated versions of the model.</span><span class="sxs-lookup"><span data-stu-id="ae639-176">You can also use the APIs to update existing deployments with updated versions of the model.</span></span> <span data-ttu-id="ae639-177">As part of the data science workflow, you recreate the model in your experimentation environment.</span><span class="sxs-lookup"><span data-stu-id="ae639-177">As part of the data science workflow, you recreate the model in your experimentation environment.</span></span> <span data-ttu-id="ae639-178">Then, you register the model with Model Management, and update existing deployments.</span><span class="sxs-lookup"><span data-stu-id="ae639-178">Then, you register the model with Model Management, and update existing deployments.</span></span> <span data-ttu-id="ae639-179">Updates are performed using a single UPDATE CLI command.</span><span class="sxs-lookup"><span data-stu-id="ae639-179">Updates are performed using a single UPDATE CLI command.</span></span> <span data-ttu-id="ae639-180">The UPDATE command updates existing deployments without changing the API URL or the key.</span><span class="sxs-lookup"><span data-stu-id="ae639-180">The UPDATE command updates existing deployments without changing the API URL or the key.</span></span> <span data-ttu-id="ae639-181">The applications consuming the model continue to work without any code change, and start getting better predictions using new model.</span><span class="sxs-lookup"><span data-stu-id="ae639-181">The applications consuming the model continue to work without any code change, and start getting better predictions using new model.</span></span>

<span data-ttu-id="ae639-182">The complete workflow describing these concepts is captured in the following figure:</span><span class="sxs-lookup"><span data-stu-id="ae639-182">The complete workflow describing these concepts is captured in the following figure:</span></span>

![](media/model-management-overview/modelmanagementworkflow.png)

## <a name="frequently-asked-questions-faq"></a><span data-ttu-id="ae639-183">Frequently asked questions (FAQ)</span><span class="sxs-lookup"><span data-stu-id="ae639-183">Frequently asked questions (FAQ)</span></span> 
- <span data-ttu-id="ae639-184">**What data types are supported? Can I pass NumPy arrays directly as input to web service?**</span><span class="sxs-lookup"><span data-stu-id="ae639-184">**What data types are supported? Can I pass NumPy arrays directly as input to web service?**</span></span>

   <span data-ttu-id="ae639-185">If you are providing schema file that was created using generate_schema SDK, then you can pass NumPy and/or Pandas DF.</span><span class="sxs-lookup"><span data-stu-id="ae639-185">If you are providing schema file that was created using generate_schema SDK, then you can pass NumPy and/or Pandas DF.</span></span> <span data-ttu-id="ae639-186">You can also pass any JSON serializable inputs.</span><span class="sxs-lookup"><span data-stu-id="ae639-186">You can also pass any JSON serializable inputs.</span></span> <span data-ttu-id="ae639-187">You can pass image as binary encoded string as well.</span><span class="sxs-lookup"><span data-stu-id="ae639-187">You can pass image as binary encoded string as well.</span></span>

- <span data-ttu-id="ae639-188">**Does the web service support multiple inputs or parse different inputs?**</span><span class="sxs-lookup"><span data-stu-id="ae639-188">**Does the web service support multiple inputs or parse different inputs?**</span></span>

   <span data-ttu-id="ae639-189">Yes, you can take multiple inputs packaged in the one JSON request as a dictionary.</span><span class="sxs-lookup"><span data-stu-id="ae639-189">Yes, you can take multiple inputs packaged in the one JSON request as a dictionary.</span></span> <span data-ttu-id="ae639-190">Each input would correspond to a single unique dictionary key.</span><span class="sxs-lookup"><span data-stu-id="ae639-190">Each input would correspond to a single unique dictionary key.</span></span>

- <span data-ttu-id="ae639-191">**Is the call activated by a request to the web service a blocking call or an asynchronous call?**</span><span class="sxs-lookup"><span data-stu-id="ae639-191">**Is the call activated by a request to the web service a blocking call or an asynchronous call?**</span></span>

   <span data-ttu-id="ae639-192">If service was created using realtime option as part of the CLI or API, then it is a blocking/synchronous call.</span><span class="sxs-lookup"><span data-stu-id="ae639-192">If service was created using realtime option as part of the CLI or API, then it is a blocking/synchronous call.</span></span> <span data-ttu-id="ae639-193">It is expected to be realtime fast.</span><span class="sxs-lookup"><span data-stu-id="ae639-193">It is expected to be realtime fast.</span></span> <span data-ttu-id="ae639-194">Although on the client side you can call it using async HTTP library to avoid blocking the client thread.</span><span class="sxs-lookup"><span data-stu-id="ae639-194">Although on the client side you can call it using async HTTP library to avoid blocking the client thread.</span></span>

- <span data-ttu-id="ae639-195">**How many requests can the web service simultaneously handle?**</span><span class="sxs-lookup"><span data-stu-id="ae639-195">**How many requests can the web service simultaneously handle?**</span></span>

   <span data-ttu-id="ae639-196">It depends on the cluster and web service scale.</span><span class="sxs-lookup"><span data-stu-id="ae639-196">It depends on the cluster and web service scale.</span></span> <span data-ttu-id="ae639-197">You can scale out your service to 100x of replicas and then it can handle many requests concurrently.</span><span class="sxs-lookup"><span data-stu-id="ae639-197">You can scale out your service to 100x of replicas and then it can handle many requests concurrently.</span></span> <span data-ttu-id="ae639-198">You can also configure the maximum concurrent request per replica to increase service throughput.</span><span class="sxs-lookup"><span data-stu-id="ae639-198">You can also configure the maximum concurrent request per replica to increase service throughput.</span></span>

- <span data-ttu-id="ae639-199">**How many requests can the web service queue up?**</span><span class="sxs-lookup"><span data-stu-id="ae639-199">**How many requests can the web service queue up?**</span></span>

   <span data-ttu-id="ae639-200">It is configurable.</span><span class="sxs-lookup"><span data-stu-id="ae639-200">It is configurable.</span></span> <span data-ttu-id="ae639-201">By default, it is set to ~10 per single replica, but you can increase/decrease it to your application requirements.</span><span class="sxs-lookup"><span data-stu-id="ae639-201">By default, it is set to ~10 per single replica, but you can increase/decrease it to your application requirements.</span></span> <span data-ttu-id="ae639-202">Typically, increasing it the number of queued requests increases the service throughput but makes the latencies worse at higher percentiles.</span><span class="sxs-lookup"><span data-stu-id="ae639-202">Typically, increasing it the number of queued requests increases the service throughput but makes the latencies worse at higher percentiles.</span></span> <span data-ttu-id="ae639-203">To keep the latencies consistent, you may want to set the queuing to a low value (1-5), and increase the number of replicas to handle the throughput.</span><span class="sxs-lookup"><span data-stu-id="ae639-203">To keep the latencies consistent, you may want to set the queuing to a low value (1-5), and increase the number of replicas to handle the throughput.</span></span> <span data-ttu-id="ae639-204">You can also turn on autoscaling to make the number of replicas adjusting automatically based on load.</span><span class="sxs-lookup"><span data-stu-id="ae639-204">You can also turn on autoscaling to make the number of replicas adjusting automatically based on load.</span></span> 

- <span data-ttu-id="ae639-205">**Can the same machine or cluster be used for multiple web service endpoints?**</span><span class="sxs-lookup"><span data-stu-id="ae639-205">**Can the same machine or cluster be used for multiple web service endpoints?**</span></span>

   <span data-ttu-id="ae639-206">Absolutely.</span><span class="sxs-lookup"><span data-stu-id="ae639-206">Absolutely.</span></span> <span data-ttu-id="ae639-207">You can run 100x of services/endpoints on the same cluster.</span><span class="sxs-lookup"><span data-stu-id="ae639-207">You can run 100x of services/endpoints on the same cluster.</span></span> 

## <a name="next-steps"></a><span data-ttu-id="ae639-208">Next steps</span><span class="sxs-lookup"><span data-stu-id="ae639-208">Next steps</span></span>
<span data-ttu-id="ae639-209">For getting started with Model Management, see [Configuring Model Management](deployment-setup-configuration.md).</span><span class="sxs-lookup"><span data-stu-id="ae639-209">For getting started with Model Management, see [Configuring Model Management](deployment-setup-configuration.md).</span></span>
