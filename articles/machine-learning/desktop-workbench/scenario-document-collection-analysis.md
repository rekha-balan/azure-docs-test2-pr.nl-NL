---
title: Document Collection Analysis - Azure | Microsoft Docs
description: How to summarize and analyze a large collection of documents, including techniques such as phrase learning, topic modeling, and topic model analysis using Azure ML Workbench.
services: machine-learning
author: kehuan
ms.author: kehuan
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, MicrosoftDocs/mlreview, mldocs
ms.service: machine-learning
ms.component: core
ms.workload: data-services
ms.topic: article
ms.date: 09/20/2017
ms.openlocfilehash: 29f493449d48df26919a98452fa7f832d653d45e
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44867694"
---
# <a name="document-collection-analysis"></a><span data-ttu-id="eaa3d-103">Document Collection Analysis</span><span class="sxs-lookup"><span data-stu-id="eaa3d-103">Document Collection Analysis</span></span>

<span data-ttu-id="eaa3d-104">This scenario demonstrates how to summarize and analyze a large collection of documents, including techniques such as phrase learning, topic modeling, and topic model analysis using Azure ML Workbench.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-104">This scenario demonstrates how to summarize and analyze a large collection of documents, including techniques such as phrase learning, topic modeling, and topic model analysis using Azure ML Workbench.</span></span> <span data-ttu-id="eaa3d-105">Azure Machine Learning Workbench provides for easy scale up for very large document collection, and provides mechanisms to train and tune models within a variety of compute contexts, ranging from local compute to Data Science Virtual Machines to Spark Cluster.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-105">Azure Machine Learning Workbench provides for easy scale up for very large document collection, and provides mechanisms to train and tune models within a variety of compute contexts, ranging from local compute to Data Science Virtual Machines to Spark Cluster.</span></span> <span data-ttu-id="eaa3d-106">Easy development is provided through Jupyter notebooks within Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-106">Easy development is provided through Jupyter notebooks within Azure Machine Learning Workbench.</span></span>

## <a name="link-to-the-gallery-github-repository"></a><span data-ttu-id="eaa3d-107">Link to the Gallery GitHub repository</span><span class="sxs-lookup"><span data-stu-id="eaa3d-107">Link to the Gallery GitHub repository</span></span>

<span data-ttu-id="eaa3d-108">The public GitHub repository for this real world scenario contains all materials, including code samples, needed for this example:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-108">The public GitHub repository for this real world scenario contains all materials, including code samples, needed for this example:</span></span>

[https://github.com/Azure/MachineLearningSamples-DocumentCollectionAnalysis](https://github.com/Azure/MachineLearningSamples-DocumentCollectionAnalysis)

## <a name="overview"></a><span data-ttu-id="eaa3d-109">Overview</span><span class="sxs-lookup"><span data-stu-id="eaa3d-109">Overview</span></span>

<span data-ttu-id="eaa3d-110">With a large amount of data (especially unstructured text data) collected every day, a significant challenge is to organize, search, and understand vast quantities of these texts.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-110">With a large amount of data (especially unstructured text data) collected every day, a significant challenge is to organize, search, and understand vast quantities of these texts.</span></span> <span data-ttu-id="eaa3d-111">This document collection analysis scenario demonstrates an efficient and automated end-to-end workflow for analyzing large document collection and enabling downstream NLP tasks.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-111">This document collection analysis scenario demonstrates an efficient and automated end-to-end workflow for analyzing large document collection and enabling downstream NLP tasks.</span></span>

<span data-ttu-id="eaa3d-112">The key elements delivered by this scenario are:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-112">The key elements delivered by this scenario are:</span></span>

1. <span data-ttu-id="eaa3d-113">Learning salient multi-words phrase from documents.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-113">Learning salient multi-words phrase from documents.</span></span>

1. <span data-ttu-id="eaa3d-114">Discovering underlying topics presented in the document collection.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-114">Discovering underlying topics presented in the document collection.</span></span>

1. <span data-ttu-id="eaa3d-115">Representing documents by the topical distribution.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-115">Representing documents by the topical distribution.</span></span>

1. <span data-ttu-id="eaa3d-116">Presenting methods for organizing, searching, and summarizing documents based on the topical content.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-116">Presenting methods for organizing, searching, and summarizing documents based on the topical content.</span></span>

<span data-ttu-id="eaa3d-117">The methods presented in this scenario could enable a variety of critical industrial workloads, such as discovery of topic trends anomaly, document collection summarization, and similar document search.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-117">The methods presented in this scenario could enable a variety of critical industrial workloads, such as discovery of topic trends anomaly, document collection summarization, and similar document search.</span></span> <span data-ttu-id="eaa3d-118">It can be applied to many different types of document analysis, such as government legislation, news stories, product reviews, customer feedbacks, and scientific research articles.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-118">It can be applied to many different types of document analysis, such as government legislation, news stories, product reviews, customer feedbacks, and scientific research articles.</span></span>

<span data-ttu-id="eaa3d-119">The machine learning techniques/algorithms used in this scenario include:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-119">The machine learning techniques/algorithms used in this scenario include:</span></span>

1. <span data-ttu-id="eaa3d-120">Text processing and cleaning</span><span class="sxs-lookup"><span data-stu-id="eaa3d-120">Text processing and cleaning</span></span>

1. <span data-ttu-id="eaa3d-121">Phrase Learning</span><span class="sxs-lookup"><span data-stu-id="eaa3d-121">Phrase Learning</span></span>

1. <span data-ttu-id="eaa3d-122">Topic modeling</span><span class="sxs-lookup"><span data-stu-id="eaa3d-122">Topic modeling</span></span>

1. <span data-ttu-id="eaa3d-123">Corpus summarization</span><span class="sxs-lookup"><span data-stu-id="eaa3d-123">Corpus summarization</span></span>

1. <span data-ttu-id="eaa3d-124">Topical trends and anomaly detection</span><span class="sxs-lookup"><span data-stu-id="eaa3d-124">Topical trends and anomaly detection</span></span>

## <a name="prerequisites"></a><span data-ttu-id="eaa3d-125">Prerequisites</span><span class="sxs-lookup"><span data-stu-id="eaa3d-125">Prerequisites</span></span>

<span data-ttu-id="eaa3d-126">The prerequisites to run this example are as follows:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-126">The prerequisites to run this example are as follows:</span></span>

* <span data-ttu-id="eaa3d-127">Make sure that you have properly installed [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) by following the [Install and create Quickstart](../service/quickstart-installation.md).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-127">Make sure that you have properly installed [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) by following the [Install and create Quickstart](../service/quickstart-installation.md).</span></span>

* <span data-ttu-id="eaa3d-128">This example could be run on any compute context.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-128">This example could be run on any compute context.</span></span> <span data-ttu-id="eaa3d-129">However, it is recommended to run it on a multi-core machine with at least of 16-GB memory and 5-GB disk space.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-129">However, it is recommended to run it on a multi-core machine with at least of 16-GB memory and 5-GB disk space.</span></span>

## <a name="create-a-new-workbench-project"></a><span data-ttu-id="eaa3d-130">Create a new Workbench project</span><span class="sxs-lookup"><span data-stu-id="eaa3d-130">Create a new Workbench project</span></span>

<span data-ttu-id="eaa3d-131">Create a new project using this example as a template:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-131">Create a new project using this example as a template:</span></span>
1.  <span data-ttu-id="eaa3d-132">Open Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="eaa3d-132">Open Azure Machine Learning Workbench</span></span>
2.  <span data-ttu-id="eaa3d-133">On the **Projects** page, click the **+** sign and select **New Project**</span><span class="sxs-lookup"><span data-stu-id="eaa3d-133">On the **Projects** page, click the **+** sign and select **New Project**</span></span>
3.  <span data-ttu-id="eaa3d-134">In the **Create New Project** pane, fill in the information for your new project</span><span class="sxs-lookup"><span data-stu-id="eaa3d-134">In the **Create New Project** pane, fill in the information for your new project</span></span>
4.  <span data-ttu-id="eaa3d-135">In the **Search Project Templates** search box, type "Document Collection Analysis" and select the template</span><span class="sxs-lookup"><span data-stu-id="eaa3d-135">In the **Search Project Templates** search box, type "Document Collection Analysis" and select the template</span></span>
5.  <span data-ttu-id="eaa3d-136">Click **Create**</span><span class="sxs-lookup"><span data-stu-id="eaa3d-136">Click **Create**</span></span>

## <a name="data-description"></a><span data-ttu-id="eaa3d-137">Data description</span><span class="sxs-lookup"><span data-stu-id="eaa3d-137">Data description</span></span>

<span data-ttu-id="eaa3d-138">The dataset used in this scenario contains text summaries and associated meta data for each legislative action taken by the US Congress.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-138">The dataset used in this scenario contains text summaries and associated meta data for each legislative action taken by the US Congress.</span></span> <span data-ttu-id="eaa3d-139">The data is collected from [GovTrack.us](https://www.govtrack.us/), which tracks the activities of United States Congress and helps Americans participate in their national legislative process.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-139">The data is collected from [GovTrack.us](https://www.govtrack.us/), which tracks the activities of United States Congress and helps Americans participate in their national legislative process.</span></span> <span data-ttu-id="eaa3d-140">The bulk data can be downloaded via [this link](https://www.govtrack.us/data/congress/) using a manual script, which is not included in this scenario.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-140">The bulk data can be downloaded via [this link](https://www.govtrack.us/data/congress/) using a manual script, which is not included in this scenario.</span></span> <span data-ttu-id="eaa3d-141">The details of how to download the data could be found in the [GovTrack API documentation](https://www.govtrack.us/developers).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-141">The details of how to download the data could be found in the [GovTrack API documentation](https://www.govtrack.us/developers).</span></span>

### <a name="data-source"></a><span data-ttu-id="eaa3d-142">Data source</span><span class="sxs-lookup"><span data-stu-id="eaa3d-142">Data source</span></span>

<span data-ttu-id="eaa3d-143">In this scenario, the raw data collected is a series of legislative actions introduced by the US Congress (proposed bills and resolutions) from 1973 to June 2017 (the 93rd to the 115th Congresses).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-143">In this scenario, the raw data collected is a series of legislative actions introduced by the US Congress (proposed bills and resolutions) from 1973 to June 2017 (the 93rd to the 115th Congresses).</span></span> <span data-ttu-id="eaa3d-144">The data collected is in JSON format and contains a rich set of information about the legislative actions.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-144">The data collected is in JSON format and contains a rich set of information about the legislative actions.</span></span> <span data-ttu-id="eaa3d-145">Refer to [this GitHub link](https://github.com/unitedstates/congress/wiki/bills) for detailed description of the data fields.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-145">Refer to [this GitHub link](https://github.com/unitedstates/congress/wiki/bills) for detailed description of the data fields.</span></span> <span data-ttu-id="eaa3d-146">For the demonstration purpose within this scenario, only a subset of data fields were extracted from the JSON files.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-146">For the demonstration purpose within this scenario, only a subset of data fields were extracted from the JSON files.</span></span> <span data-ttu-id="eaa3d-147">A pre-compiled TSV file `CongressionalDataAll_Jun_2017.tsv` containing records of those legislative actions is provided in this scenario.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-147">A pre-compiled TSV file `CongressionalDataAll_Jun_2017.tsv` containing records of those legislative actions is provided in this scenario.</span></span> <span data-ttu-id="eaa3d-148">The TSV file can be downloaded automatically either by the notebooks `1_Preprocess_Text.ipynb` under the notebook folder or `preprocessText.py` in the Python package.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-148">The TSV file can be downloaded automatically either by the notebooks `1_Preprocess_Text.ipynb` under the notebook folder or `preprocessText.py` in the Python package.</span></span>

### <a name="data-structure"></a><span data-ttu-id="eaa3d-149">Data structure</span><span class="sxs-lookup"><span data-stu-id="eaa3d-149">Data structure</span></span>

<span data-ttu-id="eaa3d-150">There are nine data fields in the data file.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-150">There are nine data fields in the data file.</span></span> <span data-ttu-id="eaa3d-151">The data field names and the descriptions are listed as follows.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-151">The data field names and the descriptions are listed as follows.</span></span>

| <span data-ttu-id="eaa3d-152">Field Name</span><span class="sxs-lookup"><span data-stu-id="eaa3d-152">Field Name</span></span> | <span data-ttu-id="eaa3d-153">Type</span><span class="sxs-lookup"><span data-stu-id="eaa3d-153">Type</span></span> | <span data-ttu-id="eaa3d-154">Description</span><span class="sxs-lookup"><span data-stu-id="eaa3d-154">Description</span></span> | <span data-ttu-id="eaa3d-155">Contain Missing Value</span><span class="sxs-lookup"><span data-stu-id="eaa3d-155">Contain Missing Value</span></span> |
|------------|------|-------------|---------------|
| `ID` | <span data-ttu-id="eaa3d-156">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-156">String</span></span> | <span data-ttu-id="eaa3d-157">The ID of the bill/resolution.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-157">The ID of the bill/resolution.</span></span> <span data-ttu-id="eaa3d-158">The format of this field is [bill_type][number]-[congress].</span><span class="sxs-lookup"><span data-stu-id="eaa3d-158">The format of this field is [bill_type][number]-[congress].</span></span> <span data-ttu-id="eaa3d-159">For example, "hconres1-93" means the bill type is "hconres" (stands for House Concurrent Resolution, refer to [this document](https://github.com/unitedstates/congress/wiki/bills#basic-information)), the bill number is '1' and the congress number is '93'.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-159">For example, "hconres1-93" means the bill type is "hconres" (stands for House Concurrent Resolution, refer to [this document](https://github.com/unitedstates/congress/wiki/bills#basic-information)), the bill number is '1' and the congress number is '93'.</span></span> | <span data-ttu-id="eaa3d-160">No</span><span class="sxs-lookup"><span data-stu-id="eaa3d-160">No</span></span> |
| `Text` | <span data-ttu-id="eaa3d-161">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-161">String</span></span> | <span data-ttu-id="eaa3d-162">The content of the bill/resolution.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-162">The content of the bill/resolution.</span></span> | <span data-ttu-id="eaa3d-163">No</span><span class="sxs-lookup"><span data-stu-id="eaa3d-163">No</span></span> |
| `Date` | <span data-ttu-id="eaa3d-164">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-164">String</span></span> | <span data-ttu-id="eaa3d-165">The date the bill/resolution initially proposed.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-165">The date the bill/resolution initially proposed.</span></span> <span data-ttu-id="eaa3d-166">In a format of 'yyyy-mm-dd'.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-166">In a format of 'yyyy-mm-dd'.</span></span> | <span data-ttu-id="eaa3d-167">No</span><span class="sxs-lookup"><span data-stu-id="eaa3d-167">No</span></span> |
| `SponsorName` | <span data-ttu-id="eaa3d-168">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-168">String</span></span> | <span data-ttu-id="eaa3d-169">The name of the primary sponsor that proposed the bill/resolution.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-169">The name of the primary sponsor that proposed the bill/resolution.</span></span> | <span data-ttu-id="eaa3d-170">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-170">Yes</span></span> |
| `Type` | <span data-ttu-id="eaa3d-171">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-171">String</span></span> | <span data-ttu-id="eaa3d-172">The title type of the primary sponsor, either 'rep' (representative) or 'sen' (senator).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-172">The title type of the primary sponsor, either 'rep' (representative) or 'sen' (senator).</span></span> | <span data-ttu-id="eaa3d-173">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-173">Yes</span></span> |
| `State` | <span data-ttu-id="eaa3d-174">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-174">String</span></span> | <span data-ttu-id="eaa3d-175">The state of the primary sponsor.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-175">The state of the primary sponsor.</span></span> | <span data-ttu-id="eaa3d-176">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-176">Yes</span></span> |
| `District` | <span data-ttu-id="eaa3d-177">Integer</span><span class="sxs-lookup"><span data-stu-id="eaa3d-177">Integer</span></span> | <span data-ttu-id="eaa3d-178">The district number of the primary sponsor if the title of the sponsor is a representative.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-178">The district number of the primary sponsor if the title of the sponsor is a representative.</span></span> | <span data-ttu-id="eaa3d-179">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-179">Yes</span></span> |
| `Party` | <span data-ttu-id="eaa3d-180">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-180">String</span></span> | <span data-ttu-id="eaa3d-181">The party of the primary sponsor.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-181">The party of the primary sponsor.</span></span> | <span data-ttu-id="eaa3d-182">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-182">Yes</span></span> |
| `Subjects` | <span data-ttu-id="eaa3d-183">String</span><span class="sxs-lookup"><span data-stu-id="eaa3d-183">String</span></span> | <span data-ttu-id="eaa3d-184">The subject terms added cumulatively by the Library of Congress to the bill.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-184">The subject terms added cumulatively by the Library of Congress to the bill.</span></span> <span data-ttu-id="eaa3d-185">The terms are concatenated by commas.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-185">The terms are concatenated by commas.</span></span> <span data-ttu-id="eaa3d-186">These terms are written by a human in the Library of Congress, and are not usually present when information on the bill is first published.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-186">These terms are written by a human in the Library of Congress, and are not usually present when information on the bill is first published.</span></span> <span data-ttu-id="eaa3d-187">They can be added at any time.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-187">They can be added at any time.</span></span> <span data-ttu-id="eaa3d-188">Thus by the end of life of a bill, some subject may not be relevant anymore.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-188">Thus by the end of life of a bill, some subject may not be relevant anymore.</span></span> | <span data-ttu-id="eaa3d-189">Yes</span><span class="sxs-lookup"><span data-stu-id="eaa3d-189">Yes</span></span> |

## <a name="scenario-structure"></a><span data-ttu-id="eaa3d-190">Scenario structure</span><span class="sxs-lookup"><span data-stu-id="eaa3d-190">Scenario structure</span></span>

<span data-ttu-id="eaa3d-191">The document collection analysis example is organized into two types of deliverables.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-191">The document collection analysis example is organized into two types of deliverables.</span></span> <span data-ttu-id="eaa3d-192">The first type is a series of iPython Notebooks that show the step-by-step descriptions of the entire workflow.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-192">The first type is a series of iPython Notebooks that show the step-by-step descriptions of the entire workflow.</span></span> <span data-ttu-id="eaa3d-193">The second type is a Python package as well as some code examples of how to use that package.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-193">The second type is a Python package as well as some code examples of how to use that package.</span></span> <span data-ttu-id="eaa3d-194">The Python package is generic enough to serve many use cases.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-194">The Python package is generic enough to serve many use cases.</span></span>

<span data-ttu-id="eaa3d-195">The files in this example are organized as follows.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-195">The files in this example are organized as follows.</span></span>

| <span data-ttu-id="eaa3d-196">File Name</span><span class="sxs-lookup"><span data-stu-id="eaa3d-196">File Name</span></span> | <span data-ttu-id="eaa3d-197">Type</span><span class="sxs-lookup"><span data-stu-id="eaa3d-197">Type</span></span> | <span data-ttu-id="eaa3d-198">Description</span><span class="sxs-lookup"><span data-stu-id="eaa3d-198">Description</span></span> |
|-----------|------|-------------|
| `aml_config` | <span data-ttu-id="eaa3d-199">Folder</span><span class="sxs-lookup"><span data-stu-id="eaa3d-199">Folder</span></span> | <span data-ttu-id="eaa3d-200">Azure Machine Learning Workbench configuration folder, refer to [this documentation](./experimentation-service-configuration-reference.md) for detailed experiment execution configuration</span><span class="sxs-lookup"><span data-stu-id="eaa3d-200">Azure Machine Learning Workbench configuration folder, refer to [this documentation](./experimentation-service-configuration-reference.md) for detailed experiment execution configuration</span></span> |
| `Code` | <span data-ttu-id="eaa3d-201">Folder</span><span class="sxs-lookup"><span data-stu-id="eaa3d-201">Folder</span></span> | <span data-ttu-id="eaa3d-202">The code folder used to save the Python scripts and Python package</span><span class="sxs-lookup"><span data-stu-id="eaa3d-202">The code folder used to save the Python scripts and Python package</span></span> |
| `Data` | <span data-ttu-id="eaa3d-203">Folder</span><span class="sxs-lookup"><span data-stu-id="eaa3d-203">Folder</span></span> | <span data-ttu-id="eaa3d-204">The data folder used to save intermediate files</span><span class="sxs-lookup"><span data-stu-id="eaa3d-204">The data folder used to save intermediate files</span></span> |
| `notebooks` | <span data-ttu-id="eaa3d-205">Folder</span><span class="sxs-lookup"><span data-stu-id="eaa3d-205">Folder</span></span> | <span data-ttu-id="eaa3d-206">The Jupyter notebooks folder</span><span class="sxs-lookup"><span data-stu-id="eaa3d-206">The Jupyter notebooks folder</span></span> |
| `Code/documentAnalysis/__init__.py` | <span data-ttu-id="eaa3d-207">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-207">Python file</span></span> | <span data-ttu-id="eaa3d-208">The Python package init file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-208">The Python package init file</span></span> |
| `Code/documentAnalysis/configs.py` | <span data-ttu-id="eaa3d-209">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-209">Python file</span></span> | <span data-ttu-id="eaa3d-210">The configuration file used by the document analysis Python package, including predefined constants</span><span class="sxs-lookup"><span data-stu-id="eaa3d-210">The configuration file used by the document analysis Python package, including predefined constants</span></span> |
| `Code/documentAnalysis/preprocessText.py` | <span data-ttu-id="eaa3d-211">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-211">Python file</span></span> | <span data-ttu-id="eaa3d-212">The Python file used to preprocess the data for downstream tasks</span><span class="sxs-lookup"><span data-stu-id="eaa3d-212">The Python file used to preprocess the data for downstream tasks</span></span> |
| `Code/documentAnalysis/phraseLearning.py` | <span data-ttu-id="eaa3d-213">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-213">Python file</span></span> | <span data-ttu-id="eaa3d-214">The Python file used to learn phrases from the data and transform the raw data</span><span class="sxs-lookup"><span data-stu-id="eaa3d-214">The Python file used to learn phrases from the data and transform the raw data</span></span> |
| `Code/documentAnalysis/topicModeling.py` | <span data-ttu-id="eaa3d-215">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-215">Python file</span></span> | <span data-ttu-id="eaa3d-216">The Python file used to train a Latent Dirichlet Allocation (LDA) topic model</span><span class="sxs-lookup"><span data-stu-id="eaa3d-216">The Python file used to train a Latent Dirichlet Allocation (LDA) topic model</span></span> |
| `Code/step1.py` | <span data-ttu-id="eaa3d-217">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-217">Python file</span></span> | <span data-ttu-id="eaa3d-218">Step 1 of document collection analysis: preprocess text</span><span class="sxs-lookup"><span data-stu-id="eaa3d-218">Step 1 of document collection analysis: preprocess text</span></span> |
| `Code/step2.py` | <span data-ttu-id="eaa3d-219">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-219">Python file</span></span> | <span data-ttu-id="eaa3d-220">Step 2 of document collection analysis: phrase learning</span><span class="sxs-lookup"><span data-stu-id="eaa3d-220">Step 2 of document collection analysis: phrase learning</span></span> |
| `Code/step3.py` | <span data-ttu-id="eaa3d-221">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-221">Python file</span></span> | <span data-ttu-id="eaa3d-222">Step 3 of document collection analysis: train and evaluate LDA topic model</span><span class="sxs-lookup"><span data-stu-id="eaa3d-222">Step 3 of document collection analysis: train and evaluate LDA topic model</span></span> |
| `Code/runme.py` | <span data-ttu-id="eaa3d-223">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-223">Python file</span></span> | <span data-ttu-id="eaa3d-224">Example of run all steps in one file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-224">Example of run all steps in one file</span></span> |
| `notebooks/1_Preprocess_Text.ipynb` | <span data-ttu-id="eaa3d-225">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-225">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-226">Preprocess text and transform the raw data</span><span class="sxs-lookup"><span data-stu-id="eaa3d-226">Preprocess text and transform the raw data</span></span> |
| `notebooks/2_Phrase_Learning.ipynb` | <span data-ttu-id="eaa3d-227">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-227">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-228">Learn phrases from text data (after data transform)</span><span class="sxs-lookup"><span data-stu-id="eaa3d-228">Learn phrases from text data (after data transform)</span></span> |
| `notebooks/3_Topic_Model_Training.ipynb` | <span data-ttu-id="eaa3d-229">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-229">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-230">Train LDA topic model</span><span class="sxs-lookup"><span data-stu-id="eaa3d-230">Train LDA topic model</span></span> |
| `notebooks/4_Topic_Model_Summarization.ipynb` | <span data-ttu-id="eaa3d-231">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-231">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-232">Summarize the contents of the document collection based on a trained LDA topic model</span><span class="sxs-lookup"><span data-stu-id="eaa3d-232">Summarize the contents of the document collection based on a trained LDA topic model</span></span> |
| `notebooks/5_Topic_Model_Analysis.ipynb` | <span data-ttu-id="eaa3d-233">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-233">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-234">Analyze the topical content of a collection of text documents and correlate topical information against other meta-data associated with the document collection</span><span class="sxs-lookup"><span data-stu-id="eaa3d-234">Analyze the topical content of a collection of text documents and correlate topical information against other meta-data associated with the document collection</span></span> |
| `notebooks/6_Interactive_Visualization.ipynb` | <span data-ttu-id="eaa3d-235">iPython Notebook</span><span class="sxs-lookup"><span data-stu-id="eaa3d-235">iPython Notebook</span></span> | <span data-ttu-id="eaa3d-236">Interactive visualization of learned topic model</span><span class="sxs-lookup"><span data-stu-id="eaa3d-236">Interactive visualization of learned topic model</span></span> |
| `notebooks/winprocess.py` | <span data-ttu-id="eaa3d-237">Python file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-237">Python file</span></span> | <span data-ttu-id="eaa3d-238">The python script for multiprocessing used by notebooks</span><span class="sxs-lookup"><span data-stu-id="eaa3d-238">The python script for multiprocessing used by notebooks</span></span> |
| `README.md` | <span data-ttu-id="eaa3d-239">Markdown file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-239">Markdown file</span></span> | <span data-ttu-id="eaa3d-240">The README markdown file</span><span class="sxs-lookup"><span data-stu-id="eaa3d-240">The README markdown file</span></span> |

### <a name="data-ingestion-and-transformation"></a><span data-ttu-id="eaa3d-241">Data ingestion and transformation</span><span class="sxs-lookup"><span data-stu-id="eaa3d-241">Data ingestion and transformation</span></span>

<span data-ttu-id="eaa3d-242">The compiled dataset `CongressionalDataAll_Jun_2017.tsv` is saved in Blob Storage and is accessible both from within the notebooks and the Python scripts.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-242">The compiled dataset `CongressionalDataAll_Jun_2017.tsv` is saved in Blob Storage and is accessible both from within the notebooks and the Python scripts.</span></span> <span data-ttu-id="eaa3d-243">There are two steps for data ingestion and transformation: preprocessing the text data, and phrase learning.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-243">There are two steps for data ingestion and transformation: preprocessing the text data, and phrase learning.</span></span>

#### <a name="preprocess-text-data"></a><span data-ttu-id="eaa3d-244">Preprocess text data</span><span class="sxs-lookup"><span data-stu-id="eaa3d-244">Preprocess text data</span></span>

<span data-ttu-id="eaa3d-245">The preprocessing step applies natural language processing techniques to clean and prepare the raw text data.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-245">The preprocessing step applies natural language processing techniques to clean and prepare the raw text data.</span></span> <span data-ttu-id="eaa3d-246">It serves as a precursor for the unsupervised phrase learning and latent topic modeling.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-246">It serves as a precursor for the unsupervised phrase learning and latent topic modeling.</span></span> <span data-ttu-id="eaa3d-247">The detailed step-by-step description can be found in the notebook `1_Preprocess_Text.ipynb`.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-247">The detailed step-by-step description can be found in the notebook `1_Preprocess_Text.ipynb`.</span></span> <span data-ttu-id="eaa3d-248">There is also a Python script `step1.py` corresponds to this notebook.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-248">There is also a Python script `step1.py` corresponds to this notebook.</span></span>

<span data-ttu-id="eaa3d-249">In this step, the TSV data file is downloaded from the Azure Blob Storage and imported as a Pandas DataFrame.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-249">In this step, the TSV data file is downloaded from the Azure Blob Storage and imported as a Pandas DataFrame.</span></span> <span data-ttu-id="eaa3d-250">Each row element in the DataFrame is a single cohesive long string of text or a 'document'.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-250">Each row element in the DataFrame is a single cohesive long string of text or a 'document'.</span></span> <span data-ttu-id="eaa3d-251">Each document is then split into chunks of text that are likely to be sentences, phrases, or subphrases.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-251">Each document is then split into chunks of text that are likely to be sentences, phrases, or subphrases.</span></span> <span data-ttu-id="eaa3d-252">The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-252">The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.</span></span>

<span data-ttu-id="eaa3d-253">There are multiple functions defined for the preprocessing step both in the notebook and the Python package.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-253">There are multiple functions defined for the preprocessing step both in the notebook and the Python package.</span></span> <span data-ttu-id="eaa3d-254">The majority of the job can be achieved by calling those two lines of codes.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-254">The majority of the job can be achieved by calling those two lines of codes.</span></span>

```python
# Read raw data into a Pandas DataFrame
textDF = getData()

# Write data frame with preprocessed text out to TSV file
cleanedDataFrame = CleanAndSplitText(textDF, saveDF=True)
```

#### <a name="phrase-learning"></a><span data-ttu-id="eaa3d-255">Phrase learning</span><span class="sxs-lookup"><span data-stu-id="eaa3d-255">Phrase learning</span></span>

<span data-ttu-id="eaa3d-256">The phrase learning step implements a basic framework to learn key phrases among a large collection of documents.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-256">The phrase learning step implements a basic framework to learn key phrases among a large collection of documents.</span></span> <span data-ttu-id="eaa3d-257">It is described in the paper entitled "[Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf)", which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-257">It is described in the paper entitled "[Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf)", which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.</span></span> <span data-ttu-id="eaa3d-258">The detailed implementation of phrase learning step is shown in the iPython Notebook `2_Phrase_Learning.ipynb` and the Python script `step2.py`.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-258">The detailed implementation of phrase learning step is shown in the iPython Notebook `2_Phrase_Learning.ipynb` and the Python script `step2.py`.</span></span>

<span data-ttu-id="eaa3d-259">This step takes the cleaned text as input and learns the most salient phrases present in a large collection of documents.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-259">This step takes the cleaned text as input and learns the most salient phrases present in a large collection of documents.</span></span> <span data-ttu-id="eaa3d-260">The phrase learning is an iterative process that can be divided into three tasks: count n-grams, rank potential phrases by the Weighted Pointwise Mutual Information of their constituent words, and rewrite phrase to text.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-260">The phrase learning is an iterative process that can be divided into three tasks: count n-grams, rank potential phrases by the Weighted Pointwise Mutual Information of their constituent words, and rewrite phrase to text.</span></span> <span data-ttu-id="eaa3d-261">Those three tasks are executed sequentially in multiple iterations until the specified phrases have been learned.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-261">Those three tasks are executed sequentially in multiple iterations until the specified phrases have been learned.</span></span>

<span data-ttu-id="eaa3d-262">In the document analysis Python package, a Python Class `PhraseLearner` is defined in the `phraseLearning.py` file.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-262">In the document analysis Python package, a Python Class `PhraseLearner` is defined in the `phraseLearning.py` file.</span></span> <span data-ttu-id="eaa3d-263">Below is the code snippet used to learn phrases.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-263">Below is the code snippet used to learn phrases.</span></span>

```python
# Instantiate a PhraseLearner and run a configuration
phraseLearner = PhraseLearner(cleanedDataFrame, "CleanedText", numPhrase,
                        maxPhrasePerIter, maxPhraseLength, minInstanceCount)

# The chunks of text in a list
textData = list(phraseLearner.textFrame['LowercaseText'])

# Learn most salient phrases present in a large collection of documents
phraseLearner.RunConfiguration(textData,
            phraseLearner.learnedPhrases,
            addSpace=True,
            writeFile=True,
            num_workers=cpu_count()-1)
```

> [!NOTE]
> <span data-ttu-id="eaa3d-264">The phrase learning step is implemented with multiprocessing.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-264">The phrase learning step is implemented with multiprocessing.</span></span> <span data-ttu-id="eaa3d-265">However, more CPU cores do **NOT** mean a faster execution time.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-265">However, more CPU cores do **NOT** mean a faster execution time.</span></span> <span data-ttu-id="eaa3d-266">In our tests, the performance is not improved with more than eight cores due to the overhead of multiprocessing.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-266">In our tests, the performance is not improved with more than eight cores due to the overhead of multiprocessing.</span></span> <span data-ttu-id="eaa3d-267">It took about two and a half hours to learn 25,000 phrases on a machine with eight cores (3.6 GHz).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-267">It took about two and a half hours to learn 25,000 phrases on a machine with eight cores (3.6 GHz).</span></span>
>

### <a name="topic-modeling"></a><span data-ttu-id="eaa3d-268">Topic modeling</span><span class="sxs-lookup"><span data-stu-id="eaa3d-268">Topic modeling</span></span>

<span data-ttu-id="eaa3d-269">Learning a latent topic model use LDA is the third step in this scenario.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-269">Learning a latent topic model use LDA is the third step in this scenario.</span></span> <span data-ttu-id="eaa3d-270">The [gensim](https://radimrehurek.com/gensim/) Python package is required in this step to learn an [LDA topic model](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-270">The [gensim](https://radimrehurek.com/gensim/) Python package is required in this step to learn an [LDA topic model](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).</span></span> <span data-ttu-id="eaa3d-271">The corresponding notebook for this step is `3_Topic_Model_Training.ipynb`.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-271">The corresponding notebook for this step is `3_Topic_Model_Training.ipynb`.</span></span> <span data-ttu-id="eaa3d-272">You can also refer to `step3.py` for how to use the document analysis package.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-272">You can also refer to `step3.py` for how to use the document analysis package.</span></span>

<span data-ttu-id="eaa3d-273">In this step, the main task is to learn an LDA topic model and tune the hyper parameters.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-273">In this step, the main task is to learn an LDA topic model and tune the hyper parameters.</span></span> <span data-ttu-id="eaa3d-274">There are multiple parameters need to be tuned when train an LDA model, but the most important parameter is the number of topics.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-274">There are multiple parameters need to be tuned when train an LDA model, but the most important parameter is the number of topics.</span></span> <span data-ttu-id="eaa3d-275">Too few topics may not have insight to the document collection; while choosing too many will result in the "over-clustering" of a corpus into many small, highly similar topics.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-275">Too few topics may not have insight to the document collection; while choosing too many will result in the "over-clustering" of a corpus into many small, highly similar topics.</span></span> <span data-ttu-id="eaa3d-276">The purpose of this scenario is to show how to tune the number of topics.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-276">The purpose of this scenario is to show how to tune the number of topics.</span></span> <span data-ttu-id="eaa3d-277">Azure Machine Learning Workbench provides the freedom to run experiments with different parameter configuration on different compute contexts.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-277">Azure Machine Learning Workbench provides the freedom to run experiments with different parameter configuration on different compute contexts.</span></span>

<span data-ttu-id="eaa3d-278">In the document analysis Python package, a few functions were defined to help the users figure out the best number of topics.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-278">In the document analysis Python package, a few functions were defined to help the users figure out the best number of topics.</span></span> <span data-ttu-id="eaa3d-279">The first approach is by evaluating the coherence of the topic model.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-279">The first approach is by evaluating the coherence of the topic model.</span></span> <span data-ttu-id="eaa3d-280">There are four evaluation matrices supported: `u_mass`, `c_v`, `c_uci`, and `c_npmi`.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-280">There are four evaluation matrices supported: `u_mass`, `c_v`, `c_uci`, and `c_npmi`.</span></span> <span data-ttu-id="eaa3d-281">The details of those four metrics are discussed in [this paper](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-281">The details of those four metrics are discussed in [this paper](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).</span></span> <span data-ttu-id="eaa3d-282">The second approach is to evaluate the perplexity on a held-out corpus.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-282">The second approach is to evaluate the perplexity on a held-out corpus.</span></span>

<span data-ttu-id="eaa3d-283">For the perplexity evaluation, a 'U' shape curve is expected to find out the best number of topics.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-283">For the perplexity evaluation, a 'U' shape curve is expected to find out the best number of topics.</span></span> <span data-ttu-id="eaa3d-284">And the elbow position is the best choice.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-284">And the elbow position is the best choice.</span></span> <span data-ttu-id="eaa3d-285">It is recommended to evaluate multiple times with different random seed and get the average.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-285">It is recommended to evaluate multiple times with different random seed and get the average.</span></span> <span data-ttu-id="eaa3d-286">The coherence evaluate is expected to be a 'n' shape, which means the coherence increases with increasing the number of topics and then decrease.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-286">The coherence evaluate is expected to be a 'n' shape, which means the coherence increases with increasing the number of topics and then decrease.</span></span> <span data-ttu-id="eaa3d-287">An example plot of perplexity and `c_v` coherence is showing as follows.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-287">An example plot of perplexity and `c_v` coherence is showing as follows.</span></span>

![Perplexity](./media/scenario-document-collection-analysis/Perplexity_Value.png)

![c_v Coherence](./media/scenario-document-collection-analysis/c_v_Coherence.png)

<span data-ttu-id="eaa3d-290">In this scenario, the perplexity increases significantly after 200 topics, while the coherence value decreases significantly after 200 topics as well.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-290">In this scenario, the perplexity increases significantly after 200 topics, while the coherence value decreases significantly after 200 topics as well.</span></span> <span data-ttu-id="eaa3d-291">Based on those graphs and the desire for more general topics versus over clustered topics, choose 200 topics should be a good option.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-291">Based on those graphs and the desire for more general topics versus over clustered topics, choose 200 topics should be a good option.</span></span>

<span data-ttu-id="eaa3d-292">You can train one LDA topic model in one experiment run, or train and evaluate multiple LDA models with different topic number configurations in a single experiment run.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-292">You can train one LDA topic model in one experiment run, or train and evaluate multiple LDA models with different topic number configurations in a single experiment run.</span></span> <span data-ttu-id="eaa3d-293">It is recommended to run multiple times for one configuration and then get the average coherence and/or perplexity evaluations.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-293">It is recommended to run multiple times for one configuration and then get the average coherence and/or perplexity evaluations.</span></span> <span data-ttu-id="eaa3d-294">The details of how to use the document analysis package can be found in `step3.py` file.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-294">The details of how to use the document analysis package can be found in `step3.py` file.</span></span> <span data-ttu-id="eaa3d-295">An example code snippet is as follows.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-295">An example code snippet is as follows.</span></span>

```python
topicmodeler = TopicModeler(docs,
        stopWordFile=FUNCTION_WORDS_FILE,
        minWordCount=MIN_WORD_COUNT,
        minDocCount=MIN_DOC_COUNT,
        maxDocFreq=MAX_DOC_FREQ,
        workers=cpu_count()-1,
        numTopics=NUM_TOPICS,
        numIterations=NUM_ITERATIONS,
        passes=NUM_PASSES,
        chunksize=CHUNK_SIZE,
        random_state=RANDOM_STATE,
        test_ratio=test_ratio)

# Train an LDA topic model
lda = topicmodeler.TrainLDA(saveModel=saveModel)

# Evaluate coherence metrics
coherence = topicmodeler.EvaluateCoherence(lda, coherence_types)

# Evaluate perplexity on a held-out corpus
perplex = topicmodeler.EvaluatePerplexity(lda)
```

> [!NOTE]
> <span data-ttu-id="eaa3d-296">The execution time to train an LDA topic model depends on multiple factors such as the size of corpus, hyper parameter configuration, as well as the number of cores on the machine.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-296">The execution time to train an LDA topic model depends on multiple factors such as the size of corpus, hyper parameter configuration, as well as the number of cores on the machine.</span></span> <span data-ttu-id="eaa3d-297">Using multiple CPU cores trains a model faster.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-297">Using multiple CPU cores trains a model faster.</span></span> <span data-ttu-id="eaa3d-298">However, with the same hyper parameter setting more cores means fewer updates during training.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-298">However, with the same hyper parameter setting more cores means fewer updates during training.</span></span> <span data-ttu-id="eaa3d-299">It is recommended to have **at least 100 updates to train a converged LDA model**.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-299">It is recommended to have **at least 100 updates to train a converged LDA model**.</span></span> <span data-ttu-id="eaa3d-300">The relationship between number of updates and hyper parameters is discussed in [this post](https://groups.google.com/forum/#!topic/gensim/ojySenxQHi4) and [this post](http://miningthedetails.com/blog/python/lda/GensimLDA/).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-300">The relationship between number of updates and hyper parameters is discussed in [this post](https://groups.google.com/forum/#!topic/gensim/ojySenxQHi4) and [this post](http://miningthedetails.com/blog/python/lda/GensimLDA/).</span></span> <span data-ttu-id="eaa3d-301">In our tests, it took about 3 hours to train an LDA model with 200 topics using the configuration of `workers=15`, `passes=10`, `chunksize=1000` on a machine with 16 cores (2.0 GHz).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-301">In our tests, it took about 3 hours to train an LDA model with 200 topics using the configuration of `workers=15`, `passes=10`, `chunksize=1000` on a machine with 16 cores (2.0 GHz).</span></span>
>

### <a name="topic-summarization-and-analysis"></a><span data-ttu-id="eaa3d-302">Topic summarization and analysis</span><span class="sxs-lookup"><span data-stu-id="eaa3d-302">Topic summarization and analysis</span></span>

<span data-ttu-id="eaa3d-303">The topic summarization and analysis consists of two notebooks, while there are no corresponding functions in the document analysis package.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-303">The topic summarization and analysis consists of two notebooks, while there are no corresponding functions in the document analysis package.</span></span>

<span data-ttu-id="eaa3d-304">In `4_Topic_Model_Summarization.ipynb`, it shows how to summarize the contents of the document based on a trained LDA topic model.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-304">In `4_Topic_Model_Summarization.ipynb`, it shows how to summarize the contents of the document based on a trained LDA topic model.</span></span> <span data-ttu-id="eaa3d-305">The summarization is applied to an LDA topic model learned in step 3.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-305">The summarization is applied to an LDA topic model learned in step 3.</span></span> <span data-ttu-id="eaa3d-306">It shows how to measure the importance or quality of a topic using topic to document purity measure.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-306">It shows how to measure the importance or quality of a topic using topic to document purity measure.</span></span> <span data-ttu-id="eaa3d-307">This purity measure assumes latent topics that dominate the documents in which they appear are more semantically important than latent topics that are weakly spread across many documents.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-307">This purity measure assumes latent topics that dominate the documents in which they appear are more semantically important than latent topics that are weakly spread across many documents.</span></span> <span data-ttu-id="eaa3d-308">This concept was introduced in the paper "[Latent Topic Modeling for Audio Corpus Summarization](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf)."</span><span class="sxs-lookup"><span data-stu-id="eaa3d-308">This concept was introduced in the paper "[Latent Topic Modeling for Audio Corpus Summarization](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf)."</span></span>

<span data-ttu-id="eaa3d-309">Notebook `5_Topic_Model_Analysis.ipynb` shows how to analyze the topical content of a collection of documents and correlate topical information against other meta-data associated with the document collection.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-309">Notebook `5_Topic_Model_Analysis.ipynb` shows how to analyze the topical content of a collection of documents and correlate topical information against other meta-data associated with the document collection.</span></span> <span data-ttu-id="eaa3d-310">A few plots are introduced in this notebook to help the users better understand the learned topic and the document collection.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-310">A few plots are introduced in this notebook to help the users better understand the learned topic and the document collection.</span></span>

<span data-ttu-id="eaa3d-311">Notebook `6_Interactive_Visualization.ipynb` shows how to interactively visualize learned topic model.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-311">Notebook `6_Interactive_Visualization.ipynb` shows how to interactively visualize learned topic model.</span></span> <span data-ttu-id="eaa3d-312">It includes four interactive visualization tasks.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-312">It includes four interactive visualization tasks.</span></span>

## <a name="conclusion"></a><span data-ttu-id="eaa3d-313">Conclusion</span><span class="sxs-lookup"><span data-stu-id="eaa3d-313">Conclusion</span></span>

<span data-ttu-id="eaa3d-314">This real world scenario highlights how to use well-known text analytics techniques (in this case, phrase learning and LDA topic modeling) to produce a robust model, and how Azure Machine Learning Workbench can help track model performance and seamlessly run learners at higher scale.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-314">This real world scenario highlights how to use well-known text analytics techniques (in this case, phrase learning and LDA topic modeling) to produce a robust model, and how Azure Machine Learning Workbench can help track model performance and seamlessly run learners at higher scale.</span></span> <span data-ttu-id="eaa3d-315">In more detail:</span><span class="sxs-lookup"><span data-stu-id="eaa3d-315">In more detail:</span></span>

* <span data-ttu-id="eaa3d-316">Use phrase learning and topic modeling to process a collection of documents and build a robust model.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-316">Use phrase learning and topic modeling to process a collection of documents and build a robust model.</span></span> <span data-ttu-id="eaa3d-317">If the collection of documents is huge, Azure Machine Learning Workbench can easily scale it up and out. Additionally, users have the freedom to run experiments on different compute context easily from within Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-317">If the collection of documents is huge, Azure Machine Learning Workbench can easily scale it up and out. Additionally, users have the freedom to run experiments on different compute context easily from within Azure Machine Learning Workbench.</span></span>

* <span data-ttu-id="eaa3d-318">Azure Machine Learning Workbench provides both options to run notebooks in a step by step manner and Python script to run an entire experiment.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-318">Azure Machine Learning Workbench provides both options to run notebooks in a step by step manner and Python script to run an entire experiment.</span></span>

* <span data-ttu-id="eaa3d-319">Hyper-parameter tuning using Azure Machine Learning Workbench to find the best number of topics needed to learn.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-319">Hyper-parameter tuning using Azure Machine Learning Workbench to find the best number of topics needed to learn.</span></span> <span data-ttu-id="eaa3d-320">Azure Machine Learning Workbench can help track the model performance and seamlessly run different learners at higher scale.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-320">Azure Machine Learning Workbench can help track the model performance and seamlessly run different learners at higher scale.</span></span>

* <span data-ttu-id="eaa3d-321">Azure Machine Learning Workbench can manage the run history and learned models.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-321">Azure Machine Learning Workbench can manage the run history and learned models.</span></span> <span data-ttu-id="eaa3d-322">It enables data scientists to quickly identify the best performing models, and to find the scripts and data used to generate them.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-322">It enables data scientists to quickly identify the best performing models, and to find the scripts and data used to generate them.</span></span>

## <a name="references"></a><span data-ttu-id="eaa3d-323">References</span><span class="sxs-lookup"><span data-stu-id="eaa3d-323">References</span></span>

* <span data-ttu-id="eaa3d-324">**Timothy J. Hazen, Fred Richardson**, [_Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech_](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-324">**Timothy J. Hazen, Fred Richardson**, [_Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech_](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf).</span></span> <span data-ttu-id="eaa3d-325">Spoken Language Technology Workshop (SLT), 2012 IEEE.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-325">Spoken Language Technology Workshop (SLT), 2012 IEEE.</span></span> <span data-ttu-id="eaa3d-326">IEEE, 2012.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-326">IEEE, 2012.</span></span>

* <span data-ttu-id="eaa3d-327">**Timothy J. Hazen**, [_Latent Topic Modeling for Audio Corpus Summarization_](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-327">**Timothy J. Hazen**, [_Latent Topic Modeling for Audio Corpus Summarization_](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf).</span></span> <span data-ttu-id="eaa3d-328">12th Annual Conference of the International Speech Communication Association.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-328">12th Annual Conference of the International Speech Communication Association.</span></span> <span data-ttu-id="eaa3d-329">2011.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-329">2011.</span></span>

* <span data-ttu-id="eaa3d-330">**Michael Roder, Andreas Both, Alexander Hinneburg**, [_Exploring the Space of Topic Coherence Measures_](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).</span><span class="sxs-lookup"><span data-stu-id="eaa3d-330">**Michael Roder, Andreas Both, Alexander Hinneburg**, [_Exploring the Space of Topic Coherence Measures_](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).</span></span> <span data-ttu-id="eaa3d-331">Proceedings of the eighth ACM international conference on Web search and data mining.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-331">Proceedings of the eighth ACM international conference on Web search and data mining.</span></span> <span data-ttu-id="eaa3d-332">ACM, 2015.</span><span class="sxs-lookup"><span data-stu-id="eaa3d-332">ACM, 2015.</span></span>
