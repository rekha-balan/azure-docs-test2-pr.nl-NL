---
title: Feature engineering and selection in Azure Machine Learning | Microsoft Docs
description: Explains the purposes of feature selection and feature engineering and provides examples of their role in the data-enhancement process of machine learning.
services: machine-learning
documentationcenter: ''
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 9ceb524d-842e-4f77-9eae-a18e599442d6
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: deprecated
ms.date: 01/18/2017
ms.author: zhangya;bradsev
ROBOTS: NOINDEX, NOFOLLOW
redirect_url: machine-learning-data-science-create-features
ms.openlocfilehash: 47b81b61ab629668d91fd38da56951cc295609c2
ms.sourcegitcommit: 5b9d839c0c0a94b293fdafe1d6e5429506c07e05
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/02/2018
ms.locfileid: "44662559"
---
# <a name="feature-engineering-and-selection-in-azure-machine-learning"></a><span data-ttu-id="00779-103">Feature engineering and selection in Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="00779-103">Feature engineering and selection in Azure Machine Learning</span></span>
<span data-ttu-id="00779-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span><span class="sxs-lookup"><span data-stu-id="00779-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span></span> <span data-ttu-id="00779-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="00779-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="00779-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span><span class="sxs-lookup"><span data-stu-id="00779-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span></span> <span data-ttu-id="00779-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span><span class="sxs-lookup"><span data-stu-id="00779-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="00779-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span><span class="sxs-lookup"><span data-stu-id="00779-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span></span>

<span data-ttu-id="00779-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span><span class="sxs-lookup"><span data-stu-id="00779-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="00779-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span><span class="sxs-lookup"><span data-stu-id="00779-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="00779-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span><span class="sxs-lookup"><span data-stu-id="00779-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="00779-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span><span class="sxs-lookup"><span data-stu-id="00779-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="00779-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span><span class="sxs-lookup"><span data-stu-id="00779-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="00779-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span><span class="sxs-lookup"><span data-stu-id="00779-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span></span>

* <span data-ttu-id="00779-115">Data collection</span><span class="sxs-lookup"><span data-stu-id="00779-115">Data collection</span></span>
* <span data-ttu-id="00779-116">Data enhancement</span><span class="sxs-lookup"><span data-stu-id="00779-116">Data enhancement</span></span>
* <span data-ttu-id="00779-117">Model construction</span><span class="sxs-lookup"><span data-stu-id="00779-117">Model construction</span></span>
* <span data-ttu-id="00779-118">Post-processing</span><span class="sxs-lookup"><span data-stu-id="00779-118">Post-processing</span></span>

<span data-ttu-id="00779-119">Engineering and selection make up the data enhancement step of machine learning.</span><span class="sxs-lookup"><span data-stu-id="00779-119">Engineering and selection make up the data enhancement step of machine learning.</span></span> <span data-ttu-id="00779-120">Three aspects of this process may be distinguished for our purposes:</span><span class="sxs-lookup"><span data-stu-id="00779-120">Three aspects of this process may be distinguished for our purposes:</span></span>

* <span data-ttu-id="00779-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span><span class="sxs-lookup"><span data-stu-id="00779-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span></span> <span data-ttu-id="00779-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span><span class="sxs-lookup"><span data-stu-id="00779-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span></span>
* <span data-ttu-id="00779-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span><span class="sxs-lookup"><span data-stu-id="00779-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="00779-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span><span class="sxs-lookup"><span data-stu-id="00779-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="00779-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span><span class="sxs-lookup"><span data-stu-id="00779-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span></span> <span data-ttu-id="00779-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span><span class="sxs-lookup"><span data-stu-id="00779-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span></span>

## <a name="creating-features-from-your-data--feature-engineering"></a><span data-ttu-id="00779-127">Creating features from your data--feature engineering</span><span class="sxs-lookup"><span data-stu-id="00779-127">Creating features from your data--feature engineering</span></span>
<span data-ttu-id="00779-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span><span class="sxs-lookup"><span data-stu-id="00779-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="00779-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span><span class="sxs-lookup"><span data-stu-id="00779-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="00779-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span><span class="sxs-lookup"><span data-stu-id="00779-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span></span>

<span data-ttu-id="00779-131">What kind of features should be created to enhance the data set when training a model?</span><span class="sxs-lookup"><span data-stu-id="00779-131">What kind of features should be created to enhance the data set when training a model?</span></span> <span data-ttu-id="00779-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span><span class="sxs-lookup"><span data-stu-id="00779-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="00779-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span><span class="sxs-lookup"><span data-stu-id="00779-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span></span> <span data-ttu-id="00779-134">Sound and productive decisions often require some domain expertise.</span><span class="sxs-lookup"><span data-stu-id="00779-134">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="00779-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="00779-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span></span> <span data-ttu-id="00779-136">Two examples are presented here:</span><span class="sxs-lookup"><span data-stu-id="00779-136">Two examples are presented here:</span></span>

* <span data-ttu-id="00779-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span><span class="sxs-lookup"><span data-stu-id="00779-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="00779-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span><span class="sxs-lookup"><span data-stu-id="00779-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span></span>

### <a name="example-1-adding-temporal-features-for-a-regression-model"></a><span data-ttu-id="00779-139">Example 1: Adding temporal features for a regression model</span><span class="sxs-lookup"><span data-stu-id="00779-139">Example 1: Adding temporal features for a regression model</span></span>
<span data-ttu-id="00779-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="00779-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span></span> <span data-ttu-id="00779-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span><span class="sxs-lookup"><span data-stu-id="00779-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span></span> <span data-ttu-id="00779-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span><span class="sxs-lookup"><span data-stu-id="00779-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span></span>

<span data-ttu-id="00779-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span><span class="sxs-lookup"><span data-stu-id="00779-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="00779-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span><span class="sxs-lookup"><span data-stu-id="00779-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="00779-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span><span class="sxs-lookup"><span data-stu-id="00779-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span></span> <span data-ttu-id="00779-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span><span class="sxs-lookup"><span data-stu-id="00779-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span></span>

<span data-ttu-id="00779-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span><span class="sxs-lookup"><span data-stu-id="00779-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span></span> <span data-ttu-id="00779-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span><span class="sxs-lookup"><span data-stu-id="00779-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="00779-149">These features are grouped into four categories:</span><span class="sxs-lookup"><span data-stu-id="00779-149">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="00779-150">A = weather + holiday + weekday + weekend features for the predicted day</span><span class="sxs-lookup"><span data-stu-id="00779-150">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="00779-151">B = number of bikes that were rented in each of the previous 12 hours</span><span class="sxs-lookup"><span data-stu-id="00779-151">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="00779-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span><span class="sxs-lookup"><span data-stu-id="00779-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="00779-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span><span class="sxs-lookup"><span data-stu-id="00779-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="00779-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span><span class="sxs-lookup"><span data-stu-id="00779-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="00779-155">Feature set B captures the recent demand for the bikes.</span><span class="sxs-lookup"><span data-stu-id="00779-155">Feature set B captures the recent demand for the bikes.</span></span> <span data-ttu-id="00779-156">Feature set C captures the demand for bikes at a particular hour.</span><span class="sxs-lookup"><span data-stu-id="00779-156">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="00779-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span><span class="sxs-lookup"><span data-stu-id="00779-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="00779-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span><span class="sxs-lookup"><span data-stu-id="00779-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="00779-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span><span class="sxs-lookup"><span data-stu-id="00779-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span></span> <span data-ttu-id="00779-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span><span class="sxs-lookup"><span data-stu-id="00779-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span></span> <span data-ttu-id="00779-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span><span class="sxs-lookup"><span data-stu-id="00779-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![Create a feature set](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/addFeature-Rscripts.png)

<span data-ttu-id="00779-163">The following table summarizes the comparison of the performance results of the four models.</span><span class="sxs-lookup"><span data-stu-id="00779-163">The following table summarizes the comparison of the performance results of the four models.</span></span> <span data-ttu-id="00779-164">The best results are shown by features A+B+C.</span><span class="sxs-lookup"><span data-stu-id="00779-164">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="00779-165">Note that the error rate decreases when additional feature sets are included in the training data.</span><span class="sxs-lookup"><span data-stu-id="00779-165">Note that the error rate decreases when additional feature sets are included in the training data.</span></span> <span data-ttu-id="00779-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span><span class="sxs-lookup"><span data-stu-id="00779-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="00779-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span><span class="sxs-lookup"><span data-stu-id="00779-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span></span>

![Compare performance results](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/result1.png)

### <a name="example2"></a> <span data-ttu-id="00779-169">Example 2: Creating features in text mining</span><span class="sxs-lookup"><span data-stu-id="00779-169">Example 2: Creating features in text mining</span></span>
<span data-ttu-id="00779-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span><span class="sxs-lookup"><span data-stu-id="00779-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="00779-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span><span class="sxs-lookup"><span data-stu-id="00779-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span></span> <span data-ttu-id="00779-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span><span class="sxs-lookup"><span data-stu-id="00779-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span></span> <span data-ttu-id="00779-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span><span class="sxs-lookup"><span data-stu-id="00779-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span></span>

<span data-ttu-id="00779-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span><span class="sxs-lookup"><span data-stu-id="00779-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="00779-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span><span class="sxs-lookup"><span data-stu-id="00779-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span></span>

<span data-ttu-id="00779-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span><span class="sxs-lookup"><span data-stu-id="00779-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span></span> <span data-ttu-id="00779-177">The following figure shows an example of using this module.</span><span class="sxs-lookup"><span data-stu-id="00779-177">The following figure shows an example of using this module.</span></span> <span data-ttu-id="00779-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span><span class="sxs-lookup"><span data-stu-id="00779-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span></span> <span data-ttu-id="00779-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span><span class="sxs-lookup"><span data-stu-id="00779-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span></span> <span data-ttu-id="00779-180">To use this module, you need to complete the following steps:</span><span class="sxs-lookup"><span data-stu-id="00779-180">To use this module, you need to complete the following steps:</span></span>

1. <span data-ttu-id="00779-181">Select the column that contains the input text (**Col2** in this example).</span><span class="sxs-lookup"><span data-stu-id="00779-181">Select the column that contains the input text (**Col2** in this example).</span></span>
2. <span data-ttu-id="00779-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span><span class="sxs-lookup"><span data-stu-id="00779-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span></span> <span data-ttu-id="00779-183">The word or phrase in the text is then hashed to 256 indices.</span><span class="sxs-lookup"><span data-stu-id="00779-183">The word or phrase in the text is then hashed to 256 indices.</span></span> <span data-ttu-id="00779-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span><span class="sxs-lookup"><span data-stu-id="00779-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span></span> <span data-ttu-id="00779-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span><span class="sxs-lookup"><span data-stu-id="00779-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span></span>
3. <span data-ttu-id="00779-186">Set the parameter *N-grams* to 2.</span><span class="sxs-lookup"><span data-stu-id="00779-186">Set the parameter *N-grams* to 2.</span></span> <span data-ttu-id="00779-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span><span class="sxs-lookup"><span data-stu-id="00779-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="00779-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span><span class="sxs-lookup"><span data-stu-id="00779-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

![Feature hashing module](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Hashing1.png)

<span data-ttu-id="00779-190">The following figure shows what these new features look like.</span><span class="sxs-lookup"><span data-stu-id="00779-190">The following figure shows what these new features look like.</span></span>

![Feature hashing example](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Hashing2.png)

## <a name="filtering-features-from-your-data--feature-selection"></a><span data-ttu-id="00779-192">Filtering features from your data--feature selection</span><span class="sxs-lookup"><span data-stu-id="00779-192">Filtering features from your data--feature selection</span></span>
<span data-ttu-id="00779-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span><span class="sxs-lookup"><span data-stu-id="00779-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="00779-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span><span class="sxs-lookup"><span data-stu-id="00779-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="00779-195">This subset of features contains the only features to be included to train the model.</span><span class="sxs-lookup"><span data-stu-id="00779-195">This subset of features contains the only features to be included to train the model.</span></span> <span data-ttu-id="00779-196">Feature selection serves two main purposes:</span><span class="sxs-lookup"><span data-stu-id="00779-196">Feature selection serves two main purposes:</span></span>

* <span data-ttu-id="00779-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span><span class="sxs-lookup"><span data-stu-id="00779-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="00779-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span><span class="sxs-lookup"><span data-stu-id="00779-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span></span> <span data-ttu-id="00779-199">This is particularly important for learners that are expensive to train such as support vector machines.</span><span class="sxs-lookup"><span data-stu-id="00779-199">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="00779-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span><span class="sxs-lookup"><span data-stu-id="00779-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span></span> <span data-ttu-id="00779-201">Feature selection methods extract a subset of original features in the data without changing them.</span><span class="sxs-lookup"><span data-stu-id="00779-201">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="00779-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span><span class="sxs-lookup"><span data-stu-id="00779-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="00779-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span><span class="sxs-lookup"><span data-stu-id="00779-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span></span>

<span data-ttu-id="00779-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span><span class="sxs-lookup"><span data-stu-id="00779-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span></span> <span data-ttu-id="00779-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span><span class="sxs-lookup"><span data-stu-id="00779-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="00779-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span><span class="sxs-lookup"><span data-stu-id="00779-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="00779-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span><span class="sxs-lookup"><span data-stu-id="00779-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span></span>

<span data-ttu-id="00779-208">Azure Machine Learning Studio provides modules for feature selection.</span><span class="sxs-lookup"><span data-stu-id="00779-208">Azure Machine Learning Studio provides modules for feature selection.</span></span> <span data-ttu-id="00779-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="00779-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Feature selection example](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Selection.png)

<span data-ttu-id="00779-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span><span class="sxs-lookup"><span data-stu-id="00779-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span></span> <span data-ttu-id="00779-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span><span class="sxs-lookup"><span data-stu-id="00779-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span></span> <span data-ttu-id="00779-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span><span class="sxs-lookup"><span data-stu-id="00779-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span></span> <span data-ttu-id="00779-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span><span class="sxs-lookup"><span data-stu-id="00779-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span></span> <span data-ttu-id="00779-215">The following figure shows the flow of this experiment and the input parameters.</span><span class="sxs-lookup"><span data-stu-id="00779-215">The following figure shows the flow of this experiment and the input parameters.</span></span>

![Feature selection example](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Selection1.png)

<span data-ttu-id="00779-217">The following figure shows the resulting data sets.</span><span class="sxs-lookup"><span data-stu-id="00779-217">The following figure shows the resulting data sets.</span></span> <span data-ttu-id="00779-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span><span class="sxs-lookup"><span data-stu-id="00779-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span></span> <span data-ttu-id="00779-219">The features with top scores are kept.</span><span class="sxs-lookup"><span data-stu-id="00779-219">The features with top scores are kept.</span></span>

![Filter-based feature selection data sets](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Selection2.png)

<span data-ttu-id="00779-221">The following figure shows the corresponding scores of the selected features.</span><span class="sxs-lookup"><span data-stu-id="00779-221">The following figure shows the corresponding scores of the selected features.</span></span>

![Selected feature scores](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-feature-selection-and-engineering/feature-Selection3.png)

<span data-ttu-id="00779-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span><span class="sxs-lookup"><span data-stu-id="00779-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span></span>

## <a name="conclusion"></a><span data-ttu-id="00779-224">Conclusion</span><span class="sxs-lookup"><span data-stu-id="00779-224">Conclusion</span></span>
<span data-ttu-id="00779-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span><span class="sxs-lookup"><span data-stu-id="00779-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span></span> <span data-ttu-id="00779-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span><span class="sxs-lookup"><span data-stu-id="00779-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="00779-227">It is not always necessarily to perform feature engineering or feature selection.</span><span class="sxs-lookup"><span data-stu-id="00779-227">It is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="00779-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span><span class="sxs-lookup"><span data-stu-id="00779-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[execute-r-script]: https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/








