---
title: Feature engineering in data science | Microsoft Docs
description: Explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning.
services: machine-learning
documentationcenter: ''
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 3fde69e8-5e7b-49ad-b3fb-ab8ef6503a4d
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: 978fc8c5d81c197aaa550d81b7705dca2d9ab958
ms.sourcegitcommit: 5b9d839c0c0a94b293fdafe1d6e5429506c07e05
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/02/2018
ms.locfileid: "44552030"
---
# <a name="feature-engineering-in-data-science"></a><span data-ttu-id="f5b1d-103">Feature engineering in data science</span><span class="sxs-lookup"><span data-stu-id="f5b1d-103">Feature engineering in data science</span></span>
<span data-ttu-id="f5b1d-104">This topic explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-104">This topic explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning.</span></span> <span data-ttu-id="f5b1d-105">The examples used to illustrate this process are drawn from Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-105">The examples used to illustrate this process are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [cap-create-features-data-selector](../../includes/cap-create-features-selector.md)]

<span data-ttu-id="f5b1d-106">This **menu** links to topics that describe how to create features for data in various environments.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-106">This **menu** links to topics that describe how to create features for data in various environments.</span></span> <span data-ttu-id="f5b1d-107">This task is a step in the [Team Data Science Process (TDSP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/).</span><span class="sxs-lookup"><span data-stu-id="f5b1d-107">This task is a step in the [Team Data Science Process (TDSP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/).</span></span>

<span data-ttu-id="f5b1d-108">Feature engineering attempts to increase the predictive power of learning algorithms by creating features from raw data that help facilitate the learning process.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-108">Feature engineering attempts to increase the predictive power of learning algorithms by creating features from raw data that help facilitate the learning process.</span></span> <span data-ttu-id="f5b1d-109">The engineering and selection of features is one part of the TDSP outlined in the [What is the Team Data Science Process lifecycle?](data-science-process-overview.md)</span><span class="sxs-lookup"><span data-stu-id="f5b1d-109">The engineering and selection of features is one part of the TDSP outlined in the [What is the Team Data Science Process lifecycle?](data-science-process-overview.md)</span></span> <span data-ttu-id="f5b1d-110">Feature engineering and selection are parts of the **Develop features** step of the TDSP.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-110">Feature engineering and selection are parts of the **Develop features** step of the TDSP.</span></span> 

* <span data-ttu-id="f5b1d-111">**feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data, and to increase the predictive power of the learning algorithm.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-111">**feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data, and to increase the predictive power of the learning algorithm.</span></span>
* <span data-ttu-id="f5b1d-112">**feature selection**: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-112">**feature selection**: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="f5b1d-113">Normally **feature engineering** is applied first to generate additional features, and then the **feature selection** step is performed to eliminate irrelevant, redundant, or highly correlated features.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-113">Normally **feature engineering** is applied first to generate additional features, and then the **feature selection** step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="f5b1d-114">The training data used in machine learning can often be enhanced by extraction of features from the raw data collected.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-114">The training data used in machine learning can often be enhanced by extraction of features from the raw data collected.</span></span> <span data-ttu-id="f5b1d-115">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is creation of a bit density map constructed from the raw bit distribution data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-115">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is creation of a bit density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="f5b1d-116">This map can help locate the edges of the characters more efficiently than simply using the raw distribution directly.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-116">This map can help locate the edges of the characters more efficiently than simply using the raw distribution directly.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

## <a name="creating-features-from-your-data---feature-engineering"></a><span data-ttu-id="f5b1d-117">Creating Features from Your Data - Feature Engineering</span><span class="sxs-lookup"><span data-stu-id="f5b1d-117">Creating Features from Your Data - Feature Engineering</span></span>
<span data-ttu-id="f5b1d-118">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span><span class="sxs-lookup"><span data-stu-id="f5b1d-118">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="f5b1d-119">The features specified in the experimental design are expected to characterize the patterns in the data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-119">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="f5b1d-120">Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-120">Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.</span></span>

<span data-ttu-id="f5b1d-121">What kind of features should be created to enhance the dataset when training a model?</span><span class="sxs-lookup"><span data-stu-id="f5b1d-121">What kind of features should be created to enhance the dataset when training a model?</span></span> <span data-ttu-id="f5b1d-122">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-122">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="f5b1d-123">We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-123">We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set.</span></span> <span data-ttu-id="f5b1d-124">But this process is something of an art.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-124">But this process is something of an art.</span></span> <span data-ttu-id="f5b1d-125">Sound and productive decisions often require some domain expertise.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-125">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="f5b1d-126">When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-126">When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio.</span></span> <span data-ttu-id="f5b1d-127">Two examples are presented here:</span><span class="sxs-lookup"><span data-stu-id="f5b1d-127">Two examples are presented here:</span></span>

* <span data-ttu-id="f5b1d-128">A regression example [Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4) in a supervised experiment where the target values are known</span><span class="sxs-lookup"><span data-stu-id="f5b1d-128">A regression example [Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="f5b1d-129">A text mining classification example using [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</span><span class="sxs-lookup"><span data-stu-id="f5b1d-129">A text mining classification example using [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</span></span>

## <a name="example-1-adding-temporal-features-for-regression-model"></a><span data-ttu-id="f5b1d-130">Example 1: Adding Temporal Features for Regression Model</span><span class="sxs-lookup"><span data-stu-id="f5b1d-130">Example 1: Adding Temporal Features for Regression Model</span></span>
<span data-ttu-id="f5b1d-131">Let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-131">Let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task.</span></span> <span data-ttu-id="f5b1d-132">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-132">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour.</span></span> <span data-ttu-id="f5b1d-133">The dataset "Bike Rental UCI dataset" is used as the raw input data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-133">The dataset "Bike Rental UCI dataset" is used as the raw input data.</span></span> <span data-ttu-id="f5b1d-134">This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-134">This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="f5b1d-135">The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-135">The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="f5b1d-136">The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday).</span><span class="sxs-lookup"><span data-stu-id="f5b1d-136">The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday).</span></span> <span data-ttu-id="f5b1d-137">The field to predict is "cnt", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-137">The field to predict is "cnt", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.</span></span>

<span data-ttu-id="f5b1d-138">With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-138">With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets.</span></span> <span data-ttu-id="f5b1d-139">The four datasets represent the same raw input data, but with an increasing number of features set.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-139">The four datasets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="f5b1d-140">These features are grouped into four categories:</span><span class="sxs-lookup"><span data-stu-id="f5b1d-140">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="f5b1d-141">A = weather + holiday + weekday + weekend features for the predicted day</span><span class="sxs-lookup"><span data-stu-id="f5b1d-141">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="f5b1d-142">B = number of bikes that were rented in each of the previous 12 hours</span><span class="sxs-lookup"><span data-stu-id="f5b1d-142">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="f5b1d-143">C = number of bikes that were rented in each of the previous 12 days at the same hour</span><span class="sxs-lookup"><span data-stu-id="f5b1d-143">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="f5b1d-144">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span><span class="sxs-lookup"><span data-stu-id="f5b1d-144">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="f5b1d-145">Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-145">Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="f5b1d-146">Feature set B captures very recent demand for the bikes.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-146">Feature set B captures very recent demand for the bikes.</span></span> <span data-ttu-id="f5b1d-147">Feature set C captures the demand for bikes at a particular hour.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-147">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="f5b1d-148">Feature set D captures demand for bikes at particular hour and particular day of the week.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-148">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="f5b1d-149">The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-149">The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="f5b1d-150">In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-150">In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset.</span></span> <span data-ttu-id="f5b1d-151">Except the left most branch, each of these branches contains an [Execute R Script](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/) module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-151">Except the left most branch, each of these branches contains an [Execute R Script](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/) module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset.</span></span> <span data-ttu-id="f5b1d-152">The following figure demonstrates the R script used to create feature set B in the second left branch.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-152">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![create features](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-data-science-create-features/addFeature-Rscripts.png)

<span data-ttu-id="f5b1d-154">The comparison of the performance results of the four models are summarized in the following table.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-154">The comparison of the performance results of the four models are summarized in the following table.</span></span> <span data-ttu-id="f5b1d-155">The best results are shown by features A+B+C.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-155">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="f5b1d-156">Note that the error rate decreases when additional feature set are included in the training data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-156">Note that the error rate decreases when additional feature set are included in the training data.</span></span> <span data-ttu-id="f5b1d-157">It verifies our presumption that the feature set B, C provide additional relevant information for the regression task.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-157">It verifies our presumption that the feature set B, C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="f5b1d-158">But adding the D feature does not seem to provide any additional reduction in the error rate.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-158">But adding the D feature does not seem to provide any additional reduction in the error rate.</span></span>

![result comparison](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-data-science-create-features/result1.png)

## <a name="example2"></a> <span data-ttu-id="f5b1d-160">Example 2: Creating Features in Text Mining</span><span class="sxs-lookup"><span data-stu-id="f5b1d-160">Example 2: Creating Features in Text Mining</span></span>
<span data-ttu-id="f5b1d-161">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-161">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="f5b1d-162">For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-162">For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category.</span></span> <span data-ttu-id="f5b1d-163">In another words, the frequency of the words/phrases distribution is able to characterize different document categories.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-163">In another words, the frequency of the words/phrases distribution is able to characterize different document categories.</span></span> <span data-ttu-id="f5b1d-164">In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-164">In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.</span></span>

<span data-ttu-id="f5b1d-165">To achieve this task, a technique called **feature hashing** is applied to efficiently turn arbitrary text features into indices.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-165">To achieve this task, a technique called **feature hashing** is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="f5b1d-166">Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-166">Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.</span></span>

<span data-ttu-id="f5b1d-167">In Azure Machine Learning, there is a [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module that creates these word/phrase features conveniently.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-167">In Azure Machine Learning, there is a [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module that creates these word/phrase features conveniently.</span></span> <span data-ttu-id="f5b1d-168">Following figure shows an example of using this module.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-168">Following figure shows an example of using this module.</span></span> <span data-ttu-id="f5b1d-169">The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-169">The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content.</span></span> <span data-ttu-id="f5b1d-170">The goal of this [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-170">The goal of this [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review.</span></span> <span data-ttu-id="f5b1d-171">To use this module, we need to complete the following steps:</span><span class="sxs-lookup"><span data-stu-id="f5b1d-171">To use this module, we need to complete the following steps:</span></span>

* <span data-ttu-id="f5b1d-172">First, select the column that contains the input text ("Col2" in this example).</span><span class="sxs-lookup"><span data-stu-id="f5b1d-172">First, select the column that contains the input text ("Col2" in this example).</span></span>
* <span data-ttu-id="f5b1d-173">Second, set the "Hashing bitsize" to 8, which means 2^8=256 features will be created.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-173">Second, set the "Hashing bitsize" to 8, which means 2^8=256 features will be created.</span></span> <span data-ttu-id="f5b1d-174">The word/phase in all the text will be hashed to 256 indices.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-174">The word/phase in all the text will be hashed to 256 indices.</span></span> <span data-ttu-id="f5b1d-175">The parameter "Hashing bitsize" ranges from 1 to 31.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-175">The parameter "Hashing bitsize" ranges from 1 to 31.</span></span> <span data-ttu-id="f5b1d-176">The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-176">The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.</span></span>
* <span data-ttu-id="f5b1d-177">Third, set the parameter "N-grams" to 2.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-177">Third, set the parameter "N-grams" to 2.</span></span> <span data-ttu-id="f5b1d-178">This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-178">This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="f5b1d-179">The parameter "N-grams" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-179">The parameter "N-grams" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

!["Feature Hashing" module](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-data-science-create-features/feature-Hashing1.png)

<span data-ttu-id="f5b1d-181">The following figure shows what the these new feature look like.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-181">The following figure shows what the these new feature look like.</span></span>

!["Feature Hashing" example](https://docstestmedia1.blob.core.windows.net/azure-media/articles/machine-learning/media/machine-learning-data-science-create-features/feature-Hashing2.png)

## <a name="conclusion"></a><span data-ttu-id="f5b1d-183">Conclusion</span><span class="sxs-lookup"><span data-stu-id="f5b1d-183">Conclusion</span></span>
<span data-ttu-id="f5b1d-184">Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-184">Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="f5b1d-185">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-185">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="f5b1d-186">Feature engineering and selection can also combine to make the learning more computationally tractable.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-186">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="f5b1d-187">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-187">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="f5b1d-188">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-188">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="f5b1d-189">Note that it is not always necessarily to perform feature engineering or feature selection.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-189">Note that it is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="f5b1d-190">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</span><span class="sxs-lookup"><span data-stu-id="f5b1d-190">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</span></span>





