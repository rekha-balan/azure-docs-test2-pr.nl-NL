---
title: Set up a Windows RDMA cluster to run MPI applications | Microsoft Docs
description: Learn how to create a Windows HPC Pack cluster with size H16r, H16mr, A8, or A9 VMs to use the Azure RDMA network to run MPI apps.
services: virtual-machines-windows
documentationcenter: ''
author: dlepow
manager: timlt
editor: ''
tags: azure-service-management,hpc-pack
ms.assetid: 7d9f5bc8-012f-48dd-b290-db81c7592215
ms.service: virtual-machines-windows
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: vm-windows
ms.workload: big-compute
ms.date: 12/29/2016
ms.author: danlep
ms.openlocfilehash: 5bbe1294f06d5152ef3167631ff872fcd8a3d669
ms.sourcegitcommit: 5b9d839c0c0a94b293fdafe1d6e5429506c07e05
ms.translationtype: HT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/02/2018
ms.locfileid: "44562972"
---
# <a name="set-up-a-windows-rdma-cluster-with-hpc-pack-to-run-mpi-applications"></a><span data-ttu-id="29ec9-103">Set up a Windows RDMA cluster with HPC Pack to run MPI applications</span><span class="sxs-lookup"><span data-stu-id="29ec9-103">Set up a Windows RDMA cluster with HPC Pack to run MPI applications</span></span>
<span data-ttu-id="29ec9-104">Set up a Windows RDMA cluster in Azure with [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) and [H-series or compute-intensive A-series instances](../../virtual-machines-windows-a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) to run parallel Message Passing Interface (MPI) applications.</span><span class="sxs-lookup"><span data-stu-id="29ec9-104">Set up a Windows RDMA cluster in Azure with [Microsoft HPC Pack](https://technet.microsoft.com/library/cc514029) and [H-series or compute-intensive A-series instances](../../virtual-machines-windows-a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json) to run parallel Message Passing Interface (MPI) applications.</span></span> <span data-ttu-id="29ec9-105">When you set up RDMA-capable, Windows Server-based nodes in an HPC Pack cluster, MPI applications communicate efficiently over a low latency, high throughput network in Azure that is based on remote direct memory access (RDMA) technology.</span><span class="sxs-lookup"><span data-stu-id="29ec9-105">When you set up RDMA-capable, Windows Server-based nodes in an HPC Pack cluster, MPI applications communicate efficiently over a low latency, high throughput network in Azure that is based on remote direct memory access (RDMA) technology.</span></span>

<span data-ttu-id="29ec9-106">If you want to run MPI workloads on Linux VMs that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="29ec9-106">If you want to run MPI workloads on Linux VMs that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

## <a name="hpc-pack-cluster-deployment-options"></a><span data-ttu-id="29ec9-107">HPC Pack cluster deployment options</span><span class="sxs-lookup"><span data-stu-id="29ec9-107">HPC Pack cluster deployment options</span></span>
<span data-ttu-id="29ec9-108">Microsoft HPC Pack is a tool provided at no additional cost to create HPC clusters on-premises or in Azure to run Windows or Linux HPC applications.</span><span class="sxs-lookup"><span data-stu-id="29ec9-108">Microsoft HPC Pack is a tool provided at no additional cost to create HPC clusters on-premises or in Azure to run Windows or Linux HPC applications.</span></span> <span data-ttu-id="29ec9-109">HPC Pack includes a runtime environment for the Microsoft implementation of the Message Passing Interface for Windows (MS-MPI).</span><span class="sxs-lookup"><span data-stu-id="29ec9-109">HPC Pack includes a runtime environment for the Microsoft implementation of the Message Passing Interface for Windows (MS-MPI).</span></span> <span data-ttu-id="29ec9-110">When used with RDMA-capable instances running a supported Windows Server operating system, HPC Pack provides an efficient option to run Windows MPI applications that access the Azure RDMA network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-110">When used with RDMA-capable instances running a supported Windows Server operating system, HPC Pack provides an efficient option to run Windows MPI applications that access the Azure RDMA network.</span></span> 

<span data-ttu-id="29ec9-111">This article introduces two scenarios and links to detailed guidance to set up a Windows RDMA cluster with Microsoft HPC Pack.</span><span class="sxs-lookup"><span data-stu-id="29ec9-111">This article introduces two scenarios and links to detailed guidance to set up a Windows RDMA cluster with Microsoft HPC Pack.</span></span> 

* <span data-ttu-id="29ec9-112">Scenario 1.</span><span class="sxs-lookup"><span data-stu-id="29ec9-112">Scenario 1.</span></span> <span data-ttu-id="29ec9-113">Deploy compute-intensive worker role instances (PaaS)</span><span class="sxs-lookup"><span data-stu-id="29ec9-113">Deploy compute-intensive worker role instances (PaaS)</span></span>
* <span data-ttu-id="29ec9-114">Scenario 2.</span><span class="sxs-lookup"><span data-stu-id="29ec9-114">Scenario 2.</span></span> <span data-ttu-id="29ec9-115">Deploy compute nodes in compute-intensive VMs (IaaS)</span><span class="sxs-lookup"><span data-stu-id="29ec9-115">Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>

<span data-ttu-id="29ec9-116">For general prerequisites to use compute-intensive instances with Windows, see [About H-series and compute-intensive A-series VMs](../../virtual-machines-windows-a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="29ec9-116">For general prerequisites to use compute-intensive instances with Windows, see [About H-series and compute-intensive A-series VMs](../../virtual-machines-windows-a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>

## <a name="scenario-1-deploy-compute-intensive-worker-role-instances-paas"></a><span data-ttu-id="29ec9-117">Scenario 1: Deploy compute-intensive worker role instances (PaaS)</span><span class="sxs-lookup"><span data-stu-id="29ec9-117">Scenario 1: Deploy compute-intensive worker role instances (PaaS)</span></span>
<span data-ttu-id="29ec9-118">From an existing HPC Pack cluster, add extra compute resources in Azure worker role instances (Azure nodes) running in a cloud service (PaaS).</span><span class="sxs-lookup"><span data-stu-id="29ec9-118">From an existing HPC Pack cluster, add extra compute resources in Azure worker role instances (Azure nodes) running in a cloud service (PaaS).</span></span> <span data-ttu-id="29ec9-119">This feature, also called “burst to Azure” from HPC Pack, supports a range of sizes for the worker role instances.</span><span class="sxs-lookup"><span data-stu-id="29ec9-119">This feature, also called “burst to Azure” from HPC Pack, supports a range of sizes for the worker role instances.</span></span> <span data-ttu-id="29ec9-120">When adding the Azure nodes, specify one of the RDMA-capable sizes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-120">When adding the Azure nodes, specify one of the RDMA-capable sizes.</span></span>

<span data-ttu-id="29ec9-121">Following are considerations and steps to burst to RDMA-capable Azure instances from an existing (typically on-premises) cluster.</span><span class="sxs-lookup"><span data-stu-id="29ec9-121">Following are considerations and steps to burst to RDMA-capable Azure instances from an existing (typically on-premises) cluster.</span></span> <span data-ttu-id="29ec9-122">Use similar procedures to add worker role instances to an HPC Pack head node that is deployed in an Azure VM.</span><span class="sxs-lookup"><span data-stu-id="29ec9-122">Use similar procedures to add worker role instances to an HPC Pack head node that is deployed in an Azure VM.</span></span>

> [!NOTE]
> <span data-ttu-id="29ec9-123">For a tutorial to burst to Azure with HPC Pack, see [Set up a hybrid cluster with HPC Pack](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="29ec9-123">For a tutorial to burst to Azure with HPC Pack, see [Set up a hybrid cluster with HPC Pack](../../../cloud-services/cloud-services-setup-hybrid-hpcpack-cluster.md).</span></span> <span data-ttu-id="29ec9-124">Note the considerations in the following steps that apply specifically to RDMA-capable Azure nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-124">Note the considerations in the following steps that apply specifically to RDMA-capable Azure nodes.</span></span>
> 
> 

![Burst to Azure][burst]

### <a name="steps"></a><span data-ttu-id="29ec9-126">Steps</span><span class="sxs-lookup"><span data-stu-id="29ec9-126">Steps</span></span>
1. <span data-ttu-id="29ec9-127">**Deploy and configure an HPC Pack 2012 R2 head node**</span><span class="sxs-lookup"><span data-stu-id="29ec9-127">**Deploy and configure an HPC Pack 2012 R2 head node**</span></span>
   
    <span data-ttu-id="29ec9-128">Download the latest HPC Pack installation package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="29ec9-128">Download the latest HPC Pack installation package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span> <span data-ttu-id="29ec9-129">For requirements and instructions to prepare for an Azure burst deployment, see [Burst to Azure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx).</span><span class="sxs-lookup"><span data-stu-id="29ec9-129">For requirements and instructions to prepare for an Azure burst deployment, see [Burst to Azure Worker Instances with Microsoft HPC Pack](https://technet.microsoft.com/library/gg481749.aspx).</span></span>
2. <span data-ttu-id="29ec9-130">**Configure a management certificate in the Azure subscription**</span><span class="sxs-lookup"><span data-stu-id="29ec9-130">**Configure a management certificate in the Azure subscription**</span></span>
   
    <span data-ttu-id="29ec9-131">Configure a certificate to secure the connection between the head node and Azure.</span><span class="sxs-lookup"><span data-stu-id="29ec9-131">Configure a certificate to secure the connection between the head node and Azure.</span></span> <span data-ttu-id="29ec9-132">For options and procedures, see [Scenarios to Configure the Azure Management Certificate for HPC Pack](http://technet.microsoft.com/library/gg481759.aspx).</span><span class="sxs-lookup"><span data-stu-id="29ec9-132">For options and procedures, see [Scenarios to Configure the Azure Management Certificate for HPC Pack](http://technet.microsoft.com/library/gg481759.aspx).</span></span> <span data-ttu-id="29ec9-133">For test deployments, HPC Pack installs a Default Microsoft HPC Azure Management Certificate you can quickly upload to your Azure subscription.</span><span class="sxs-lookup"><span data-stu-id="29ec9-133">For test deployments, HPC Pack installs a Default Microsoft HPC Azure Management Certificate you can quickly upload to your Azure subscription.</span></span>
3. <span data-ttu-id="29ec9-134">**Create a new cloud service and a storage account**</span><span class="sxs-lookup"><span data-stu-id="29ec9-134">**Create a new cloud service and a storage account**</span></span>
   
    <span data-ttu-id="29ec9-135">Use the Azure classic portal to create a cloud service and a storage account for the deployment in a region where the RDMA-capable instances are available.</span><span class="sxs-lookup"><span data-stu-id="29ec9-135">Use the Azure classic portal to create a cloud service and a storage account for the deployment in a region where the RDMA-capable instances are available.</span></span>
4. <span data-ttu-id="29ec9-136">**Create an Azure node template**</span><span class="sxs-lookup"><span data-stu-id="29ec9-136">**Create an Azure node template**</span></span>
   
    <span data-ttu-id="29ec9-137">Use the Create Node Template Wizard in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-137">Use the Create Node Template Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-138">For steps, see [Create an Azure node template](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) in “Steps to Deploy Azure Nodes with Microsoft HPC Pack”.</span><span class="sxs-lookup"><span data-stu-id="29ec9-138">For steps, see [Create an Azure node template](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Templ) in “Steps to Deploy Azure Nodes with Microsoft HPC Pack”.</span></span>
   
    <span data-ttu-id="29ec9-139">For initial tests, we suggest configuring a manual availability policy in the template.</span><span class="sxs-lookup"><span data-stu-id="29ec9-139">For initial tests, we suggest configuring a manual availability policy in the template.</span></span>
5. <span data-ttu-id="29ec9-140">**Add nodes to the cluster**</span><span class="sxs-lookup"><span data-stu-id="29ec9-140">**Add nodes to the cluster**</span></span>
   
    <span data-ttu-id="29ec9-141">Use the Add Node Wizard in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-141">Use the Add Node Wizard in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-142">For more information, see [Add Azure Nodes to the Windows HPC Cluster](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span><span class="sxs-lookup"><span data-stu-id="29ec9-142">For more information, see [Add Azure Nodes to the Windows HPC Cluster](http://technet.microsoft.com/library/gg481758.aspx#BKMK_Add).</span></span>
   
    <span data-ttu-id="29ec9-143">When specifying the size of the nodes, select one of the RDMA-capable instance sizes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-143">When specifying the size of the nodes, select one of the RDMA-capable instance sizes.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="29ec9-144">In each burst to Azure deployment with the compute-intensive instances, HPC Pack automatically deploys a minimum of two RDMA-capable instances (such as A8) as proxy nodes, in addition to the Azure worker role instances you specify.</span><span class="sxs-lookup"><span data-stu-id="29ec9-144">In each burst to Azure deployment with the compute-intensive instances, HPC Pack automatically deploys a minimum of two RDMA-capable instances (such as A8) as proxy nodes, in addition to the Azure worker role instances you specify.</span></span> <span data-ttu-id="29ec9-145">The proxy nodes use cores that are allocated to the subscription and incur charges along with the Azure worker role instances.</span><span class="sxs-lookup"><span data-stu-id="29ec9-145">The proxy nodes use cores that are allocated to the subscription and incur charges along with the Azure worker role instances.</span></span>
   > 
   > 
6. <span data-ttu-id="29ec9-146">**Start (provision) the nodes and bring them online to run jobs**</span><span class="sxs-lookup"><span data-stu-id="29ec9-146">**Start (provision) the nodes and bring them online to run jobs**</span></span>
   
    <span data-ttu-id="29ec9-147">Select the nodes and use the **Start** action in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-147">Select the nodes and use the **Start** action in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-148">When provisioning is complete, select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-148">When provisioning is complete, select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-149">The nodes are ready to run jobs.</span><span class="sxs-lookup"><span data-stu-id="29ec9-149">The nodes are ready to run jobs.</span></span>
7. <span data-ttu-id="29ec9-150">**Submit jobs to the cluster**</span><span class="sxs-lookup"><span data-stu-id="29ec9-150">**Submit jobs to the cluster**</span></span>
   
   <span data-ttu-id="29ec9-151">Use HPC Pack job submission tools to run cluster jobs.</span><span class="sxs-lookup"><span data-stu-id="29ec9-151">Use HPC Pack job submission tools to run cluster jobs.</span></span> <span data-ttu-id="29ec9-152">See [Microsoft HPC Pack: Job Management](http://technet.microsoft.com/library/jj899585.aspx).</span><span class="sxs-lookup"><span data-stu-id="29ec9-152">See [Microsoft HPC Pack: Job Management](http://technet.microsoft.com/library/jj899585.aspx).</span></span>
8. <span data-ttu-id="29ec9-153">**Stop (deprovision) the nodes**</span><span class="sxs-lookup"><span data-stu-id="29ec9-153">**Stop (deprovision) the nodes**</span></span>
   
   <span data-ttu-id="29ec9-154">When you are done running jobs, take the nodes offline and use the **Stop** action in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-154">When you are done running jobs, take the nodes offline and use the **Stop** action in HPC Cluster Manager.</span></span>

## <a name="scenario-2-deploy-compute-nodes-in-compute-intensive-vms-iaas"></a><span data-ttu-id="29ec9-155">Scenario 2: Deploy compute nodes in compute-intensive VMs (IaaS)</span><span class="sxs-lookup"><span data-stu-id="29ec9-155">Scenario 2: Deploy compute nodes in compute-intensive VMs (IaaS)</span></span>
<span data-ttu-id="29ec9-156">In this scenario, you deploy the HPC Pack head node and cluster compute nodes on VMs in an Azure virtual network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-156">In this scenario, you deploy the HPC Pack head node and cluster compute nodes on VMs in an Azure virtual network.</span></span> <span data-ttu-id="29ec9-157">HPC Pack provides several [deployment options in Azure VMs](../../linux/hpcpack-cluster-options.md), including automated deployment scripts and Azure quickstart templates.</span><span class="sxs-lookup"><span data-stu-id="29ec9-157">HPC Pack provides several [deployment options in Azure VMs](../../linux/hpcpack-cluster-options.md), including automated deployment scripts and Azure quickstart templates.</span></span> <span data-ttu-id="29ec9-158">As an example, the following considerations and steps guide you to use the [HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md) to automate the deployment of an HPC Pack 2012 R2 cluster in Azure.</span><span class="sxs-lookup"><span data-stu-id="29ec9-158">As an example, the following considerations and steps guide you to use the [HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md) to automate the deployment of an HPC Pack 2012 R2 cluster in Azure.</span></span>

![Cluster in Azure VMs][iaas]

### <a name="steps"></a><span data-ttu-id="29ec9-160">Steps</span><span class="sxs-lookup"><span data-stu-id="29ec9-160">Steps</span></span>
1. <span data-ttu-id="29ec9-161">**Create a cluster head node and compute node VMs by running the HPC Pack IaaS deployment script on a client computer**</span><span class="sxs-lookup"><span data-stu-id="29ec9-161">**Create a cluster head node and compute node VMs by running the HPC Pack IaaS deployment script on a client computer**</span></span>
   
    <span data-ttu-id="29ec9-162">Download the HPC Pack IaaS Deployment Script package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span><span class="sxs-lookup"><span data-stu-id="29ec9-162">Download the HPC Pack IaaS Deployment Script package from the [Microsoft Download Center](https://www.microsoft.com/download/details.aspx?id=49922).</span></span>
   
    <span data-ttu-id="29ec9-163">To prepare the client computer, create the script configuration file, and run the script, see [Create an HPC Cluster with the HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md).</span><span class="sxs-lookup"><span data-stu-id="29ec9-163">To prepare the client computer, create the script configuration file, and run the script, see [Create an HPC Cluster with the HPC Pack IaaS deployment script](hpcpack-cluster-powershell-script.md).</span></span> 
   
    <span data-ttu-id="29ec9-164">To deploy RDMA-capable compute nodes, note the following additional considerations:</span><span class="sxs-lookup"><span data-stu-id="29ec9-164">To deploy RDMA-capable compute nodes, note the following additional considerations:</span></span>
   
   * <span data-ttu-id="29ec9-165">**Virtual network**: Specify a new virtual network in a region in which the RDMA-capable instance size you want to use is available.</span><span class="sxs-lookup"><span data-stu-id="29ec9-165">**Virtual network**: Specify a new virtual network in a region in which the RDMA-capable instance size you want to use is available.</span></span>
   * <span data-ttu-id="29ec9-166">**Windows Server operating system**: To support RDMA connectivity, specify a Windows Server 2012 R2 or Windows Server 2012 operating system for the compute node VMs.</span><span class="sxs-lookup"><span data-stu-id="29ec9-166">**Windows Server operating system**: To support RDMA connectivity, specify a Windows Server 2012 R2 or Windows Server 2012 operating system for the compute node VMs.</span></span>
   * <span data-ttu-id="29ec9-167">**Cloud services**: We recommend deploying your head node in one cloud service and your compute nodes in a different cloud service.</span><span class="sxs-lookup"><span data-stu-id="29ec9-167">**Cloud services**: We recommend deploying your head node in one cloud service and your compute nodes in a different cloud service.</span></span>
   * <span data-ttu-id="29ec9-168">**Head node size**: For this scenario, consider a size of at least A4 (Extra Large) for the head node.</span><span class="sxs-lookup"><span data-stu-id="29ec9-168">**Head node size**: For this scenario, consider a size of at least A4 (Extra Large) for the head node.</span></span>
   * <span data-ttu-id="29ec9-169">**HpcVmDrivers extension**: The deployment script installs the Azure VM Agent and the HpcVmDrivers extension automatically when you deploy size A8 or A9 compute nodes with a Windows Server operating system.</span><span class="sxs-lookup"><span data-stu-id="29ec9-169">**HpcVmDrivers extension**: The deployment script installs the Azure VM Agent and the HpcVmDrivers extension automatically when you deploy size A8 or A9 compute nodes with a Windows Server operating system.</span></span> <span data-ttu-id="29ec9-170">HpcVmDrivers installs drivers on the compute node VMs so they can connect to the RDMA network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-170">HpcVmDrivers installs drivers on the compute node VMs so they can connect to the RDMA network.</span></span> <span data-ttu-id="29ec9-171">On RDMA-capable H-series VMs, you must manually install the HpcVmDrivers extension.</span><span class="sxs-lookup"><span data-stu-id="29ec9-171">On RDMA-capable H-series VMs, you must manually install the HpcVmDrivers extension.</span></span> <span data-ttu-id="29ec9-172">See [About H-series and compute-intensive A-series VMs](../a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json#access-to-the-rdma-network).</span><span class="sxs-lookup"><span data-stu-id="29ec9-172">See [About H-series and compute-intensive A-series VMs](../a8-a9-a10-a11-specs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json#access-to-the-rdma-network).</span></span>
   * <span data-ttu-id="29ec9-173">**Cluster network configuration**: The deployment script automatically sets up the HPC Pack cluster in Topology 5 (all nodes on the Enterprise network).</span><span class="sxs-lookup"><span data-stu-id="29ec9-173">**Cluster network configuration**: The deployment script automatically sets up the HPC Pack cluster in Topology 5 (all nodes on the Enterprise network).</span></span> <span data-ttu-id="29ec9-174">This topology is required for all HPC Pack cluster deployments in VMs.</span><span class="sxs-lookup"><span data-stu-id="29ec9-174">This topology is required for all HPC Pack cluster deployments in VMs.</span></span> <span data-ttu-id="29ec9-175">Do not change the cluster network topology later.</span><span class="sxs-lookup"><span data-stu-id="29ec9-175">Do not change the cluster network topology later.</span></span>
2. <span data-ttu-id="29ec9-176">**Bring the compute nodes online to run jobs**</span><span class="sxs-lookup"><span data-stu-id="29ec9-176">**Bring the compute nodes online to run jobs**</span></span>
   
    <span data-ttu-id="29ec9-177">Select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-177">Select the nodes and use the **Bring Online** action in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-178">The nodes are ready to run jobs.</span><span class="sxs-lookup"><span data-stu-id="29ec9-178">The nodes are ready to run jobs.</span></span>
3. <span data-ttu-id="29ec9-179">**Submit jobs to the cluster**</span><span class="sxs-lookup"><span data-stu-id="29ec9-179">**Submit jobs to the cluster**</span></span>
   
    <span data-ttu-id="29ec9-180">Connect to the head node to submit jobs, or set up an on-premises computer to do this.</span><span class="sxs-lookup"><span data-stu-id="29ec9-180">Connect to the head node to submit jobs, or set up an on-premises computer to do this.</span></span> <span data-ttu-id="29ec9-181">For information, see [Submit Jobs to an HPC cluster in Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span><span class="sxs-lookup"><span data-stu-id="29ec9-181">For information, see [Submit Jobs to an HPC cluster in Azure](../../virtual-machines-windows-hpcpack-cluster-submit-jobs.md?toc=%2fazure%2fvirtual-machines%2fwindows%2ftoc.json).</span></span>
4. <span data-ttu-id="29ec9-182">**Take the nodes offline and stop (deallocate) them**</span><span class="sxs-lookup"><span data-stu-id="29ec9-182">**Take the nodes offline and stop (deallocate) them**</span></span>
   
    <span data-ttu-id="29ec9-183">When you are done running jobs, take the nodes offline in HPC Cluster Manager.</span><span class="sxs-lookup"><span data-stu-id="29ec9-183">When you are done running jobs, take the nodes offline in HPC Cluster Manager.</span></span> <span data-ttu-id="29ec9-184">Then, use Azure management tools to shut them down.</span><span class="sxs-lookup"><span data-stu-id="29ec9-184">Then, use Azure management tools to shut them down.</span></span>

## <a name="run-mpi-applications-on-the-cluster"></a><span data-ttu-id="29ec9-185">Run MPI applications on the cluster</span><span class="sxs-lookup"><span data-stu-id="29ec9-185">Run MPI applications on the cluster</span></span>
### <a name="example-run-mpipingpong-on-an-hpc-pack-cluster"></a><span data-ttu-id="29ec9-186">Example: Run mpipingpong on an HPC Pack cluster</span><span class="sxs-lookup"><span data-stu-id="29ec9-186">Example: Run mpipingpong on an HPC Pack cluster</span></span>
<span data-ttu-id="29ec9-187">To verify an HPC Pack deployment of the RDMA-capable instances, run the HPC Pack **mpipingpong** command on the cluster.</span><span class="sxs-lookup"><span data-stu-id="29ec9-187">To verify an HPC Pack deployment of the RDMA-capable instances, run the HPC Pack **mpipingpong** command on the cluster.</span></span> <span data-ttu-id="29ec9-188">**mpipingpong** sends packets of data between paired nodes repeatedly to calculate latency and throughput measurements and statistics for the RDMA-enabled application network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-188">**mpipingpong** sends packets of data between paired nodes repeatedly to calculate latency and throughput measurements and statistics for the RDMA-enabled application network.</span></span> <span data-ttu-id="29ec9-189">This example shows a typical pattern for running an MPI job (in this case, **mpipingpong**) by using the cluster **mpiexec** command.</span><span class="sxs-lookup"><span data-stu-id="29ec9-189">This example shows a typical pattern for running an MPI job (in this case, **mpipingpong**) by using the cluster **mpiexec** command.</span></span>

<span data-ttu-id="29ec9-190">This example assumes you added Azure nodes in a “burst to Azure” configuration ([Scenario 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span><span class="sxs-lookup"><span data-stu-id="29ec9-190">This example assumes you added Azure nodes in a “burst to Azure” configuration ([Scenario 1](#scenario-1.-deploy-compute-intensive-worker-role-instances-\(PaaS\) in this article).</span></span> <span data-ttu-id="29ec9-191">If you deployed HPC Pack on a cluster of Azure VMs, you’ll need to modify the command syntax to specify a different node group and set additional environment variables to direct network traffic to the RDMA network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-191">If you deployed HPC Pack on a cluster of Azure VMs, you’ll need to modify the command syntax to specify a different node group and set additional environment variables to direct network traffic to the RDMA network.</span></span>

<span data-ttu-id="29ec9-192">To run mpipingpong on the cluster:</span><span class="sxs-lookup"><span data-stu-id="29ec9-192">To run mpipingpong on the cluster:</span></span>

1. <span data-ttu-id="29ec9-193">On the head node or on a properly configured client computer, open a Command Prompt.</span><span class="sxs-lookup"><span data-stu-id="29ec9-193">On the head node or on a properly configured client computer, open a Command Prompt.</span></span>
2. <span data-ttu-id="29ec9-194">To estimate latency between pairs of nodes in an Azure burst deployment of four nodes, type the following command to submit a job to run mpipingpong with a small packet size and many iterations:</span><span class="sxs-lookup"><span data-stu-id="29ec9-194">To estimate latency between pairs of nodes in an Azure burst deployment of four nodes, type the following command to submit a job to run mpipingpong with a small packet size and many iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 1:100000 -op -s nul
    ```
   
    <span data-ttu-id="29ec9-195">The command returns the ID of the job that is submitted.</span><span class="sxs-lookup"><span data-stu-id="29ec9-195">The command returns the ID of the job that is submitted.</span></span>
   
    <span data-ttu-id="29ec9-196">If you deployed the HPC Pack cluster deployed on Azure VMs, specify a node group that contains compute node VMs deployed in a single cloud service, and modify the **mpiexec** command as follows:</span><span class="sxs-lookup"><span data-stu-id="29ec9-196">If you deployed the HPC Pack cluster deployed on Azure VMs, specify a node group that contains compute node VMs deployed in a single cloud service, and modify the **mpiexec** command as follows:</span></span>
   
    ```Command
    job submit /nodegroup:vmcomputenodes /numnodes:4 mpiexec -c 1 -affinity -env MSMPI_DISABLE_SOCK 1 -env MSMPI_PRECONNECT all -env MPICH_NETMASK 172.16.0.0/255.255.0.0 mpipingpong -p 1:100000 -op -s nul
    ```
3. <span data-ttu-id="29ec9-197">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following</span><span class="sxs-lookup"><span data-stu-id="29ec9-197">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
    <span data-ttu-id="29ec9-198">where &lt;*JobID*&gt; is the ID of the job that was submitted.</span><span class="sxs-lookup"><span data-stu-id="29ec9-198">where &lt;*JobID*&gt; is the ID of the job that was submitted.</span></span>
   
    <span data-ttu-id="29ec9-199">The output includes latency results similar to the following.</span><span class="sxs-lookup"><span data-stu-id="29ec9-199">The output includes latency results similar to the following.</span></span>
   
    ![Ping pong latency][pingpong1]
4. <span data-ttu-id="29ec9-201">To estimate throughput between pairs of Azure burst nodes, type the following command to submit a job to run **mpipingpong** with a large packet size and a few iterations:</span><span class="sxs-lookup"><span data-stu-id="29ec9-201">To estimate throughput between pairs of Azure burst nodes, type the following command to submit a job to run **mpipingpong** with a large packet size and a few iterations:</span></span>
   
    ```Command
    job submit /nodegroup:azurenodes /numnodes:4 mpiexec -c 1 -affinity mpipingpong -p 4000000:1000 -op -s nul
    ```
   
    <span data-ttu-id="29ec9-202">The command returns the ID of the job that is submitted.</span><span class="sxs-lookup"><span data-stu-id="29ec9-202">The command returns the ID of the job that is submitted.</span></span>
   
    <span data-ttu-id="29ec9-203">On an HPC Pack cluster deployed on Azure VMs, modify the command as noted in step 2.</span><span class="sxs-lookup"><span data-stu-id="29ec9-203">On an HPC Pack cluster deployed on Azure VMs, modify the command as noted in step 2.</span></span>
5. <span data-ttu-id="29ec9-204">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following:</span><span class="sxs-lookup"><span data-stu-id="29ec9-204">When the job completes, to view the output (in this case, the output of task 1 of the job), type the following:</span></span>
   
    ```Command
    task view <JobID>.1
    ```
   
   <span data-ttu-id="29ec9-205">The output includes throughput results similar to the following.</span><span class="sxs-lookup"><span data-stu-id="29ec9-205">The output includes throughput results similar to the following.</span></span>
   
   ![Ping pong throughput][pingpong2]

### <a name="mpi-application-considerations"></a><span data-ttu-id="29ec9-207">MPI application considerations</span><span class="sxs-lookup"><span data-stu-id="29ec9-207">MPI application considerations</span></span>
<span data-ttu-id="29ec9-208">Following are considerations for running MPI applications with HPC Pack in Azure.</span><span class="sxs-lookup"><span data-stu-id="29ec9-208">Following are considerations for running MPI applications with HPC Pack in Azure.</span></span> <span data-ttu-id="29ec9-209">Some apply only to deployments of Azure nodes (worker role instances added in a “burst to Azure” configuration).</span><span class="sxs-lookup"><span data-stu-id="29ec9-209">Some apply only to deployments of Azure nodes (worker role instances added in a “burst to Azure” configuration).</span></span>

* <span data-ttu-id="29ec9-210">Worker role instances in a cloud service are periodically reprovisioned without notice by Azure (for example, for system maintenance, or in case an instance fails).</span><span class="sxs-lookup"><span data-stu-id="29ec9-210">Worker role instances in a cloud service are periodically reprovisioned without notice by Azure (for example, for system maintenance, or in case an instance fails).</span></span> <span data-ttu-id="29ec9-211">If an instance is reprovisioned while it is running an MPI job, the instance loses its data and returns to the state when it was first deployed, which can cause the MPI job to fail.</span><span class="sxs-lookup"><span data-stu-id="29ec9-211">If an instance is reprovisioned while it is running an MPI job, the instance loses its data and returns to the state when it was first deployed, which can cause the MPI job to fail.</span></span> <span data-ttu-id="29ec9-212">The more nodes that you use for a single MPI job, and the longer the job runs, the more likely that one of the instances is reprovisioned while a job is running.</span><span class="sxs-lookup"><span data-stu-id="29ec9-212">The more nodes that you use for a single MPI job, and the longer the job runs, the more likely that one of the instances is reprovisioned while a job is running.</span></span> <span data-ttu-id="29ec9-213">Also consider this if you designate a single node in the deployment as a file server.</span><span class="sxs-lookup"><span data-stu-id="29ec9-213">Also consider this if you designate a single node in the deployment as a file server.</span></span>
* <span data-ttu-id="29ec9-214">To run MPI jobs in Azure, you don't have to use the RDMA-capable instances.</span><span class="sxs-lookup"><span data-stu-id="29ec9-214">To run MPI jobs in Azure, you don't have to use the RDMA-capable instances.</span></span> <span data-ttu-id="29ec9-215">You can use any instance size that is supported by HPC Pack.</span><span class="sxs-lookup"><span data-stu-id="29ec9-215">You can use any instance size that is supported by HPC Pack.</span></span> <span data-ttu-id="29ec9-216">However, the RDMA-capable instances are recommended for running relatively large-scale MPI jobs that are sensitive to the latency and the bandwidth of the network that connects the nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-216">However, the RDMA-capable instances are recommended for running relatively large-scale MPI jobs that are sensitive to the latency and the bandwidth of the network that connects the nodes.</span></span> <span data-ttu-id="29ec9-217">If you use other sizes to run latency- and bandwidth-sensitive MPI jobs, we recommend running small jobs, in which a single task runs on only a few nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-217">If you use other sizes to run latency- and bandwidth-sensitive MPI jobs, we recommend running small jobs, in which a single task runs on only a few nodes.</span></span>
* <span data-ttu-id="29ec9-218">Applications deployed to Azure instances are subject to the licensing terms associated with the application.</span><span class="sxs-lookup"><span data-stu-id="29ec9-218">Applications deployed to Azure instances are subject to the licensing terms associated with the application.</span></span> <span data-ttu-id="29ec9-219">Check with the vendor of any commercial application for licensing or other restrictions for running in the cloud.</span><span class="sxs-lookup"><span data-stu-id="29ec9-219">Check with the vendor of any commercial application for licensing or other restrictions for running in the cloud.</span></span> <span data-ttu-id="29ec9-220">Not all vendors offer pay-as-you-go licensing.</span><span class="sxs-lookup"><span data-stu-id="29ec9-220">Not all vendors offer pay-as-you-go licensing.</span></span>
* <span data-ttu-id="29ec9-221">Azure instances need further setup to access on-premises nodes, shares, and license servers.</span><span class="sxs-lookup"><span data-stu-id="29ec9-221">Azure instances need further setup to access on-premises nodes, shares, and license servers.</span></span> <span data-ttu-id="29ec9-222">For example, to enable the Azure nodes to access an on-premises license server, you can configure a site-to-site Azure virtual network.</span><span class="sxs-lookup"><span data-stu-id="29ec9-222">For example, to enable the Azure nodes to access an on-premises license server, you can configure a site-to-site Azure virtual network.</span></span>
* <span data-ttu-id="29ec9-223">To run MPI applications on Azure instances, register each MPI application with Windows Firewall on the instances by running the **hpcfwutil** command.</span><span class="sxs-lookup"><span data-stu-id="29ec9-223">To run MPI applications on Azure instances, register each MPI application with Windows Firewall on the instances by running the **hpcfwutil** command.</span></span> <span data-ttu-id="29ec9-224">This allows MPI communications to take place on a port that is assigned dynamically by the firewall.</span><span class="sxs-lookup"><span data-stu-id="29ec9-224">This allows MPI communications to take place on a port that is assigned dynamically by the firewall.</span></span>
  
  > [!NOTE]
  > <span data-ttu-id="29ec9-225">For burst to Azure deployments, you can also configure a firewall exception command to run automatically on all new Azure nodes that are added to your cluster.</span><span class="sxs-lookup"><span data-stu-id="29ec9-225">For burst to Azure deployments, you can also configure a firewall exception command to run automatically on all new Azure nodes that are added to your cluster.</span></span> <span data-ttu-id="29ec9-226">After you run the **hpcfwutil** command and verify that your application works, add the command to a startup script for your Azure nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-226">After you run the **hpcfwutil** command and verify that your application works, add the command to a startup script for your Azure nodes.</span></span> <span data-ttu-id="29ec9-227">For more information, see [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx).</span><span class="sxs-lookup"><span data-stu-id="29ec9-227">For more information, see [Use a Startup Script for Azure Nodes](https://technet.microsoft.com/library/jj899632.aspx).</span></span>
  > 
  > 
* <span data-ttu-id="29ec9-228">HPC Pack uses the CCP_MPI_NETMASK cluster environment variable to specify a range of acceptable addresses for MPI communication.</span><span class="sxs-lookup"><span data-stu-id="29ec9-228">HPC Pack uses the CCP_MPI_NETMASK cluster environment variable to specify a range of acceptable addresses for MPI communication.</span></span> <span data-ttu-id="29ec9-229">Starting in HPC Pack 2012 R2, the CCP_MPI_NETMASK cluster environment variable only affects MPI communication between domain-joined cluster compute nodes (either on-premises or in Azure VMs).</span><span class="sxs-lookup"><span data-stu-id="29ec9-229">Starting in HPC Pack 2012 R2, the CCP_MPI_NETMASK cluster environment variable only affects MPI communication between domain-joined cluster compute nodes (either on-premises or in Azure VMs).</span></span> <span data-ttu-id="29ec9-230">The variable is ignored by nodes added in a burst to Azure configuration.</span><span class="sxs-lookup"><span data-stu-id="29ec9-230">The variable is ignored by nodes added in a burst to Azure configuration.</span></span>
* <span data-ttu-id="29ec9-231">MPI jobs can't run across Azure instances that are deployed in different cloud services (for example, in burst to Azure deployments with different node templates, or Azure VM compute nodes deployed in multiple cloud services).</span><span class="sxs-lookup"><span data-stu-id="29ec9-231">MPI jobs can't run across Azure instances that are deployed in different cloud services (for example, in burst to Azure deployments with different node templates, or Azure VM compute nodes deployed in multiple cloud services).</span></span> <span data-ttu-id="29ec9-232">If you have multiple Azure node deployments that are started with different node templates, the MPI job must run on only one set of Azure nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-232">If you have multiple Azure node deployments that are started with different node templates, the MPI job must run on only one set of Azure nodes.</span></span>
* <span data-ttu-id="29ec9-233">When you add Azure nodes to your cluster and bring them online, the HPC Job Scheduler Service immediately tries to start jobs on the nodes.</span><span class="sxs-lookup"><span data-stu-id="29ec9-233">When you add Azure nodes to your cluster and bring them online, the HPC Job Scheduler Service immediately tries to start jobs on the nodes.</span></span> <span data-ttu-id="29ec9-234">If only a portion of your workload can run on Azure, ensure that you update or create job templates to define what job types can run on Azure.</span><span class="sxs-lookup"><span data-stu-id="29ec9-234">If only a portion of your workload can run on Azure, ensure that you update or create job templates to define what job types can run on Azure.</span></span> <span data-ttu-id="29ec9-235">For example, to ensure that jobs submitted with a job template only run on Azure nodes, add the Node Groups property to the job template and select AzureNodes as the required value.</span><span class="sxs-lookup"><span data-stu-id="29ec9-235">For example, to ensure that jobs submitted with a job template only run on Azure nodes, add the Node Groups property to the job template and select AzureNodes as the required value.</span></span> <span data-ttu-id="29ec9-236">To create custom groups for your Azure nodes, use the Add-HpcGroup HPC PowerShell cmdlet.</span><span class="sxs-lookup"><span data-stu-id="29ec9-236">To create custom groups for your Azure nodes, use the Add-HpcGroup HPC PowerShell cmdlet.</span></span>

## <a name="next-steps"></a><span data-ttu-id="29ec9-237">Next steps</span><span class="sxs-lookup"><span data-stu-id="29ec9-237">Next steps</span></span>
* <span data-ttu-id="29ec9-238">As an alternative to using HPC Pack, develop with the Azure Batch service to run MPI applications on managed pools of compute nodes in Azure.</span><span class="sxs-lookup"><span data-stu-id="29ec9-238">As an alternative to using HPC Pack, develop with the Azure Batch service to run MPI applications on managed pools of compute nodes in Azure.</span></span> <span data-ttu-id="29ec9-239">See [Use multi-instance tasks to run Message Passing Interface (MPI) applications in Azure Batch](../../../batch/batch-mpi.md).</span><span class="sxs-lookup"><span data-stu-id="29ec9-239">See [Use multi-instance tasks to run Message Passing Interface (MPI) applications in Azure Batch](../../../batch/batch-mpi.md).</span></span>
* <span data-ttu-id="29ec9-240">If you want to run Linux MPI applications that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span><span class="sxs-lookup"><span data-stu-id="29ec9-240">If you want to run Linux MPI applications that access the Azure RDMA network, see [Set up a Linux RDMA cluster to run MPI applications](../../linux/classic/rdma-cluster.md).</span></span>

<!--Image references-->
[burst]:https://docstestmedia1.blob.core.windows.net/azure-media/articles/virtual-machines/windows/classic/media/hpcpack-rdma-cluster/burst.png
[iaas]:https://docstestmedia1.blob.core.windows.net/azure-media/articles/virtual-machines/windows/classic/media/hpcpack-rdma-cluster/iaas.png
[pingpong1]:https://docstestmedia1.blob.core.windows.net/azure-media/articles/virtual-machines/windows/classic/media/hpcpack-rdma-cluster/pingpong1.png
[pingpong2]:https://docstestmedia1.blob.core.windows.net/azure-media/articles/virtual-machines/windows/classic/media/hpcpack-rdma-cluster/pingpong2.png




