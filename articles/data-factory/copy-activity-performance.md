---
title: Copy Activity performance and tuning guide in Azure Data Factory | Microsoft Docs
description: Learn about key factors that affect the performance of data movement in Azure Data Factory when you use Copy Activity.
services: data-factory
documentationcenter: ''
author: linda33wj
manager: craigg
ms.reviewer: douglasl
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 07/06/2018
ms.author: jingwang
ms.openlocfilehash: 958d1ea09ce4d85afc59af412e1050efc6290a1a
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44969426"
---
# <a name="copy-activity-performance-and-tuning-guide"></a><span data-ttu-id="f9569-103">Copy Activity performance and tuning guide</span><span class="sxs-lookup"><span data-stu-id="f9569-103">Copy Activity performance and tuning guide</span></span>
> [!div class="op_single_selector" title1="Select the version of Data Factory service you are using:"]
> * [Version 1](v1/data-factory-copy-activity-performance.md)
> * [Current version](copy-activity-performance.md)


<span data-ttu-id="f9569-106">Azure Data Factory Copy Activity delivers a first-class secure, reliable, and high-performance data loading solution.</span><span class="sxs-lookup"><span data-stu-id="f9569-106">Azure Data Factory Copy Activity delivers a first-class secure, reliable, and high-performance data loading solution.</span></span> <span data-ttu-id="f9569-107">It enables you to copy tens of terabytes of data every day across a rich variety of cloud and on-premises data stores.</span><span class="sxs-lookup"><span data-stu-id="f9569-107">It enables you to copy tens of terabytes of data every day across a rich variety of cloud and on-premises data stores.</span></span> <span data-ttu-id="f9569-108">Blazing-fast data loading performance is key to ensure you can focus on the core “big data” problem: building advanced analytics solutions and getting deep insights from all that data.</span><span class="sxs-lookup"><span data-stu-id="f9569-108">Blazing-fast data loading performance is key to ensure you can focus on the core “big data” problem: building advanced analytics solutions and getting deep insights from all that data.</span></span>

<span data-ttu-id="f9569-109">Azure provides a set of enterprise-grade data storage and data warehouse solutions, and Copy Activity offers a highly optimized data loading experience that is easy to configure and set up.</span><span class="sxs-lookup"><span data-stu-id="f9569-109">Azure provides a set of enterprise-grade data storage and data warehouse solutions, and Copy Activity offers a highly optimized data loading experience that is easy to configure and set up.</span></span> <span data-ttu-id="f9569-110">With just a single copy activity, you can achieve:</span><span class="sxs-lookup"><span data-stu-id="f9569-110">With just a single copy activity, you can achieve:</span></span>

* <span data-ttu-id="f9569-111">Loading data into **Azure SQL Data Warehouse** at **1.2 GBps**.</span><span class="sxs-lookup"><span data-stu-id="f9569-111">Loading data into **Azure SQL Data Warehouse** at **1.2 GBps**.</span></span>
* <span data-ttu-id="f9569-112">Loading data into **Azure Blob storage** at **1.0 GBps**</span><span class="sxs-lookup"><span data-stu-id="f9569-112">Loading data into **Azure Blob storage** at **1.0 GBps**</span></span>
* <span data-ttu-id="f9569-113">Loading data into **Azure Data Lake Store** at **1.0 GBps**</span><span class="sxs-lookup"><span data-stu-id="f9569-113">Loading data into **Azure Data Lake Store** at **1.0 GBps**</span></span>

<span data-ttu-id="f9569-114">This article describes:</span><span class="sxs-lookup"><span data-stu-id="f9569-114">This article describes:</span></span>

* <span data-ttu-id="f9569-115">[Performance reference numbers](#performance-reference) for supported source and sink data stores to help you plan your project;</span><span class="sxs-lookup"><span data-stu-id="f9569-115">[Performance reference numbers](#performance-reference) for supported source and sink data stores to help you plan your project;</span></span>
* <span data-ttu-id="f9569-116">Features that can boost the copy throughput in different scenarios, including [data integration units](#data-integration-units), [parallel copy](#parallel-copy), and [staged Copy](#staged-copy);</span><span class="sxs-lookup"><span data-stu-id="f9569-116">Features that can boost the copy throughput in different scenarios, including [data integration units](#data-integration-units), [parallel copy](#parallel-copy), and [staged Copy](#staged-copy);</span></span>
* <span data-ttu-id="f9569-117">[Performance tuning guidance](#performance-tuning-steps) on how to tune the performance and the key factors that can impact copy performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-117">[Performance tuning guidance](#performance-tuning-steps) on how to tune the performance and the key factors that can impact copy performance.</span></span>

> [!NOTE]
> If you are not familiar with Copy Activity in general, see [Copy Activity Overview](copy-activity-overview.md) before reading this article.
>

## <a name="performance-reference"></a><span data-ttu-id="f9569-119">Performance reference</span><span class="sxs-lookup"><span data-stu-id="f9569-119">Performance reference</span></span>

<span data-ttu-id="f9569-120">As a reference, below table shows the copy throughput number **in MBps** for the given source and sink pairs **in a single copy activity run** based on in-house testing.</span><span class="sxs-lookup"><span data-stu-id="f9569-120">As a reference, below table shows the copy throughput number **in MBps** for the given source and sink pairs **in a single copy activity run** based on in-house testing.</span></span> <span data-ttu-id="f9569-121">For comparison, it also demonstrates how different settings of [Data Integration Units](#data-integration-units) or [Self-hosted Integration Runtime scalability](concepts-integration-runtime.md#self-hosted-integration-runtime) (multiple nodes) can help on copy performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-121">For comparison, it also demonstrates how different settings of [Data Integration Units](#data-integration-units) or [Self-hosted Integration Runtime scalability](concepts-integration-runtime.md#self-hosted-integration-runtime) (multiple nodes) can help on copy performance.</span></span>

![Performance matrix](./media/copy-activity-performance/CopyPerfRef.png)

> [!IMPORTANT]
> When copy activity is executed on an Azure Integration Runtime, the minimal allowed Data Integration Units (formerly known as Data Movement Units) is two. If not specified, see default Data Integration Units being used in [Data Integration Units](#data-integration-units).

<span data-ttu-id="f9569-125">Points to note:</span><span class="sxs-lookup"><span data-stu-id="f9569-125">Points to note:</span></span>

* <span data-ttu-id="f9569-126">Throughput is calculated by using the following formula: [size of data read from source]/[Copy Activity run duration].</span><span class="sxs-lookup"><span data-stu-id="f9569-126">Throughput is calculated by using the following formula: [size of data read from source]/[Copy Activity run duration].</span></span>
* <span data-ttu-id="f9569-127">The performance reference numbers in the table were measured using [TPC-H](http://www.tpc.org/tpch/) dataset in a single copy activity run.</span><span class="sxs-lookup"><span data-stu-id="f9569-127">The performance reference numbers in the table were measured using [TPC-H](http://www.tpc.org/tpch/) dataset in a single copy activity run.</span></span>
* <span data-ttu-id="f9569-128">In Azure data stores, the source and sink are in the same Azure region.</span><span class="sxs-lookup"><span data-stu-id="f9569-128">In Azure data stores, the source and sink are in the same Azure region.</span></span>
* <span data-ttu-id="f9569-129">For hybrid copy between on-premises and cloud data stores, each Self-hosted Integration Runtime node was running on a machine that was separate from the data store with below specification.</span><span class="sxs-lookup"><span data-stu-id="f9569-129">For hybrid copy between on-premises and cloud data stores, each Self-hosted Integration Runtime node was running on a machine that was separate from the data store with below specification.</span></span> <span data-ttu-id="f9569-130">When a single activity was running, the copy operation consumed only a small portion of the test machine's CPU, memory, or network bandwidth.</span><span class="sxs-lookup"><span data-stu-id="f9569-130">When a single activity was running, the copy operation consumed only a small portion of the test machine's CPU, memory, or network bandwidth.</span></span>
    <table>
    <tr>
        <td><span data-ttu-id="f9569-131">CPU</span><span class="sxs-lookup"><span data-stu-id="f9569-131">CPU</span></span></td>
        <td><span data-ttu-id="f9569-132">32 cores 2.20 GHz Intel Xeon E5-2660 v2</span><span class="sxs-lookup"><span data-stu-id="f9569-132">32 cores 2.20 GHz Intel Xeon E5-2660 v2</span></span></td>
    </tr>
    <tr>
        <td><span data-ttu-id="f9569-133">Memory</span><span class="sxs-lookup"><span data-stu-id="f9569-133">Memory</span></span></td>
        <td><span data-ttu-id="f9569-134">128 GB</span><span class="sxs-lookup"><span data-stu-id="f9569-134">128 GB</span></span></td>
    </tr>
    <tr>
        <td><span data-ttu-id="f9569-135">Network</span><span class="sxs-lookup"><span data-stu-id="f9569-135">Network</span></span></td>
        <td><span data-ttu-id="f9569-136">Internet interface: 10 Gbps; intranet interface: 40 Gbps</span><span class="sxs-lookup"><span data-stu-id="f9569-136">Internet interface: 10 Gbps; intranet interface: 40 Gbps</span></span></td>
    </tr>
    </table>


> [!TIP]
> You can achieve higher throughput by using more Data Integration Units (DIU) than the default allowed maximum DIUs, which are 32 for a cloud-to-cloud copy activity run. For example, with 100 DIUs, you can achieve copying data from Azure Blob into Azure Data Lake Store at **1.0GBps**. See the [Data Integration Units](#data-integration-units) section for details about this feature and the supported scenario. Contact [Azure support](https://azure.microsoft.com/support/) to request more DIUs.

## <a name="data-integration-units"></a><span data-ttu-id="f9569-141">Data integration units</span><span class="sxs-lookup"><span data-stu-id="f9569-141">Data integration units</span></span>

<span data-ttu-id="f9569-142">A **Data Integration Unit (DIU)** (formerly known as Cloud Data Movement Unit or DMU) is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Data Factory.</span><span class="sxs-lookup"><span data-stu-id="f9569-142">A **Data Integration Unit (DIU)** (formerly known as Cloud Data Movement Unit or DMU) is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Data Factory.</span></span> <span data-ttu-id="f9569-143">**DIU only applies to [Azure Integration Runtime](concepts-integration-runtime.md#azure-integration-runtime)**, but not [Self-hosted Integration Runtime](concepts-integration-runtime.md#self-hosted-integration-runtime).</span><span class="sxs-lookup"><span data-stu-id="f9569-143">**DIU only applies to [Azure Integration Runtime](concepts-integration-runtime.md#azure-integration-runtime)**, but not [Self-hosted Integration Runtime](concepts-integration-runtime.md#self-hosted-integration-runtime).</span></span>

<span data-ttu-id="f9569-144">**The minimal Data Integration Units to empower Copy Activity run is two.**</span><span class="sxs-lookup"><span data-stu-id="f9569-144">**The minimal Data Integration Units to empower Copy Activity run is two.**</span></span> <span data-ttu-id="f9569-145">If not specified, the following table lists the default DIUs used in different copy scenarios:</span><span class="sxs-lookup"><span data-stu-id="f9569-145">If not specified, the following table lists the default DIUs used in different copy scenarios:</span></span>

| <span data-ttu-id="f9569-146">Copy scenario</span><span class="sxs-lookup"><span data-stu-id="f9569-146">Copy scenario</span></span> | <span data-ttu-id="f9569-147">Default DIUs determined by service</span><span class="sxs-lookup"><span data-stu-id="f9569-147">Default DIUs determined by service</span></span> |
|:--- |:--- |
| <span data-ttu-id="f9569-148">Copy data between file-based stores</span><span class="sxs-lookup"><span data-stu-id="f9569-148">Copy data between file-based stores</span></span> | <span data-ttu-id="f9569-149">Between 4 and 32 depending on the number and size of the files.</span><span class="sxs-lookup"><span data-stu-id="f9569-149">Between 4 and 32 depending on the number and size of the files.</span></span> |
| <span data-ttu-id="f9569-150">All other copy scenarios</span><span class="sxs-lookup"><span data-stu-id="f9569-150">All other copy scenarios</span></span> | <span data-ttu-id="f9569-151">4</span><span class="sxs-lookup"><span data-stu-id="f9569-151">4</span></span> |

<span data-ttu-id="f9569-152">To override this default, specify a value for the **dataIntegrationUnits** property as follows.</span><span class="sxs-lookup"><span data-stu-id="f9569-152">To override this default, specify a value for the **dataIntegrationUnits** property as follows.</span></span> <span data-ttu-id="f9569-153">The **allowed values** for the **dataIntegrationUnits** property is **up to 256**.</span><span class="sxs-lookup"><span data-stu-id="f9569-153">The **allowed values** for the **dataIntegrationUnits** property is **up to 256**.</span></span> <span data-ttu-id="f9569-154">The **actual number of DIUs** that the copy operation uses at run time is equal to or less than the configured value, depending on your data pattern.</span><span class="sxs-lookup"><span data-stu-id="f9569-154">The **actual number of DIUs** that the copy operation uses at run time is equal to or less than the configured value, depending on your data pattern.</span></span> <span data-ttu-id="f9569-155">For information about the level of performance gain you might get when you configure more units for a specific copy source and sink, see the [performance reference](#performance-reference).</span><span class="sxs-lookup"><span data-stu-id="f9569-155">For information about the level of performance gain you might get when you configure more units for a specific copy source and sink, see the [performance reference](#performance-reference).</span></span>

<span data-ttu-id="f9569-156">You can see the actually used Data Integration Units for each copy run in the copy activity output when monitoring an activity run.</span><span class="sxs-lookup"><span data-stu-id="f9569-156">You can see the actually used Data Integration Units for each copy run in the copy activity output when monitoring an activity run.</span></span> <span data-ttu-id="f9569-157">Learn details from [Copy activity monitoring](copy-activity-overview.md#monitoring).</span><span class="sxs-lookup"><span data-stu-id="f9569-157">Learn details from [Copy activity monitoring](copy-activity-overview.md#monitoring).</span></span>

> [!NOTE]
> If you need more DIUs for a higher throughput, contact [Azure support](https://azure.microsoft.com/support/). Setting of 8 and above currently works only when you **copy multiple files from Blob storage/Data Lake Store/Amazon S3/cloud FTP/cloud SFTP to any other cloud data stores.**.
>

<span data-ttu-id="f9569-160">**Example:**</span><span class="sxs-lookup"><span data-stu-id="f9569-160">**Example:**</span></span>

```json
"activities":[
    {
        "name": "Sample copy activity",
        "type": "Copy",
        "inputs": [...],
        "outputs": [...],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
            },
            "sink": {
                "type": "AzureDataLakeStoreSink"
            },
            "dataIntegrationUnits": 32
        }
    }
]
```

### <a name="data-integration-units-billing-impact"></a><span data-ttu-id="f9569-161">Data Integration Units billing impact</span><span class="sxs-lookup"><span data-stu-id="f9569-161">Data Integration Units billing impact</span></span>

<span data-ttu-id="f9569-162">It's **important** to remember that you are charged based on the total time of the copy operation.</span><span class="sxs-lookup"><span data-stu-id="f9569-162">It's **important** to remember that you are charged based on the total time of the copy operation.</span></span> <span data-ttu-id="f9569-163">The total duration you are billed for data movement is the sum of duration across DIUs.</span><span class="sxs-lookup"><span data-stu-id="f9569-163">The total duration you are billed for data movement is the sum of duration across DIUs.</span></span> <span data-ttu-id="f9569-164">If a copy job used to take one hour with two cloud units and now it takes 15 minutes with eight cloud units, the overall bill remains almost the same.</span><span class="sxs-lookup"><span data-stu-id="f9569-164">If a copy job used to take one hour with two cloud units and now it takes 15 minutes with eight cloud units, the overall bill remains almost the same.</span></span>

## <a name="parallel-copy"></a><span data-ttu-id="f9569-165">Parallel Copy</span><span class="sxs-lookup"><span data-stu-id="f9569-165">Parallel Copy</span></span>

<span data-ttu-id="f9569-166">You can use the **parallelCopies** property to indicate the parallelism that you want Copy Activity to use.</span><span class="sxs-lookup"><span data-stu-id="f9569-166">You can use the **parallelCopies** property to indicate the parallelism that you want Copy Activity to use.</span></span> <span data-ttu-id="f9569-167">You can think of this property as the maximum number of threads within Copy Activity that can read from your source or write to your sink data stores in parallel.</span><span class="sxs-lookup"><span data-stu-id="f9569-167">You can think of this property as the maximum number of threads within Copy Activity that can read from your source or write to your sink data stores in parallel.</span></span>

<span data-ttu-id="f9569-168">For each Copy Activity run, Data Factory determines the number of parallel copies to use to copy data from the source data store and to the destination data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-168">For each Copy Activity run, Data Factory determines the number of parallel copies to use to copy data from the source data store and to the destination data store.</span></span> <span data-ttu-id="f9569-169">The default number of parallel copies that it uses depends on the type of source and sink that you are using:</span><span class="sxs-lookup"><span data-stu-id="f9569-169">The default number of parallel copies that it uses depends on the type of source and sink that you are using:</span></span>

| <span data-ttu-id="f9569-170">Copy scenario</span><span class="sxs-lookup"><span data-stu-id="f9569-170">Copy scenario</span></span> | <span data-ttu-id="f9569-171">Default parallel copy count determined by service</span><span class="sxs-lookup"><span data-stu-id="f9569-171">Default parallel copy count determined by service</span></span> |
| --- | --- |
| <span data-ttu-id="f9569-172">Copy data between file-based stores</span><span class="sxs-lookup"><span data-stu-id="f9569-172">Copy data between file-based stores</span></span> |<span data-ttu-id="f9569-173">Depends on the size of the files and the number of Data Integration Units (DIUs) used to copy data between two cloud data stores, or the physical configuration of the Self-hosted Integration Runtime machine.</span><span class="sxs-lookup"><span data-stu-id="f9569-173">Depends on the size of the files and the number of Data Integration Units (DIUs) used to copy data between two cloud data stores, or the physical configuration of the Self-hosted Integration Runtime machine.</span></span> |
| <span data-ttu-id="f9569-174">Copy data from any source data store to Azure Table storage</span><span class="sxs-lookup"><span data-stu-id="f9569-174">Copy data from any source data store to Azure Table storage</span></span> |<span data-ttu-id="f9569-175">4</span><span class="sxs-lookup"><span data-stu-id="f9569-175">4</span></span> |
| <span data-ttu-id="f9569-176">All other copy scenarios</span><span class="sxs-lookup"><span data-stu-id="f9569-176">All other copy scenarios</span></span> |<span data-ttu-id="f9569-177">1</span><span class="sxs-lookup"><span data-stu-id="f9569-177">1</span></span> |

[!TIP]
> When copying data between file-based stores, the default behavior (auto determined) usually give you the best throughput. 

<span data-ttu-id="f9569-179">To control the load on machines that host your data stores, or to tune copy performance, you may choose to override the default value and specify a value for the **parallelCopies** property.</span><span class="sxs-lookup"><span data-stu-id="f9569-179">To control the load on machines that host your data stores, or to tune copy performance, you may choose to override the default value and specify a value for the **parallelCopies** property.</span></span> <span data-ttu-id="f9569-180">The value must be an integer greater than or equal to 1.</span><span class="sxs-lookup"><span data-stu-id="f9569-180">The value must be an integer greater than or equal to 1.</span></span> <span data-ttu-id="f9569-181">At run time, for the best performance, Copy Activity uses a value that is less than or equal to the value that you set.</span><span class="sxs-lookup"><span data-stu-id="f9569-181">At run time, for the best performance, Copy Activity uses a value that is less than or equal to the value that you set.</span></span>

```json
"activities":[
    {
        "name": "Sample copy activity",
        "type": "Copy",
        "inputs": [...],
        "outputs": [...],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
            },
            "sink": {
                "type": "AzureDataLakeStoreSink"
            },
            "parallelCopies": 32
        }
    }
]
```

<span data-ttu-id="f9569-182">Points to note:</span><span class="sxs-lookup"><span data-stu-id="f9569-182">Points to note:</span></span>

* <span data-ttu-id="f9569-183">When you copy data between file-based stores, the **parallelCopies** determine the parallelism at the file level.</span><span class="sxs-lookup"><span data-stu-id="f9569-183">When you copy data between file-based stores, the **parallelCopies** determine the parallelism at the file level.</span></span> <span data-ttu-id="f9569-184">The chunking within a single file would happen underneath automatically and transparently, and it's designed to use the best suitable chunk size for a given source data store type to load data in parallel and orthogonal to parallelCopies.</span><span class="sxs-lookup"><span data-stu-id="f9569-184">The chunking within a single file would happen underneath automatically and transparently, and it's designed to use the best suitable chunk size for a given source data store type to load data in parallel and orthogonal to parallelCopies.</span></span> <span data-ttu-id="f9569-185">The actual number of parallel copies the data movement service uses for the copy operation at run time is no more than the number of files you have.</span><span class="sxs-lookup"><span data-stu-id="f9569-185">The actual number of parallel copies the data movement service uses for the copy operation at run time is no more than the number of files you have.</span></span> <span data-ttu-id="f9569-186">If the copy behavior is **mergeFile**, Copy Activity cannot take advantage of file-level parallelism.</span><span class="sxs-lookup"><span data-stu-id="f9569-186">If the copy behavior is **mergeFile**, Copy Activity cannot take advantage of file-level parallelism.</span></span>
* <span data-ttu-id="f9569-187">When you specify a value for the **parallelCopies** property, consider the load increase on your source and sink data stores, and to Self-Hosted Integration Runtime if the copy activity is empowered by it for example, for hybrid copy.</span><span class="sxs-lookup"><span data-stu-id="f9569-187">When you specify a value for the **parallelCopies** property, consider the load increase on your source and sink data stores, and to Self-Hosted Integration Runtime if the copy activity is empowered by it for example, for hybrid copy.</span></span> <span data-ttu-id="f9569-188">This happens especially when you have multiple activities or concurrent runs of the same activities that run against the same data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-188">This happens especially when you have multiple activities or concurrent runs of the same activities that run against the same data store.</span></span> <span data-ttu-id="f9569-189">If you notice that either the data store or Self-hosted Integration Runtime is overwhelmed with the load, decrease the **parallelCopies** value to relieve the load.</span><span class="sxs-lookup"><span data-stu-id="f9569-189">If you notice that either the data store or Self-hosted Integration Runtime is overwhelmed with the load, decrease the **parallelCopies** value to relieve the load.</span></span>
* <span data-ttu-id="f9569-190">When you copy data from stores that are not file-based to stores that are file-based, the data movement service ignores the **parallelCopies** property.</span><span class="sxs-lookup"><span data-stu-id="f9569-190">When you copy data from stores that are not file-based to stores that are file-based, the data movement service ignores the **parallelCopies** property.</span></span> <span data-ttu-id="f9569-191">Even if parallelism is specified, it's not applied in this case.</span><span class="sxs-lookup"><span data-stu-id="f9569-191">Even if parallelism is specified, it's not applied in this case.</span></span>
* <span data-ttu-id="f9569-192">**parallelCopies** is orthogonal to **dataIntegrationUnits**.</span><span class="sxs-lookup"><span data-stu-id="f9569-192">**parallelCopies** is orthogonal to **dataIntegrationUnits**.</span></span> <span data-ttu-id="f9569-193">The former is counted across all the Data Integration Units.</span><span class="sxs-lookup"><span data-stu-id="f9569-193">The former is counted across all the Data Integration Units.</span></span>

## <a name="staged-copy"></a><span data-ttu-id="f9569-194">Staged copy</span><span class="sxs-lookup"><span data-stu-id="f9569-194">Staged copy</span></span>

<span data-ttu-id="f9569-195">When you copy data from a source data store to a sink data store, you might choose to use Blob storage as an interim staging store.</span><span class="sxs-lookup"><span data-stu-id="f9569-195">When you copy data from a source data store to a sink data store, you might choose to use Blob storage as an interim staging store.</span></span> <span data-ttu-id="f9569-196">Staging is especially useful in the following cases:</span><span class="sxs-lookup"><span data-stu-id="f9569-196">Staging is especially useful in the following cases:</span></span>

- <span data-ttu-id="f9569-197">**You want to ingest data from various data stores into SQL Data Warehouse via PolyBase**.</span><span class="sxs-lookup"><span data-stu-id="f9569-197">**You want to ingest data from various data stores into SQL Data Warehouse via PolyBase**.</span></span> <span data-ttu-id="f9569-198">SQL Data Warehouse uses PolyBase as a high-throughput mechanism to load a large amount of data into SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="f9569-198">SQL Data Warehouse uses PolyBase as a high-throughput mechanism to load a large amount of data into SQL Data Warehouse.</span></span> <span data-ttu-id="f9569-199">However, the source data must be in Blob storage or Azure Data Lake Store, and it must meet additional criteria.</span><span class="sxs-lookup"><span data-stu-id="f9569-199">However, the source data must be in Blob storage or Azure Data Lake Store, and it must meet additional criteria.</span></span> <span data-ttu-id="f9569-200">When you load data from a data store other than Blob storage or Azure Data Lake Store, you can activate data copying via interim staging Blob storage.</span><span class="sxs-lookup"><span data-stu-id="f9569-200">When you load data from a data store other than Blob storage or Azure Data Lake Store, you can activate data copying via interim staging Blob storage.</span></span> <span data-ttu-id="f9569-201">In that case, Data Factory performs the required data transformations to ensure that it meets the requirements of PolyBase.</span><span class="sxs-lookup"><span data-stu-id="f9569-201">In that case, Data Factory performs the required data transformations to ensure that it meets the requirements of PolyBase.</span></span> <span data-ttu-id="f9569-202">Then it uses PolyBase to load data into SQL Data Warehouse efficiently.</span><span class="sxs-lookup"><span data-stu-id="f9569-202">Then it uses PolyBase to load data into SQL Data Warehouse efficiently.</span></span> <span data-ttu-id="f9569-203">For more information, see [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse).</span><span class="sxs-lookup"><span data-stu-id="f9569-203">For more information, see [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse).</span></span>
- <span data-ttu-id="f9569-204">**Sometimes it takes a while to perform a hybrid data movement (that is, to copy from an on-premises data store to a cloud data store) over a slow network connection**.</span><span class="sxs-lookup"><span data-stu-id="f9569-204">**Sometimes it takes a while to perform a hybrid data movement (that is, to copy from an on-premises data store to a cloud data store) over a slow network connection**.</span></span> <span data-ttu-id="f9569-205">To improve performance, you can use staged copy to compress the data on-premises so that it takes less time to move data to the staging data store in the cloud then decompress the data in the staging store before loading into the destination data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-205">To improve performance, you can use staged copy to compress the data on-premises so that it takes less time to move data to the staging data store in the cloud then decompress the data in the staging store before loading into the destination data store.</span></span>
- <span data-ttu-id="f9569-206">**You don't want to open ports other than port 80 and port 443 in your firewall, because of corporate IT policies**.</span><span class="sxs-lookup"><span data-stu-id="f9569-206">**You don't want to open ports other than port 80 and port 443 in your firewall, because of corporate IT policies**.</span></span> <span data-ttu-id="f9569-207">For example, when you copy data from an on-premises data store to an Azure SQL Database sink or an Azure SQL Data Warehouse sink, you need to activate outbound TCP communication on port 1433 for both the Windows firewall and your corporate firewall.</span><span class="sxs-lookup"><span data-stu-id="f9569-207">For example, when you copy data from an on-premises data store to an Azure SQL Database sink or an Azure SQL Data Warehouse sink, you need to activate outbound TCP communication on port 1433 for both the Windows firewall and your corporate firewall.</span></span> <span data-ttu-id="f9569-208">In this scenario, staged copy can take advantage of the Self-hosted Integration Runtime to first copy data to a Blob storage staging instance over HTTP or HTTPS on port 443, then load the data into SQL Database or SQL Data Warehouse from Blob storage staging.</span><span class="sxs-lookup"><span data-stu-id="f9569-208">In this scenario, staged copy can take advantage of the Self-hosted Integration Runtime to first copy data to a Blob storage staging instance over HTTP or HTTPS on port 443, then load the data into SQL Database or SQL Data Warehouse from Blob storage staging.</span></span> <span data-ttu-id="f9569-209">In this flow, you don't need to enable port 1433.</span><span class="sxs-lookup"><span data-stu-id="f9569-209">In this flow, you don't need to enable port 1433.</span></span>

### <a name="how-staged-copy-works"></a><span data-ttu-id="f9569-210">How staged copy works</span><span class="sxs-lookup"><span data-stu-id="f9569-210">How staged copy works</span></span>

<span data-ttu-id="f9569-211">When you activate the staging feature, first the data is copied from the source data store to the staging Blob storage (bring your own).</span><span class="sxs-lookup"><span data-stu-id="f9569-211">When you activate the staging feature, first the data is copied from the source data store to the staging Blob storage (bring your own).</span></span> <span data-ttu-id="f9569-212">Next, the data is copied from the staging data store to the sink data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-212">Next, the data is copied from the staging data store to the sink data store.</span></span> <span data-ttu-id="f9569-213">Data Factory automatically manages the two-stage flow for you.</span><span class="sxs-lookup"><span data-stu-id="f9569-213">Data Factory automatically manages the two-stage flow for you.</span></span> <span data-ttu-id="f9569-214">Data Factory also cleans up temporary data from the staging storage after the data movement is complete.</span><span class="sxs-lookup"><span data-stu-id="f9569-214">Data Factory also cleans up temporary data from the staging storage after the data movement is complete.</span></span>

![Staged copy](media/copy-activity-performance/staged-copy.png)

<span data-ttu-id="f9569-216">When you activate data movement by using a staging store, you can specify whether you want the data to be compressed before moving data from the source data store to an interim or staging data store, and then decompressed before moving data from an interim or staging data store to the sink data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-216">When you activate data movement by using a staging store, you can specify whether you want the data to be compressed before moving data from the source data store to an interim or staging data store, and then decompressed before moving data from an interim or staging data store to the sink data store.</span></span>

<span data-ttu-id="f9569-217">Currently, you can't copy data between two on-premises data stores by using a staging store.</span><span class="sxs-lookup"><span data-stu-id="f9569-217">Currently, you can't copy data between two on-premises data stores by using a staging store.</span></span>

### <a name="configuration"></a><span data-ttu-id="f9569-218">Configuration</span><span class="sxs-lookup"><span data-stu-id="f9569-218">Configuration</span></span>

<span data-ttu-id="f9569-219">Configure the **enableStaging** setting in Copy Activity to specify whether you want the data to be staged in Blob storage before you load it into a destination data store.</span><span class="sxs-lookup"><span data-stu-id="f9569-219">Configure the **enableStaging** setting in Copy Activity to specify whether you want the data to be staged in Blob storage before you load it into a destination data store.</span></span> <span data-ttu-id="f9569-220">When you set **enableStaging** to `TRUE`, specify the additional properties listed in the next table.</span><span class="sxs-lookup"><span data-stu-id="f9569-220">When you set **enableStaging** to `TRUE`, specify the additional properties listed in the next table.</span></span> <span data-ttu-id="f9569-221">If you don’t have one, you also need to create an Azure Storage or Storage shared access signature-linked service for staging.</span><span class="sxs-lookup"><span data-stu-id="f9569-221">If you don’t have one, you also need to create an Azure Storage or Storage shared access signature-linked service for staging.</span></span>

| <span data-ttu-id="f9569-222">Property</span><span class="sxs-lookup"><span data-stu-id="f9569-222">Property</span></span> | <span data-ttu-id="f9569-223">Description</span><span class="sxs-lookup"><span data-stu-id="f9569-223">Description</span></span> | <span data-ttu-id="f9569-224">Default value</span><span class="sxs-lookup"><span data-stu-id="f9569-224">Default value</span></span> | <span data-ttu-id="f9569-225">Required</span><span class="sxs-lookup"><span data-stu-id="f9569-225">Required</span></span> |
| --- | --- | --- | --- |
| <span data-ttu-id="f9569-226">**enableStaging**</span><span class="sxs-lookup"><span data-stu-id="f9569-226">**enableStaging**</span></span> |<span data-ttu-id="f9569-227">Specify whether you want to copy data via an interim staging store.</span><span class="sxs-lookup"><span data-stu-id="f9569-227">Specify whether you want to copy data via an interim staging store.</span></span> |<span data-ttu-id="f9569-228">False</span><span class="sxs-lookup"><span data-stu-id="f9569-228">False</span></span> |<span data-ttu-id="f9569-229">No</span><span class="sxs-lookup"><span data-stu-id="f9569-229">No</span></span> |
| <span data-ttu-id="f9569-230">**linkedServiceName**</span><span class="sxs-lookup"><span data-stu-id="f9569-230">**linkedServiceName**</span></span> |<span data-ttu-id="f9569-231">Specify the name of an [AzureStorage](connector-azure-blob-storage.md#linked-service-properties) linked service, which refers to the instance of Storage that you use as an interim staging store.</span><span class="sxs-lookup"><span data-stu-id="f9569-231">Specify the name of an [AzureStorage](connector-azure-blob-storage.md#linked-service-properties) linked service, which refers to the instance of Storage that you use as an interim staging store.</span></span> <br/><br/> <span data-ttu-id="f9569-232">You cannot use Storage with a shared access signature to load data into SQL Data Warehouse via PolyBase.</span><span class="sxs-lookup"><span data-stu-id="f9569-232">You cannot use Storage with a shared access signature to load data into SQL Data Warehouse via PolyBase.</span></span> <span data-ttu-id="f9569-233">You can use it in all other scenarios.</span><span class="sxs-lookup"><span data-stu-id="f9569-233">You can use it in all other scenarios.</span></span> |<span data-ttu-id="f9569-234">N/A</span><span class="sxs-lookup"><span data-stu-id="f9569-234">N/A</span></span> |<span data-ttu-id="f9569-235">Yes, when **enableStaging** is set to TRUE</span><span class="sxs-lookup"><span data-stu-id="f9569-235">Yes, when **enableStaging** is set to TRUE</span></span> |
| <span data-ttu-id="f9569-236">**path**</span><span class="sxs-lookup"><span data-stu-id="f9569-236">**path**</span></span> |<span data-ttu-id="f9569-237">Specify the Blob storage path that you want to contain the staged data.</span><span class="sxs-lookup"><span data-stu-id="f9569-237">Specify the Blob storage path that you want to contain the staged data.</span></span> <span data-ttu-id="f9569-238">If you do not provide a path, the service creates a container to store temporary data.</span><span class="sxs-lookup"><span data-stu-id="f9569-238">If you do not provide a path, the service creates a container to store temporary data.</span></span> <br/><br/> <span data-ttu-id="f9569-239">Specify a path only if you use Storage with a shared access signature, or you require temporary data to be in a specific location.</span><span class="sxs-lookup"><span data-stu-id="f9569-239">Specify a path only if you use Storage with a shared access signature, or you require temporary data to be in a specific location.</span></span> |<span data-ttu-id="f9569-240">N/A</span><span class="sxs-lookup"><span data-stu-id="f9569-240">N/A</span></span> |<span data-ttu-id="f9569-241">No</span><span class="sxs-lookup"><span data-stu-id="f9569-241">No</span></span> |
| <span data-ttu-id="f9569-242">**enableCompression**</span><span class="sxs-lookup"><span data-stu-id="f9569-242">**enableCompression**</span></span> |<span data-ttu-id="f9569-243">Specifies whether data should be compressed before it is copied to the destination.</span><span class="sxs-lookup"><span data-stu-id="f9569-243">Specifies whether data should be compressed before it is copied to the destination.</span></span> <span data-ttu-id="f9569-244">This setting reduces the volume of data being transferred.</span><span class="sxs-lookup"><span data-stu-id="f9569-244">This setting reduces the volume of data being transferred.</span></span> |<span data-ttu-id="f9569-245">False</span><span class="sxs-lookup"><span data-stu-id="f9569-245">False</span></span> |<span data-ttu-id="f9569-246">No</span><span class="sxs-lookup"><span data-stu-id="f9569-246">No</span></span> |

<span data-ttu-id="f9569-247">Here's a sample definition of Copy Activity with the properties that are described in the preceding table:</span><span class="sxs-lookup"><span data-stu-id="f9569-247">Here's a sample definition of Copy Activity with the properties that are described in the preceding table:</span></span>

```json
"activities":[
    {
        "name": "Sample copy activity",
        "type": "Copy",
        "inputs": [...],
        "outputs": [...],
        "typeProperties": {
            "source": {
                "type": "SqlSource",
            },
            "sink": {
                "type": "SqlSink"
            },
            "enableStaging": true,
            "stagingSettings": {
                "linkedServiceName": {
                    "referenceName": "MyStagingBlob",
                    "type": "LinkedServiceReference"
                },
                "path": "stagingcontainer/path",
                "enableCompression": true
            }
        }
    }
]
```

### <a name="staged-copy-billing-impact"></a><span data-ttu-id="f9569-248">Staged copy billing impact</span><span class="sxs-lookup"><span data-stu-id="f9569-248">Staged copy billing impact</span></span>

<span data-ttu-id="f9569-249">You are charged based on two steps: copy duration and copy type.</span><span class="sxs-lookup"><span data-stu-id="f9569-249">You are charged based on two steps: copy duration and copy type.</span></span>

* <span data-ttu-id="f9569-250">When you use staging during a cloud copy (copying data from a cloud data store to another cloud data store, both stages empowered by Azure Integration Runtime), you are charged the [sum of copy duration for step 1 and step 2] x [cloud copy unit price].</span><span class="sxs-lookup"><span data-stu-id="f9569-250">When you use staging during a cloud copy (copying data from a cloud data store to another cloud data store, both stages empowered by Azure Integration Runtime), you are charged the [sum of copy duration for step 1 and step 2] x [cloud copy unit price].</span></span>
* <span data-ttu-id="f9569-251">When you use staging during a hybrid copy (copying data from an on-premises data store to a cloud data store, one stage empowered by Self-hosted Integration Runtime), you are charged for [hybrid copy duration] x [hybrid copy unit price] + [cloud copy duration] x [cloud copy unit price].</span><span class="sxs-lookup"><span data-stu-id="f9569-251">When you use staging during a hybrid copy (copying data from an on-premises data store to a cloud data store, one stage empowered by Self-hosted Integration Runtime), you are charged for [hybrid copy duration] x [hybrid copy unit price] + [cloud copy duration] x [cloud copy unit price].</span></span>

## <a name="performance-tuning-steps"></a><span data-ttu-id="f9569-252">Performance tuning steps</span><span class="sxs-lookup"><span data-stu-id="f9569-252">Performance tuning steps</span></span>

<span data-ttu-id="f9569-253">We suggest that you take these steps to tune the performance of your Data Factory service with Copy Activity:</span><span class="sxs-lookup"><span data-stu-id="f9569-253">We suggest that you take these steps to tune the performance of your Data Factory service with Copy Activity:</span></span>

1. <span data-ttu-id="f9569-254">**Establish a baseline**.</span><span class="sxs-lookup"><span data-stu-id="f9569-254">**Establish a baseline**.</span></span> <span data-ttu-id="f9569-255">During the development phase, test your pipeline by using Copy Activity against a representative data sample.</span><span class="sxs-lookup"><span data-stu-id="f9569-255">During the development phase, test your pipeline by using Copy Activity against a representative data sample.</span></span> <span data-ttu-id="f9569-256">Collect execution details and performance characteristics following [Copy activity monitoring](copy-activity-overview.md#monitoring).</span><span class="sxs-lookup"><span data-stu-id="f9569-256">Collect execution details and performance characteristics following [Copy activity monitoring](copy-activity-overview.md#monitoring).</span></span>

2. <span data-ttu-id="f9569-257">**Diagnose and optimize performance**.</span><span class="sxs-lookup"><span data-stu-id="f9569-257">**Diagnose and optimize performance**.</span></span> <span data-ttu-id="f9569-258">If the performance you observe doesn't meet your expectations, you need to identify performance bottlenecks.</span><span class="sxs-lookup"><span data-stu-id="f9569-258">If the performance you observe doesn't meet your expectations, you need to identify performance bottlenecks.</span></span> <span data-ttu-id="f9569-259">Then, optimize performance to remove or reduce the effect of bottlenecks.</span><span class="sxs-lookup"><span data-stu-id="f9569-259">Then, optimize performance to remove or reduce the effect of bottlenecks.</span></span> <span data-ttu-id="f9569-260">A full description of performance diagnosis is beyond the scope of this article, but here are some common considerations:</span><span class="sxs-lookup"><span data-stu-id="f9569-260">A full description of performance diagnosis is beyond the scope of this article, but here are some common considerations:</span></span>

   * <span data-ttu-id="f9569-261">Performance features:</span><span class="sxs-lookup"><span data-stu-id="f9569-261">Performance features:</span></span>
     * [<span data-ttu-id="f9569-262">Parallel copy</span><span class="sxs-lookup"><span data-stu-id="f9569-262">Parallel copy</span></span>](#parallel-copy)
     * [<span data-ttu-id="f9569-263">Data integration units</span><span class="sxs-lookup"><span data-stu-id="f9569-263">Data integration units</span></span>](#data-integration-units)
     * [<span data-ttu-id="f9569-264">Staged copy</span><span class="sxs-lookup"><span data-stu-id="f9569-264">Staged copy</span></span>](#staged-copy)
     * [<span data-ttu-id="f9569-265">Self-hosted Integration Runtime scalability</span><span class="sxs-lookup"><span data-stu-id="f9569-265">Self-hosted Integration Runtime scalability</span></span>](concepts-integration-runtime.md#self-hosted-integration-runtime)
   * [<span data-ttu-id="f9569-266">Self-hosted Integration Runtime</span><span class="sxs-lookup"><span data-stu-id="f9569-266">Self-hosted Integration Runtime</span></span>](#considerations-for-self-hosted-integration-runtime)
   * [<span data-ttu-id="f9569-267">Source</span><span class="sxs-lookup"><span data-stu-id="f9569-267">Source</span></span>](#considerations-for-the-source)
   * [<span data-ttu-id="f9569-268">Sink</span><span class="sxs-lookup"><span data-stu-id="f9569-268">Sink</span></span>](#considerations-for-the-sink)
   * [<span data-ttu-id="f9569-269">Serialization and deserialization</span><span class="sxs-lookup"><span data-stu-id="f9569-269">Serialization and deserialization</span></span>](#considerations-for-serialization-and-deserialization)
   * [<span data-ttu-id="f9569-270">Compression</span><span class="sxs-lookup"><span data-stu-id="f9569-270">Compression</span></span>](#considerations-for-compression)
   * [<span data-ttu-id="f9569-271">Column mapping</span><span class="sxs-lookup"><span data-stu-id="f9569-271">Column mapping</span></span>](#considerations-for-column-mapping)
   * [<span data-ttu-id="f9569-272">Other considerations</span><span class="sxs-lookup"><span data-stu-id="f9569-272">Other considerations</span></span>](#other-considerations)

3. <span data-ttu-id="f9569-273">**Expand the configuration to your entire data set**.</span><span class="sxs-lookup"><span data-stu-id="f9569-273">**Expand the configuration to your entire data set**.</span></span> <span data-ttu-id="f9569-274">When you're satisfied with the execution results and performance, you can expand the definition and pipeline to cover your entire data set.</span><span class="sxs-lookup"><span data-stu-id="f9569-274">When you're satisfied with the execution results and performance, you can expand the definition and pipeline to cover your entire data set.</span></span>

## <a name="considerations-for-self-hosted-integration-runtime"></a><span data-ttu-id="f9569-275">Considerations for Self-hosted Integration Runtime</span><span class="sxs-lookup"><span data-stu-id="f9569-275">Considerations for Self-hosted Integration Runtime</span></span>

<span data-ttu-id="f9569-276">If your copy activity is executed on a Self-hosted Integration Runtime, note the following:</span><span class="sxs-lookup"><span data-stu-id="f9569-276">If your copy activity is executed on a Self-hosted Integration Runtime, note the following:</span></span>

<span data-ttu-id="f9569-277">**Setup**: We recommend that you use a dedicated machine to host Integration Runtime.</span><span class="sxs-lookup"><span data-stu-id="f9569-277">**Setup**: We recommend that you use a dedicated machine to host Integration Runtime.</span></span> <span data-ttu-id="f9569-278">See [Considerations for using Self-hosted Integration Runtime](concepts-integration-runtime.md).</span><span class="sxs-lookup"><span data-stu-id="f9569-278">See [Considerations for using Self-hosted Integration Runtime](concepts-integration-runtime.md).</span></span>

<span data-ttu-id="f9569-279">**Scale out**: A single logical Self-hosted Integration Runtime with one or more nodes can serve multiple Copy Activity runs at the same time concurrently.</span><span class="sxs-lookup"><span data-stu-id="f9569-279">**Scale out**: A single logical Self-hosted Integration Runtime with one or more nodes can serve multiple Copy Activity runs at the same time concurrently.</span></span> <span data-ttu-id="f9569-280">If you have heavy need on hybrid data movement either with large number of concurrent copy activity runs or with large volume of data to copy, consider to [scale out Self-hosted Integration Runtime](create-self-hosted-integration-runtime.md#high-availability-and-scalability) so as to provision more resource to empower copy.</span><span class="sxs-lookup"><span data-stu-id="f9569-280">If you have heavy need on hybrid data movement either with large number of concurrent copy activity runs or with large volume of data to copy, consider to [scale out Self-hosted Integration Runtime](create-self-hosted-integration-runtime.md#high-availability-and-scalability) so as to provision more resource to empower copy.</span></span>

## <a name="considerations-for-the-source"></a><span data-ttu-id="f9569-281">Considerations for the source</span><span class="sxs-lookup"><span data-stu-id="f9569-281">Considerations for the source</span></span>

### <a name="general"></a><span data-ttu-id="f9569-282">General</span><span class="sxs-lookup"><span data-stu-id="f9569-282">General</span></span>

<span data-ttu-id="f9569-283">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span><span class="sxs-lookup"><span data-stu-id="f9569-283">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span></span>

<span data-ttu-id="f9569-284">For Microsoft data stores, see [monitoring and tuning topics](#performance-reference) that are specific to data stores, and help you understand data store performance characteristics, minimize response times, and maximize throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-284">For Microsoft data stores, see [monitoring and tuning topics](#performance-reference) that are specific to data stores, and help you understand data store performance characteristics, minimize response times, and maximize throughput.</span></span>

* <span data-ttu-id="f9569-285">If you copy data **from Blob storage to SQL Data Warehouse**, consider using **PolyBase** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-285">If you copy data **from Blob storage to SQL Data Warehouse**, consider using **PolyBase** to boost performance.</span></span> <span data-ttu-id="f9569-286">See [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-286">See [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span></span>
* <span data-ttu-id="f9569-287">If you copy data **from HDFS to Azure Blob/Azure Data Lake Store**, consider using **DistCp** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-287">If you copy data **from HDFS to Azure Blob/Azure Data Lake Store**, consider using **DistCp** to boost performance.</span></span> <span data-ttu-id="f9569-288">See [Use DistCp to copy data from HDFS](connector-hdfs.md#use-distcp-to-copy-data-from-hdfs) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-288">See [Use DistCp to copy data from HDFS](connector-hdfs.md#use-distcp-to-copy-data-from-hdfs) for details.</span></span>
* <span data-ttu-id="f9569-289">If you copy data **from Redshift to Azure SQL Data Warehouse/Azure BLob/Azure Data Lake Store**, consider using **UNLOAD** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-289">If you copy data **from Redshift to Azure SQL Data Warehouse/Azure BLob/Azure Data Lake Store**, consider using **UNLOAD** to boost performance.</span></span> <span data-ttu-id="f9569-290">See [Use UNLOAD to copy data from Amazon Redshift](connector-amazon-redshift.md#use-unload-to-copy-data-from-amazon-redshift) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-290">See [Use UNLOAD to copy data from Amazon Redshift](connector-amazon-redshift.md#use-unload-to-copy-data-from-amazon-redshift) for details.</span></span>

### <a name="file-based-data-stores"></a><span data-ttu-id="f9569-291">File-based data stores</span><span class="sxs-lookup"><span data-stu-id="f9569-291">File-based data stores</span></span>

* <span data-ttu-id="f9569-292">**Average file size and file count**: Copy Activity transfers data one file at a time.</span><span class="sxs-lookup"><span data-stu-id="f9569-292">**Average file size and file count**: Copy Activity transfers data one file at a time.</span></span> <span data-ttu-id="f9569-293">With the same amount of data to be moved, the overall throughput is lower if the data consists of many small files rather than a few large files due to the bootstrap phase for each file.</span><span class="sxs-lookup"><span data-stu-id="f9569-293">With the same amount of data to be moved, the overall throughput is lower if the data consists of many small files rather than a few large files due to the bootstrap phase for each file.</span></span> <span data-ttu-id="f9569-294">Therefore, if possible, combine small files into larger files to gain higher throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-294">Therefore, if possible, combine small files into larger files to gain higher throughput.</span></span>
* <span data-ttu-id="f9569-295">**File format and compression**: For more ways to improve performance, see the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections.</span><span class="sxs-lookup"><span data-stu-id="f9569-295">**File format and compression**: For more ways to improve performance, see the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections.</span></span>

### <a name="relational-data-stores"></a><span data-ttu-id="f9569-296">Relational data stores</span><span class="sxs-lookup"><span data-stu-id="f9569-296">Relational data stores</span></span>

* <span data-ttu-id="f9569-297">**Data pattern**: Your table schema affects copy throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-297">**Data pattern**: Your table schema affects copy throughput.</span></span> <span data-ttu-id="f9569-298">A large row size gives you a better performance than small row size, to copy the same amount of data.</span><span class="sxs-lookup"><span data-stu-id="f9569-298">A large row size gives you a better performance than small row size, to copy the same amount of data.</span></span> <span data-ttu-id="f9569-299">The reason is that the database can more efficiently retrieve fewer batches of data that contain fewer rows.</span><span class="sxs-lookup"><span data-stu-id="f9569-299">The reason is that the database can more efficiently retrieve fewer batches of data that contain fewer rows.</span></span>
* <span data-ttu-id="f9569-300">**Query or stored procedure**: Optimize the logic of the query or stored procedure you specify in the Copy Activity source to fetch data more efficiently.</span><span class="sxs-lookup"><span data-stu-id="f9569-300">**Query or stored procedure**: Optimize the logic of the query or stored procedure you specify in the Copy Activity source to fetch data more efficiently.</span></span>

## <a name="considerations-for-the-sink"></a><span data-ttu-id="f9569-301">Considerations for the sink</span><span class="sxs-lookup"><span data-stu-id="f9569-301">Considerations for the sink</span></span>

### <a name="general"></a><span data-ttu-id="f9569-302">General</span><span class="sxs-lookup"><span data-stu-id="f9569-302">General</span></span>

<span data-ttu-id="f9569-303">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span><span class="sxs-lookup"><span data-stu-id="f9569-303">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span></span>

<span data-ttu-id="f9569-304">For Microsoft data stores, refer to [monitoring and tuning topics](#performance-reference) that are specific to data stores.</span><span class="sxs-lookup"><span data-stu-id="f9569-304">For Microsoft data stores, refer to [monitoring and tuning topics](#performance-reference) that are specific to data stores.</span></span> <span data-ttu-id="f9569-305">These topics can help you understand data store performance characteristics and how to minimize response times and maximize throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-305">These topics can help you understand data store performance characteristics and how to minimize response times and maximize throughput.</span></span>

* <span data-ttu-id="f9569-306">If you copy data **from Blob storage to SQL Data Warehouse**, consider using **PolyBase** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-306">If you copy data **from Blob storage to SQL Data Warehouse**, consider using **PolyBase** to boost performance.</span></span> <span data-ttu-id="f9569-307">See [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-307">See [Use PolyBase to load data into Azure SQL Data Warehouse](connector-azure-sql-data-warehouse.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span></span>
* <span data-ttu-id="f9569-308">If you copy data **from HDFS to Azure Blob/Azure Data Lake Store**, consider using **DistCp** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-308">If you copy data **from HDFS to Azure Blob/Azure Data Lake Store**, consider using **DistCp** to boost performance.</span></span> <span data-ttu-id="f9569-309">See [Use DistCp to copy data from HDFS](connector-hdfs.md#use-distcp-to-copy-data-from-hdfs) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-309">See [Use DistCp to copy data from HDFS](connector-hdfs.md#use-distcp-to-copy-data-from-hdfs) for details.</span></span>
* <span data-ttu-id="f9569-310">If you copy data **from Redshift to Azure SQL Data Warehouse/Azure BLob/Azure Data Lake Store**, consider using **UNLOAD** to boost performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-310">If you copy data **from Redshift to Azure SQL Data Warehouse/Azure BLob/Azure Data Lake Store**, consider using **UNLOAD** to boost performance.</span></span> <span data-ttu-id="f9569-311">See [Use UNLOAD to copy data from Amazon Redshift](connector-amazon-redshift.md#use-unload-to-copy-data-from-amazon-redshift) for details.</span><span class="sxs-lookup"><span data-stu-id="f9569-311">See [Use UNLOAD to copy data from Amazon Redshift](connector-amazon-redshift.md#use-unload-to-copy-data-from-amazon-redshift) for details.</span></span>

### <a name="file-based-data-stores"></a><span data-ttu-id="f9569-312">File-based data stores</span><span class="sxs-lookup"><span data-stu-id="f9569-312">File-based data stores</span></span>

* <span data-ttu-id="f9569-313">**Copy behavior**: If you copy data from a different file-based data store, Copy Activity has three options via the **copyBehavior** property.</span><span class="sxs-lookup"><span data-stu-id="f9569-313">**Copy behavior**: If you copy data from a different file-based data store, Copy Activity has three options via the **copyBehavior** property.</span></span> <span data-ttu-id="f9569-314">It preserves hierarchy, flattens hierarchy, or merges files.</span><span class="sxs-lookup"><span data-stu-id="f9569-314">It preserves hierarchy, flattens hierarchy, or merges files.</span></span> <span data-ttu-id="f9569-315">Either preserving or flattening hierarchy has little or no performance overhead, but merging files causes performance overhead to increase.</span><span class="sxs-lookup"><span data-stu-id="f9569-315">Either preserving or flattening hierarchy has little or no performance overhead, but merging files causes performance overhead to increase.</span></span>
* <span data-ttu-id="f9569-316">**File format and compression**: See the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections for more ways to improve performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-316">**File format and compression**: See the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections for more ways to improve performance.</span></span>

### <a name="relational-data-stores"></a><span data-ttu-id="f9569-317">Relational data stores</span><span class="sxs-lookup"><span data-stu-id="f9569-317">Relational data stores</span></span>

* <span data-ttu-id="f9569-318">**Copy behavior**: Depending on the properties you've set for **sqlSink**, Copy Activity writes data to the destination database in different ways.</span><span class="sxs-lookup"><span data-stu-id="f9569-318">**Copy behavior**: Depending on the properties you've set for **sqlSink**, Copy Activity writes data to the destination database in different ways.</span></span>
  * <span data-ttu-id="f9569-319">By default, the data movement service uses the Bulk Copy API to insert data in append mode, which provides the best performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-319">By default, the data movement service uses the Bulk Copy API to insert data in append mode, which provides the best performance.</span></span>
  * <span data-ttu-id="f9569-320">If you configure a stored procedure in the sink, the database applies the data one row at a time instead of as a bulk load.</span><span class="sxs-lookup"><span data-stu-id="f9569-320">If you configure a stored procedure in the sink, the database applies the data one row at a time instead of as a bulk load.</span></span> <span data-ttu-id="f9569-321">Performance drops significantly.</span><span class="sxs-lookup"><span data-stu-id="f9569-321">Performance drops significantly.</span></span> <span data-ttu-id="f9569-322">If your data set is large, when applicable, consider switching to using the **preCopyScript** property.</span><span class="sxs-lookup"><span data-stu-id="f9569-322">If your data set is large, when applicable, consider switching to using the **preCopyScript** property.</span></span>
  * <span data-ttu-id="f9569-323">If you configure the **preCopyScript** property for each Copy Activity run, the service triggers the script, and then you use the Bulk Copy API to insert the data.</span><span class="sxs-lookup"><span data-stu-id="f9569-323">If you configure the **preCopyScript** property for each Copy Activity run, the service triggers the script, and then you use the Bulk Copy API to insert the data.</span></span> <span data-ttu-id="f9569-324">For example, to overwrite the entire table with the latest data, you can specify a script to first delete all records before bulk-loading the new data from the source.</span><span class="sxs-lookup"><span data-stu-id="f9569-324">For example, to overwrite the entire table with the latest data, you can specify a script to first delete all records before bulk-loading the new data from the source.</span></span>
* <span data-ttu-id="f9569-325">**Data pattern and batch size**:</span><span class="sxs-lookup"><span data-stu-id="f9569-325">**Data pattern and batch size**:</span></span>
  * <span data-ttu-id="f9569-326">Your table schema affects copy throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-326">Your table schema affects copy throughput.</span></span> <span data-ttu-id="f9569-327">To copy the same amount of data, a large row size gives you better performance than a small row size because the database can more efficiently commit fewer batches of data.</span><span class="sxs-lookup"><span data-stu-id="f9569-327">To copy the same amount of data, a large row size gives you better performance than a small row size because the database can more efficiently commit fewer batches of data.</span></span>
  * <span data-ttu-id="f9569-328">Copy Activity inserts data in a series of batches.</span><span class="sxs-lookup"><span data-stu-id="f9569-328">Copy Activity inserts data in a series of batches.</span></span> <span data-ttu-id="f9569-329">You can set the number of rows in a batch by using the **writeBatchSize** property.</span><span class="sxs-lookup"><span data-stu-id="f9569-329">You can set the number of rows in a batch by using the **writeBatchSize** property.</span></span> <span data-ttu-id="f9569-330">If your data has small rows, you can set the **writeBatchSize** property with a higher value to benefit from lower batch overhead and higher throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-330">If your data has small rows, you can set the **writeBatchSize** property with a higher value to benefit from lower batch overhead and higher throughput.</span></span> <span data-ttu-id="f9569-331">If the row size of your data is large, be careful when you increase **writeBatchSize**.</span><span class="sxs-lookup"><span data-stu-id="f9569-331">If the row size of your data is large, be careful when you increase **writeBatchSize**.</span></span> <span data-ttu-id="f9569-332">A high value might lead to a copy failure caused by overloading the database.</span><span class="sxs-lookup"><span data-stu-id="f9569-332">A high value might lead to a copy failure caused by overloading the database.</span></span>

### <a name="nosql-stores"></a><span data-ttu-id="f9569-333">NoSQL stores</span><span class="sxs-lookup"><span data-stu-id="f9569-333">NoSQL stores</span></span>

* <span data-ttu-id="f9569-334">For **Table storage**:</span><span class="sxs-lookup"><span data-stu-id="f9569-334">For **Table storage**:</span></span>
  * <span data-ttu-id="f9569-335">**Partition**: Writing data to interleaved partitions dramatically degrades performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-335">**Partition**: Writing data to interleaved partitions dramatically degrades performance.</span></span> <span data-ttu-id="f9569-336">Sort your source data by partition key so that the data is inserted efficiently into one partition after another, or adjust the logic to write the data to a single partition.</span><span class="sxs-lookup"><span data-stu-id="f9569-336">Sort your source data by partition key so that the data is inserted efficiently into one partition after another, or adjust the logic to write the data to a single partition.</span></span>

## <a name="considerations-for-serialization-and-deserialization"></a><span data-ttu-id="f9569-337">Considerations for serialization and deserialization</span><span class="sxs-lookup"><span data-stu-id="f9569-337">Considerations for serialization and deserialization</span></span>

<span data-ttu-id="f9569-338">Serialization and deserialization can occur when your input data set or output data set is a file.</span><span class="sxs-lookup"><span data-stu-id="f9569-338">Serialization and deserialization can occur when your input data set or output data set is a file.</span></span> <span data-ttu-id="f9569-339">See [Supported file and compression formats](supported-file-formats-and-compression-codecs.md) with details on supported file formats by Copy Activity.</span><span class="sxs-lookup"><span data-stu-id="f9569-339">See [Supported file and compression formats](supported-file-formats-and-compression-codecs.md) with details on supported file formats by Copy Activity.</span></span>

<span data-ttu-id="f9569-340">**Copy behavior**:</span><span class="sxs-lookup"><span data-stu-id="f9569-340">**Copy behavior**:</span></span>

* <span data-ttu-id="f9569-341">Copying files between file-based data stores:</span><span class="sxs-lookup"><span data-stu-id="f9569-341">Copying files between file-based data stores:</span></span>
  * <span data-ttu-id="f9569-342">When input and output data sets both have the same or no file format settings, the data movement service executes a **binary copy** without any serialization or deserialization.</span><span class="sxs-lookup"><span data-stu-id="f9569-342">When input and output data sets both have the same or no file format settings, the data movement service executes a **binary copy** without any serialization or deserialization.</span></span> <span data-ttu-id="f9569-343">You see a higher throughput compared to the scenario, in which the source and sink file format settings are different from each other.</span><span class="sxs-lookup"><span data-stu-id="f9569-343">You see a higher throughput compared to the scenario, in which the source and sink file format settings are different from each other.</span></span>
  * <span data-ttu-id="f9569-344">When input and output data sets both are in text format and only the encoding type is different, the data movement service only does encoding conversion.</span><span class="sxs-lookup"><span data-stu-id="f9569-344">When input and output data sets both are in text format and only the encoding type is different, the data movement service only does encoding conversion.</span></span> <span data-ttu-id="f9569-345">It doesn't do any serialization and deserialization, which causes some performance overhead compared to a binary copy.</span><span class="sxs-lookup"><span data-stu-id="f9569-345">It doesn't do any serialization and deserialization, which causes some performance overhead compared to a binary copy.</span></span>
  * <span data-ttu-id="f9569-346">When input and output data sets both have different file formats or different configurations, like delimiters, the data movement service deserializes source data to stream, transform, and then serialize it into the output format you indicated.</span><span class="sxs-lookup"><span data-stu-id="f9569-346">When input and output data sets both have different file formats or different configurations, like delimiters, the data movement service deserializes source data to stream, transform, and then serialize it into the output format you indicated.</span></span> <span data-ttu-id="f9569-347">This operation results in a much more significant performance overhead compared to other scenarios.</span><span class="sxs-lookup"><span data-stu-id="f9569-347">This operation results in a much more significant performance overhead compared to other scenarios.</span></span>
* <span data-ttu-id="f9569-348">When you copy files to/from a data store that is not file-based (for example, from a file-based store to a relational store), the serialization or deserialization step is required.</span><span class="sxs-lookup"><span data-stu-id="f9569-348">When you copy files to/from a data store that is not file-based (for example, from a file-based store to a relational store), the serialization or deserialization step is required.</span></span> <span data-ttu-id="f9569-349">This step results in significant performance overhead.</span><span class="sxs-lookup"><span data-stu-id="f9569-349">This step results in significant performance overhead.</span></span>

<span data-ttu-id="f9569-350">**File format**: The file format you choose might affect copy performance.</span><span class="sxs-lookup"><span data-stu-id="f9569-350">**File format**: The file format you choose might affect copy performance.</span></span> <span data-ttu-id="f9569-351">For example, Avro is a compact binary format that stores metadata with data.</span><span class="sxs-lookup"><span data-stu-id="f9569-351">For example, Avro is a compact binary format that stores metadata with data.</span></span> <span data-ttu-id="f9569-352">It has broad support in the Hadoop ecosystem for processing and querying.</span><span class="sxs-lookup"><span data-stu-id="f9569-352">It has broad support in the Hadoop ecosystem for processing and querying.</span></span> <span data-ttu-id="f9569-353">However, Avro is more expensive for serialization and deserialization, which results in lower copy throughput compared to text format.</span><span class="sxs-lookup"><span data-stu-id="f9569-353">However, Avro is more expensive for serialization and deserialization, which results in lower copy throughput compared to text format.</span></span> <span data-ttu-id="f9569-354">Make your choice of file format throughout the processing flow holistically.</span><span class="sxs-lookup"><span data-stu-id="f9569-354">Make your choice of file format throughout the processing flow holistically.</span></span> <span data-ttu-id="f9569-355">Start with what form the data is stored in, source data stores or to be extracted from external systems; the best format for storage, analytical processing, and querying; and in what format the data should be exported into data marts for reporting and visualization tools.</span><span class="sxs-lookup"><span data-stu-id="f9569-355">Start with what form the data is stored in, source data stores or to be extracted from external systems; the best format for storage, analytical processing, and querying; and in what format the data should be exported into data marts for reporting and visualization tools.</span></span> <span data-ttu-id="f9569-356">Sometimes a file format that is suboptimal for read and write performance might be a good choice when you consider the overall analytical process.</span><span class="sxs-lookup"><span data-stu-id="f9569-356">Sometimes a file format that is suboptimal for read and write performance might be a good choice when you consider the overall analytical process.</span></span>

## <a name="considerations-for-compression"></a><span data-ttu-id="f9569-357">Considerations for compression</span><span class="sxs-lookup"><span data-stu-id="f9569-357">Considerations for compression</span></span>

<span data-ttu-id="f9569-358">When your input or output data set is a file, you can set Copy Activity to perform compression or decompression as it writes data to the destination.</span><span class="sxs-lookup"><span data-stu-id="f9569-358">When your input or output data set is a file, you can set Copy Activity to perform compression or decompression as it writes data to the destination.</span></span> <span data-ttu-id="f9569-359">When you choose compression, you make a tradeoff between input/output (I/O) and CPU.</span><span class="sxs-lookup"><span data-stu-id="f9569-359">When you choose compression, you make a tradeoff between input/output (I/O) and CPU.</span></span> <span data-ttu-id="f9569-360">Compressing the data costs extra in compute resources.</span><span class="sxs-lookup"><span data-stu-id="f9569-360">Compressing the data costs extra in compute resources.</span></span> <span data-ttu-id="f9569-361">But in return, it reduces network I/O and storage.</span><span class="sxs-lookup"><span data-stu-id="f9569-361">But in return, it reduces network I/O and storage.</span></span> <span data-ttu-id="f9569-362">Depending on your data, you may see a boost in overall copy throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-362">Depending on your data, you may see a boost in overall copy throughput.</span></span>

<span data-ttu-id="f9569-363">**Codec**: Each compression codec has advantages.</span><span class="sxs-lookup"><span data-stu-id="f9569-363">**Codec**: Each compression codec has advantages.</span></span> <span data-ttu-id="f9569-364">For example, bzip2 has the lowest copy throughput, but you get the best Hive query performance with bzip2 because you can split it for processing.</span><span class="sxs-lookup"><span data-stu-id="f9569-364">For example, bzip2 has the lowest copy throughput, but you get the best Hive query performance with bzip2 because you can split it for processing.</span></span> <span data-ttu-id="f9569-365">Gzip is the most balanced option, and it is used the most often.</span><span class="sxs-lookup"><span data-stu-id="f9569-365">Gzip is the most balanced option, and it is used the most often.</span></span> <span data-ttu-id="f9569-366">Choose the codec that best suits your end-to-end scenario.</span><span class="sxs-lookup"><span data-stu-id="f9569-366">Choose the codec that best suits your end-to-end scenario.</span></span>

<span data-ttu-id="f9569-367">**Level**: You can choose from two options for each compression codec: fastest compressed and optimally compressed.</span><span class="sxs-lookup"><span data-stu-id="f9569-367">**Level**: You can choose from two options for each compression codec: fastest compressed and optimally compressed.</span></span> <span data-ttu-id="f9569-368">The fastest compressed option compresses the data as quickly as possible, even if the resulting file is not optimally compressed.</span><span class="sxs-lookup"><span data-stu-id="f9569-368">The fastest compressed option compresses the data as quickly as possible, even if the resulting file is not optimally compressed.</span></span> <span data-ttu-id="f9569-369">The optimally compressed option spends more time on compression and yields a minimal amount of data.</span><span class="sxs-lookup"><span data-stu-id="f9569-369">The optimally compressed option spends more time on compression and yields a minimal amount of data.</span></span> <span data-ttu-id="f9569-370">You can test both options to see which provides better overall performance in your case.</span><span class="sxs-lookup"><span data-stu-id="f9569-370">You can test both options to see which provides better overall performance in your case.</span></span>

<span data-ttu-id="f9569-371">**A consideration**: To copy a large amount of data between an on-premises store and the cloud, consider using [Staged copy](#staged-copy) with compression enabled.</span><span class="sxs-lookup"><span data-stu-id="f9569-371">**A consideration**: To copy a large amount of data between an on-premises store and the cloud, consider using [Staged copy](#staged-copy) with compression enabled.</span></span> <span data-ttu-id="f9569-372">Using interim storage is helpful when the bandwidth of your corporate network and your Azure services is the limiting factor, and you want the input data set and output data set both to be in uncompressed form.</span><span class="sxs-lookup"><span data-stu-id="f9569-372">Using interim storage is helpful when the bandwidth of your corporate network and your Azure services is the limiting factor, and you want the input data set and output data set both to be in uncompressed form.</span></span>

## <a name="considerations-for-column-mapping"></a><span data-ttu-id="f9569-373">Considerations for column mapping</span><span class="sxs-lookup"><span data-stu-id="f9569-373">Considerations for column mapping</span></span>

<span data-ttu-id="f9569-374">You can set the **columnMappings** property in Copy Activity to map all or a subset of the input columns to the output columns.</span><span class="sxs-lookup"><span data-stu-id="f9569-374">You can set the **columnMappings** property in Copy Activity to map all or a subset of the input columns to the output columns.</span></span> <span data-ttu-id="f9569-375">After the data movement service reads the data from the source, it needs to perform column mapping on the data before it writes the data to the sink.</span><span class="sxs-lookup"><span data-stu-id="f9569-375">After the data movement service reads the data from the source, it needs to perform column mapping on the data before it writes the data to the sink.</span></span> <span data-ttu-id="f9569-376">This extra processing reduces copy throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-376">This extra processing reduces copy throughput.</span></span>

<span data-ttu-id="f9569-377">If your source data store is queryable, for example, if it's a relational store like SQL Database or SQL Server, or if it's a NoSQL store like Table storage or Azure Cosmos DB, consider pushing the column filtering and reordering logic to the **query** property instead of using column mapping.</span><span class="sxs-lookup"><span data-stu-id="f9569-377">If your source data store is queryable, for example, if it's a relational store like SQL Database or SQL Server, or if it's a NoSQL store like Table storage or Azure Cosmos DB, consider pushing the column filtering and reordering logic to the **query** property instead of using column mapping.</span></span> <span data-ttu-id="f9569-378">This way, the projection occurs while the data movement service reads data from the source data store, where it is much more efficient.</span><span class="sxs-lookup"><span data-stu-id="f9569-378">This way, the projection occurs while the data movement service reads data from the source data store, where it is much more efficient.</span></span>

<span data-ttu-id="f9569-379">Learn more from [Copy Activity schema mapping](copy-activity-schema-and-type-mapping.md).</span><span class="sxs-lookup"><span data-stu-id="f9569-379">Learn more from [Copy Activity schema mapping](copy-activity-schema-and-type-mapping.md).</span></span>

## <a name="other-considerations"></a><span data-ttu-id="f9569-380">Other considerations</span><span class="sxs-lookup"><span data-stu-id="f9569-380">Other considerations</span></span>

<span data-ttu-id="f9569-381">If the size of data you want to copy is large, you can adjust your business logic to further partition the data and schedule Copy Activity to run more frequently to reduce the data size for each Copy Activity run.</span><span class="sxs-lookup"><span data-stu-id="f9569-381">If the size of data you want to copy is large, you can adjust your business logic to further partition the data and schedule Copy Activity to run more frequently to reduce the data size for each Copy Activity run.</span></span>

<span data-ttu-id="f9569-382">Be cautious about the number of data sets and copy activities requiring Data Factory to connect to the same data store at the same time.</span><span class="sxs-lookup"><span data-stu-id="f9569-382">Be cautious about the number of data sets and copy activities requiring Data Factory to connect to the same data store at the same time.</span></span> <span data-ttu-id="f9569-383">Many concurrent copy jobs might throttle a data store and lead to degraded performance, copy job internal retries, and in some cases, execution failures.</span><span class="sxs-lookup"><span data-stu-id="f9569-383">Many concurrent copy jobs might throttle a data store and lead to degraded performance, copy job internal retries, and in some cases, execution failures.</span></span>

## <a name="sample-scenario-copy-from-an-on-premises-sql-server-to-blob-storage"></a><span data-ttu-id="f9569-384">Sample scenario: Copy from an on-premises SQL Server to Blob storage</span><span class="sxs-lookup"><span data-stu-id="f9569-384">Sample scenario: Copy from an on-premises SQL Server to Blob storage</span></span>

<span data-ttu-id="f9569-385">**Scenario**: A pipeline is built to copy data from an on-premises SQL Server to Blob storage in CSV format.</span><span class="sxs-lookup"><span data-stu-id="f9569-385">**Scenario**: A pipeline is built to copy data from an on-premises SQL Server to Blob storage in CSV format.</span></span> <span data-ttu-id="f9569-386">To make the copy job faster, the CSV files should be compressed into bzip2 format.</span><span class="sxs-lookup"><span data-stu-id="f9569-386">To make the copy job faster, the CSV files should be compressed into bzip2 format.</span></span>

<span data-ttu-id="f9569-387">**Test and analysis**: The throughput of Copy Activity is less than 2 MBps, which is much slower than the performance benchmark.</span><span class="sxs-lookup"><span data-stu-id="f9569-387">**Test and analysis**: The throughput of Copy Activity is less than 2 MBps, which is much slower than the performance benchmark.</span></span>

<span data-ttu-id="f9569-388">**Performance analysis and tuning**: To troubleshoot the performance issue, let’s look at how the data is processed and moved.</span><span class="sxs-lookup"><span data-stu-id="f9569-388">**Performance analysis and tuning**: To troubleshoot the performance issue, let’s look at how the data is processed and moved.</span></span>

1. <span data-ttu-id="f9569-389">**Read data**: Integration runtime opens a connection to SQL Server and sends the query.</span><span class="sxs-lookup"><span data-stu-id="f9569-389">**Read data**: Integration runtime opens a connection to SQL Server and sends the query.</span></span> <span data-ttu-id="f9569-390">SQL Server responds by sending the data stream to integration runtime via the intranet.</span><span class="sxs-lookup"><span data-stu-id="f9569-390">SQL Server responds by sending the data stream to integration runtime via the intranet.</span></span>
2. <span data-ttu-id="f9569-391">**Serialize and compress data**: Integration runtime serializes the data stream to CSV format, and compresses the data to a bzip2 stream.</span><span class="sxs-lookup"><span data-stu-id="f9569-391">**Serialize and compress data**: Integration runtime serializes the data stream to CSV format, and compresses the data to a bzip2 stream.</span></span>
3. <span data-ttu-id="f9569-392">**Write data**: Integration runtime uploads the bzip2 stream to Blob storage via the Internet.</span><span class="sxs-lookup"><span data-stu-id="f9569-392">**Write data**: Integration runtime uploads the bzip2 stream to Blob storage via the Internet.</span></span>

<span data-ttu-id="f9569-393">As you can see, the data is being processed and moved in a streaming sequential manner: SQL Server > LAN > Integration runtime > WAN > Blob storage.</span><span class="sxs-lookup"><span data-stu-id="f9569-393">As you can see, the data is being processed and moved in a streaming sequential manner: SQL Server > LAN > Integration runtime > WAN > Blob storage.</span></span> <span data-ttu-id="f9569-394">**The overall performance is gated by the minimum throughput across the pipeline**.</span><span class="sxs-lookup"><span data-stu-id="f9569-394">**The overall performance is gated by the minimum throughput across the pipeline**.</span></span>

![Data flow](./media/copy-activity-performance/case-study-pic-1.png)

<span data-ttu-id="f9569-396">One or more of the following factors might cause the performance bottleneck:</span><span class="sxs-lookup"><span data-stu-id="f9569-396">One or more of the following factors might cause the performance bottleneck:</span></span>

* <span data-ttu-id="f9569-397">**Source**: SQL Server itself has low throughput because of heavy loads.</span><span class="sxs-lookup"><span data-stu-id="f9569-397">**Source**: SQL Server itself has low throughput because of heavy loads.</span></span>
* <span data-ttu-id="f9569-398">**Self-hosted Integration Runtime**:</span><span class="sxs-lookup"><span data-stu-id="f9569-398">**Self-hosted Integration Runtime**:</span></span>
  * <span data-ttu-id="f9569-399">**LAN**: Integration runtime is located far from the SQL Server machine and has a low-bandwidth connection.</span><span class="sxs-lookup"><span data-stu-id="f9569-399">**LAN**: Integration runtime is located far from the SQL Server machine and has a low-bandwidth connection.</span></span>
  * <span data-ttu-id="f9569-400">**Integration runtime**: Integration runtime has reached its load limitations to perform the following operations:</span><span class="sxs-lookup"><span data-stu-id="f9569-400">**Integration runtime**: Integration runtime has reached its load limitations to perform the following operations:</span></span>
    * <span data-ttu-id="f9569-401">**Serialization**: Serializing the data stream to CSV format has slow throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-401">**Serialization**: Serializing the data stream to CSV format has slow throughput.</span></span>
    * <span data-ttu-id="f9569-402">**Compression**: You chose a slow compression codec (for example, bzip2, which is 2.8 MBps with Core i7).</span><span class="sxs-lookup"><span data-stu-id="f9569-402">**Compression**: You chose a slow compression codec (for example, bzip2, which is 2.8 MBps with Core i7).</span></span>
  * <span data-ttu-id="f9569-403">**WAN**: The bandwidth between the corporate network and your Azure services is low (for example, T1 = 1,544 kbps; T2 = 6,312 kbps).</span><span class="sxs-lookup"><span data-stu-id="f9569-403">**WAN**: The bandwidth between the corporate network and your Azure services is low (for example, T1 = 1,544 kbps; T2 = 6,312 kbps).</span></span>
* <span data-ttu-id="f9569-404">**Sink**: Blob storage has low throughput.</span><span class="sxs-lookup"><span data-stu-id="f9569-404">**Sink**: Blob storage has low throughput.</span></span> <span data-ttu-id="f9569-405">(This scenario is unlikely because its SLA guarantees a minimum of 60 MBps.)</span><span class="sxs-lookup"><span data-stu-id="f9569-405">(This scenario is unlikely because its SLA guarantees a minimum of 60 MBps.)</span></span>

<span data-ttu-id="f9569-406">In this case, bzip2 data compression might be slowing down the entire pipeline.</span><span class="sxs-lookup"><span data-stu-id="f9569-406">In this case, bzip2 data compression might be slowing down the entire pipeline.</span></span> <span data-ttu-id="f9569-407">Switching to a gzip compression codec might ease this bottleneck.</span><span class="sxs-lookup"><span data-stu-id="f9569-407">Switching to a gzip compression codec might ease this bottleneck.</span></span>

## <a name="reference"></a><span data-ttu-id="f9569-408">Reference</span><span class="sxs-lookup"><span data-stu-id="f9569-408">Reference</span></span>

<span data-ttu-id="f9569-409">Here is performance monitoring and tuning references for some of the supported data stores:</span><span class="sxs-lookup"><span data-stu-id="f9569-409">Here is performance monitoring and tuning references for some of the supported data stores:</span></span>

* <span data-ttu-id="f9569-410">Azure Storage (including Blob storage and Table storage): [Azure Storage scalability targets](../storage/common/storage-scalability-targets.md) and [Azure Storage performance and scalability checklist](../storage/common/storage-performance-checklist.md)</span><span class="sxs-lookup"><span data-stu-id="f9569-410">Azure Storage (including Blob storage and Table storage): [Azure Storage scalability targets](../storage/common/storage-scalability-targets.md) and [Azure Storage performance and scalability checklist](../storage/common/storage-performance-checklist.md)</span></span>
* <span data-ttu-id="f9569-411">Azure SQL Database: You can [monitor the performance](../sql-database/sql-database-single-database-monitor.md) and check the database transaction unit (DTU) percentage</span><span class="sxs-lookup"><span data-stu-id="f9569-411">Azure SQL Database: You can [monitor the performance](../sql-database/sql-database-single-database-monitor.md) and check the database transaction unit (DTU) percentage</span></span>
* <span data-ttu-id="f9569-412">Azure SQL Data Warehouse: Its capability is measured in data warehouse units (DWUs); see [Manage compute power in Azure SQL Data Warehouse (Overview)](../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md)</span><span class="sxs-lookup"><span data-stu-id="f9569-412">Azure SQL Data Warehouse: Its capability is measured in data warehouse units (DWUs); see [Manage compute power in Azure SQL Data Warehouse (Overview)](../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md)</span></span>
* <span data-ttu-id="f9569-413">Azure Cosmos DB: [Performance levels in Azure Cosmos DB](../cosmos-db/performance-levels.md)</span><span class="sxs-lookup"><span data-stu-id="f9569-413">Azure Cosmos DB: [Performance levels in Azure Cosmos DB](../cosmos-db/performance-levels.md)</span></span>
* <span data-ttu-id="f9569-414">On-premises SQL Server: [Monitor and tune for performance](https://msdn.microsoft.com/library/ms189081.aspx)</span><span class="sxs-lookup"><span data-stu-id="f9569-414">On-premises SQL Server: [Monitor and tune for performance](https://msdn.microsoft.com/library/ms189081.aspx)</span></span>
* <span data-ttu-id="f9569-415">On-premises file server: [Performance tuning for file servers](https://msdn.microsoft.com/library/dn567661.aspx)</span><span class="sxs-lookup"><span data-stu-id="f9569-415">On-premises file server: [Performance tuning for file servers](https://msdn.microsoft.com/library/dn567661.aspx)</span></span>

## <a name="next-steps"></a><span data-ttu-id="f9569-416">Next steps</span><span class="sxs-lookup"><span data-stu-id="f9569-416">Next steps</span></span>
<span data-ttu-id="f9569-417">See the other Copy Activity articles:</span><span class="sxs-lookup"><span data-stu-id="f9569-417">See the other Copy Activity articles:</span></span>

- [<span data-ttu-id="f9569-418">Copy activity overview</span><span class="sxs-lookup"><span data-stu-id="f9569-418">Copy activity overview</span></span>](copy-activity-overview.md)
- [<span data-ttu-id="f9569-419">Copy Activity schema mapping</span><span class="sxs-lookup"><span data-stu-id="f9569-419">Copy Activity schema mapping</span></span>](copy-activity-schema-and-type-mapping.md)
- [<span data-ttu-id="f9569-420">Copy activity fault tolerance</span><span class="sxs-lookup"><span data-stu-id="f9569-420">Copy activity fault tolerance</span></span>](copy-activity-fault-tolerance.md)
