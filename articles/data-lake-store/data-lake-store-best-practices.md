---
title: Best practices for using Azure Data Lake Storage Gen1 | Microsoft Docs
description: Learn the best practices about data ingestion, date security, and performance related to using Azure Data Lake Storage Gen1 (previously known as Azure Data Lake Store)
services: data-lake-store
documentationcenter: ''
author: sachinsbigdata
manager: jhubbard
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.date: 06/27/2018
ms.author: sachins
ms.openlocfilehash: ef2b5fe6c9b70eaea5ab4db2d4a0ca59ff82dbb9
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44869423"
---
# <a name="best-practices-for-using-azure-data-lake-storage-gen1"></a><span data-ttu-id="90115-103">Best practices for using Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-103">Best practices for using Azure Data Lake Storage Gen1</span></span>

[!INCLUDE [data-lake-storage-gen1-rename-note.md](../../includes/data-lake-storage-gen1-rename-note.md)]

<span data-ttu-id="90115-104">In this article, you learn about best practices and considerations for working with Azure Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-104">In this article, you learn about best practices and considerations for working with Azure Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-105">This article provides information around security, performance, resiliency, and monitoring for Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-105">This article provides information around security, performance, resiliency, and monitoring for Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-106">Before Data Lake Storage Gen1, working with truly big data in services like Azure HDInsight was complex.</span><span class="sxs-lookup"><span data-stu-id="90115-106">Before Data Lake Storage Gen1, working with truly big data in services like Azure HDInsight was complex.</span></span> <span data-ttu-id="90115-107">You had to shard data across multiple Blob storage accounts so that petabyte storage and optimal performance at that scale could be achieved.</span><span class="sxs-lookup"><span data-stu-id="90115-107">You had to shard data across multiple Blob storage accounts so that petabyte storage and optimal performance at that scale could be achieved.</span></span> <span data-ttu-id="90115-108">With Data Lake Storage Gen1, most of the hard limits for size and performance are removed.</span><span class="sxs-lookup"><span data-stu-id="90115-108">With Data Lake Storage Gen1, most of the hard limits for size and performance are removed.</span></span> <span data-ttu-id="90115-109">However, there are still some considerations that this article covers so that you can get the best performance with Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-109">However, there are still some considerations that this article covers so that you can get the best performance with Data Lake Storage Gen1.</span></span> 

## <a name="security-considerations"></a><span data-ttu-id="90115-110">Security considerations</span><span class="sxs-lookup"><span data-stu-id="90115-110">Security considerations</span></span>

<span data-ttu-id="90115-111">Azure Data Lake Storage Gen1 offers POSIX access controls and detailed auditing for Azure Active Directory (Azure AD) users, groups, and service principals.</span><span class="sxs-lookup"><span data-stu-id="90115-111">Azure Data Lake Storage Gen1 offers POSIX access controls and detailed auditing for Azure Active Directory (Azure AD) users, groups, and service principals.</span></span> <span data-ttu-id="90115-112">These access controls can be set to existing files and folders.</span><span class="sxs-lookup"><span data-stu-id="90115-112">These access controls can be set to existing files and folders.</span></span> <span data-ttu-id="90115-113">The access controls can also be used to create defaults that can be applied to new files or folders.</span><span class="sxs-lookup"><span data-stu-id="90115-113">The access controls can also be used to create defaults that can be applied to new files or folders.</span></span> <span data-ttu-id="90115-114">When permissions are set to existing folders and child objects, the permissions need to be propagated recursively on each object.</span><span class="sxs-lookup"><span data-stu-id="90115-114">When permissions are set to existing folders and child objects, the permissions need to be propagated recursively on each object.</span></span> <span data-ttu-id="90115-115">If there are large number of files,  propagating the permissions can take a long time.</span><span class="sxs-lookup"><span data-stu-id="90115-115">If there are large number of files,  propagating the permissions can take a long time.</span></span> <span data-ttu-id="90115-116">The time taken can range between 30-50 objects processed per second.</span><span class="sxs-lookup"><span data-stu-id="90115-116">The time taken can range between 30-50 objects processed per second.</span></span> <span data-ttu-id="90115-117">Hence, plan the folder structure and user groups appropriately.</span><span class="sxs-lookup"><span data-stu-id="90115-117">Hence, plan the folder structure and user groups appropriately.</span></span> <span data-ttu-id="90115-118">Otherwise, it can cause unanticipated delays and issues when you work with your data.</span><span class="sxs-lookup"><span data-stu-id="90115-118">Otherwise, it can cause unanticipated delays and issues when you work with your data.</span></span> 

<span data-ttu-id="90115-119">Assume you have a folder with 100,000 child objects.</span><span class="sxs-lookup"><span data-stu-id="90115-119">Assume you have a folder with 100,000 child objects.</span></span> <span data-ttu-id="90115-120">If you take the lower bound of 30 objects processed per second, to update the permission for the whole folder could take an hour.</span><span class="sxs-lookup"><span data-stu-id="90115-120">If you take the lower bound of 30 objects processed per second, to update the permission for the whole folder could take an hour.</span></span> <span data-ttu-id="90115-121">More details on Data Lake Storage Gen1 ACLs are available at [Access control in Azure Data Lake Storage Gen1](data-lake-store-access-control.md).</span><span class="sxs-lookup"><span data-stu-id="90115-121">More details on Data Lake Storage Gen1 ACLs are available at [Access control in Azure Data Lake Storage Gen1](data-lake-store-access-control.md).</span></span> <span data-ttu-id="90115-122">For improved performance on assigning ACLs recursively, you can use the Azure Data Lake Command-Line Tool.</span><span class="sxs-lookup"><span data-stu-id="90115-122">For improved performance on assigning ACLs recursively, you can use the Azure Data Lake Command-Line Tool.</span></span> <span data-ttu-id="90115-123">The tool creates multiple threads and recursive navigation logic to quickly apply ACLs to millions of files.</span><span class="sxs-lookup"><span data-stu-id="90115-123">The tool creates multiple threads and recursive navigation logic to quickly apply ACLs to millions of files.</span></span> <span data-ttu-id="90115-124">The tool is available for Linux and Windows, and the [documentation](https://github.com/Azure/data-lake-adlstool) and [downloads](http://aka.ms/adlstool-download) for this tool can be found on GitHub.</span><span class="sxs-lookup"><span data-stu-id="90115-124">The tool is available for Linux and Windows, and the [documentation](https://github.com/Azure/data-lake-adlstool) and [downloads](http://aka.ms/adlstool-download) for this tool can be found on GitHub.</span></span> <span data-ttu-id="90115-125">These same performance improvements can be enabled by your own tools written with the Data Lake Storage Gen1 [.NET](data-lake-store-data-operations-net-sdk.md) and [Java](data-lake-store-get-started-java-sdk.md) SDKs.</span><span class="sxs-lookup"><span data-stu-id="90115-125">These same performance improvements can be enabled by your own tools written with the Data Lake Storage Gen1 [.NET](data-lake-store-data-operations-net-sdk.md) and [Java](data-lake-store-get-started-java-sdk.md) SDKs.</span></span>

### <a name="use-security-groups-versus-individual-users"></a><span data-ttu-id="90115-126">Use security groups versus individual users</span><span class="sxs-lookup"><span data-stu-id="90115-126">Use security groups versus individual users</span></span> 

<span data-ttu-id="90115-127">When working with big data in Data Lake Storage Gen1, most likely a service principal is used to allow services such as Azure  HDInsight to work with the data.</span><span class="sxs-lookup"><span data-stu-id="90115-127">When working with big data in Data Lake Storage Gen1, most likely a service principal is used to allow services such as Azure  HDInsight to work with the data.</span></span> <span data-ttu-id="90115-128">However, there might be cases where individual users need access to the data as well.</span><span class="sxs-lookup"><span data-stu-id="90115-128">However, there might be cases where individual users need access to the data as well.</span></span> <span data-ttu-id="90115-129">In such cases, you must use Azure Active Directory [security groups](data-lake-store-secure-data.md#create-security-groups-in-azure-active-directory) instead of assigning individual users to folders and files.</span><span class="sxs-lookup"><span data-stu-id="90115-129">In such cases, you must use Azure Active Directory [security groups](data-lake-store-secure-data.md#create-security-groups-in-azure-active-directory) instead of assigning individual users to folders and files.</span></span> 

<span data-ttu-id="90115-130">Once a security group is assigned permissions, adding or removing users from the group doesn’t require any updates to Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-130">Once a security group is assigned permissions, adding or removing users from the group doesn’t require any updates to Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-131">This also helps ensure you don't exceed the limit of [32 Access and Default ACLs](../azure-subscription-service-limits.md#data-lake-store-limits) (this includes the four POSIX-style ACLs that are always associated with every file and folder: [the owning user](data-lake-store-access-control.md#the-owning-user), [the owning group](data-lake-store-access-control.md#the-owning-group), [the mask](data-lake-store-access-control.md#the-mask), and other).</span><span class="sxs-lookup"><span data-stu-id="90115-131">This also helps ensure you don't exceed the limit of [32 Access and Default ACLs](../azure-subscription-service-limits.md#data-lake-store-limits) (this includes the four POSIX-style ACLs that are always associated with every file and folder: [the owning user](data-lake-store-access-control.md#the-owning-user), [the owning group](data-lake-store-access-control.md#the-owning-group), [the mask](data-lake-store-access-control.md#the-mask), and other).</span></span>

### <a name="security-for-groups"></a><span data-ttu-id="90115-132">Security for groups</span><span class="sxs-lookup"><span data-stu-id="90115-132">Security for groups</span></span> 

<span data-ttu-id="90115-133">As discussed, when users need access to Data Lake Storage Gen1, it’s best to use Azure Active Directory security groups.</span><span class="sxs-lookup"><span data-stu-id="90115-133">As discussed, when users need access to Data Lake Storage Gen1, it’s best to use Azure Active Directory security groups.</span></span> <span data-ttu-id="90115-134">Some recommended groups to start with might be **ReadOnlyUsers**, **WriteAccessUsers**, and **FullAccessUsers** for the root of the account, and even separate ones for key subfolders.</span><span class="sxs-lookup"><span data-stu-id="90115-134">Some recommended groups to start with might be **ReadOnlyUsers**, **WriteAccessUsers**, and **FullAccessUsers** for the root of the account, and even separate ones for key subfolders.</span></span> <span data-ttu-id="90115-135">If there are any other anticipated groups of users that might be added later, but have not been identified yet, you might consider creating dummy security groups that have access to certain folders.</span><span class="sxs-lookup"><span data-stu-id="90115-135">If there are any other anticipated groups of users that might be added later, but have not been identified yet, you might consider creating dummy security groups that have access to certain folders.</span></span> <span data-ttu-id="90115-136">Using security group ensures that later you do not need a long processing time for assigning new permissions to thousands of files.</span><span class="sxs-lookup"><span data-stu-id="90115-136">Using security group ensures that later you do not need a long processing time for assigning new permissions to thousands of files.</span></span> 

### <a name="security-for-service-principals"></a><span data-ttu-id="90115-137">Security for service principals</span><span class="sxs-lookup"><span data-stu-id="90115-137">Security for service principals</span></span> 

<span data-ttu-id="90115-138">Azure Active Directory service principals are typically used by services like Azure HDInsight to access data in Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-138">Azure Active Directory service principals are typically used by services like Azure HDInsight to access data in Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-139">Depending on the access requirements across multiple workloads, there might be some considerations to ensure security inside and outside of the organization.</span><span class="sxs-lookup"><span data-stu-id="90115-139">Depending on the access requirements across multiple workloads, there might be some considerations to ensure security inside and outside of the organization.</span></span> <span data-ttu-id="90115-140">For many customers, a single Azure Active Directory service principal might be adequate, and it can have full permissions at the root of the Data Lake Storage Gen1 account.</span><span class="sxs-lookup"><span data-stu-id="90115-140">For many customers, a single Azure Active Directory service principal might be adequate, and it can have full permissions at the root of the Data Lake Storage Gen1 account.</span></span> <span data-ttu-id="90115-141">Other customers might require multiple clusters with different service principals where one cluster has full access to the data, and another cluster with only read access.</span><span class="sxs-lookup"><span data-stu-id="90115-141">Other customers might require multiple clusters with different service principals where one cluster has full access to the data, and another cluster with only read access.</span></span> <span data-ttu-id="90115-142">As with the security groups, you might consider making a service principal for each anticipated scenario (read, write, full) once a Data Lake Storage Gen1 account is created.</span><span class="sxs-lookup"><span data-stu-id="90115-142">As with the security groups, you might consider making a service principal for each anticipated scenario (read, write, full) once a Data Lake Storage Gen1 account is created.</span></span> 

### <a name="enable-the-data-lake-storage-gen1-firewall-with-azure-service-access"></a><span data-ttu-id="90115-143">Enable the Data Lake Storage Gen1 firewall with Azure service access</span><span class="sxs-lookup"><span data-stu-id="90115-143">Enable the Data Lake Storage Gen1 firewall with Azure service access</span></span> 

<span data-ttu-id="90115-144">Data Lake Storage Gen1 supports the option of turning on a firewall and limiting access only to Azure services, which is recommended for a smaller attack vector from outside intrusions.</span><span class="sxs-lookup"><span data-stu-id="90115-144">Data Lake Storage Gen1 supports the option of turning on a firewall and limiting access only to Azure services, which is recommended for a smaller attack vector from outside intrusions.</span></span> <span data-ttu-id="90115-145">Firewall can be enabled on the Data Lake Storage Gen1 account in the Azure portal via the **Firewall** > **Enable Firewall (ON)** > **Allow access to Azure services** options.</span><span class="sxs-lookup"><span data-stu-id="90115-145">Firewall can be enabled on the Data Lake Storage Gen1 account in the Azure portal via the **Firewall** > **Enable Firewall (ON)** > **Allow access to Azure services** options.</span></span>  

<span data-ttu-id="90115-146">![Firewall settings in Data Lake Storage Gen1](./media/data-lake-store-best-practices/data-lake-store-firewall-setting.png "Firewall settings in Data Lake Storage Gen1")</span><span class="sxs-lookup"><span data-stu-id="90115-146">![Firewall settings in Data Lake Storage Gen1](./media/data-lake-store-best-practices/data-lake-store-firewall-setting.png "Firewall settings in Data Lake Storage Gen1")</span></span>

<span data-ttu-id="90115-147">Once firewall is enabled, only Azure services such as HDInsight, Data Factory, SQL Data Warehouse, etc. have access to Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-147">Once firewall is enabled, only Azure services such as HDInsight, Data Factory, SQL Data Warehouse, etc. have access to Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-148">Due to the internal network address translation used by Azure, the Data Lake Storage Gen1 firewall does not support restricting specific services by IP and is only intended for restrictions of endpoints outside of Azure, such as on-premises.</span><span class="sxs-lookup"><span data-stu-id="90115-148">Due to the internal network address translation used by Azure, the Data Lake Storage Gen1 firewall does not support restricting specific services by IP and is only intended for restrictions of endpoints outside of Azure, such as on-premises.</span></span> 

## <a name="performance-and-scale-considerations"></a><span data-ttu-id="90115-149">Performance and scale considerations</span><span class="sxs-lookup"><span data-stu-id="90115-149">Performance and scale considerations</span></span>

<span data-ttu-id="90115-150">One of the most powerful features of Data Lake Storage Gen1 is that it removes the hard limits on data throughput.</span><span class="sxs-lookup"><span data-stu-id="90115-150">One of the most powerful features of Data Lake Storage Gen1 is that it removes the hard limits on data throughput.</span></span> <span data-ttu-id="90115-151">Removing the limits enables customers to grow their data size and accompanied performance requirements without needing to shard the data.</span><span class="sxs-lookup"><span data-stu-id="90115-151">Removing the limits enables customers to grow their data size and accompanied performance requirements without needing to shard the data.</span></span> <span data-ttu-id="90115-152">One of the most important considerations for optimizing Data Lake Storage Gen1 performance is that it performs the best when given parallelism.</span><span class="sxs-lookup"><span data-stu-id="90115-152">One of the most important considerations for optimizing Data Lake Storage Gen1 performance is that it performs the best when given parallelism.</span></span>

### <a name="improve-throughput-with-parallelism"></a><span data-ttu-id="90115-153">Improve throughput with parallelism</span><span class="sxs-lookup"><span data-stu-id="90115-153">Improve throughput with parallelism</span></span> 

<span data-ttu-id="90115-154">Consider giving 8-12 threads per core for the most optimal read/write throughput.</span><span class="sxs-lookup"><span data-stu-id="90115-154">Consider giving 8-12 threads per core for the most optimal read/write throughput.</span></span> <span data-ttu-id="90115-155">This is due to blocking reads/writes on a single thread, and more threads can allow higher concurrency on the VM.</span><span class="sxs-lookup"><span data-stu-id="90115-155">This is due to blocking reads/writes on a single thread, and more threads can allow higher concurrency on the VM.</span></span> <span data-ttu-id="90115-156">To ensure that levels are healthy and parallelism can be increased, be sure to monitor the VM’s CPU utilization.</span><span class="sxs-lookup"><span data-stu-id="90115-156">To ensure that levels are healthy and parallelism can be increased, be sure to monitor the VM’s CPU utilization.</span></span>   

### <a name="avoid-small-file-sizes"></a><span data-ttu-id="90115-157">Avoid small file sizes</span><span class="sxs-lookup"><span data-stu-id="90115-157">Avoid small file sizes</span></span>

<span data-ttu-id="90115-158">POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files.</span><span class="sxs-lookup"><span data-stu-id="90115-158">POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files.</span></span> <span data-ttu-id="90115-159">As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-159">As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-160">Avoiding small file sizes can have multiple benefits, such as:</span><span class="sxs-lookup"><span data-stu-id="90115-160">Avoiding small file sizes can have multiple benefits, such as:</span></span>

* <span data-ttu-id="90115-161">Lowering the authentication checks across multiple files</span><span class="sxs-lookup"><span data-stu-id="90115-161">Lowering the authentication checks across multiple files</span></span>
* <span data-ttu-id="90115-162">Reduced open file connections</span><span class="sxs-lookup"><span data-stu-id="90115-162">Reduced open file connections</span></span>
* <span data-ttu-id="90115-163">Faster copying/replication</span><span class="sxs-lookup"><span data-stu-id="90115-163">Faster copying/replication</span></span>
* <span data-ttu-id="90115-164">Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions</span><span class="sxs-lookup"><span data-stu-id="90115-164">Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions</span></span> 

<span data-ttu-id="90115-165">Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater.</span><span class="sxs-lookup"><span data-stu-id="90115-165">Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater.</span></span> <span data-ttu-id="90115-166">If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.</span><span class="sxs-lookup"><span data-stu-id="90115-166">If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.</span></span> <span data-ttu-id="90115-167">For more information and recommendation on file sizes and organizing the data in Data Lake Storage Gen1, see [Structure your data set](data-lake-store-performance-tuning-guidance.md#structure-your-data-set).</span><span class="sxs-lookup"><span data-stu-id="90115-167">For more information and recommendation on file sizes and organizing the data in Data Lake Storage Gen1, see [Structure your data set](data-lake-store-performance-tuning-guidance.md#structure-your-data-set).</span></span>

### <a name="large-file-sizes-and-potential-performance-impact"></a><span data-ttu-id="90115-168">Large file sizes and potential performance impact</span><span class="sxs-lookup"><span data-stu-id="90115-168">Large file sizes and potential performance impact</span></span>

<span data-ttu-id="90115-169">Although Data Lake Storage Gen1 supports large files up to petabytes in size, for optimal performance and depending on the process reading the data, it might not be ideal to go above 2 GB on average.</span><span class="sxs-lookup"><span data-stu-id="90115-169">Although Data Lake Storage Gen1 supports large files up to petabytes in size, for optimal performance and depending on the process reading the data, it might not be ideal to go above 2 GB on average.</span></span> <span data-ttu-id="90115-170">For example, when using **Distcp** to copy data between locations or different storage accounts, files are the finest level of granularity used to determine map tasks.</span><span class="sxs-lookup"><span data-stu-id="90115-170">For example, when using **Distcp** to copy data between locations or different storage accounts, files are the finest level of granularity used to determine map tasks.</span></span> <span data-ttu-id="90115-171">So, if you are copying 10 files that are 1 TB each, at most 10 mappers are allocated.</span><span class="sxs-lookup"><span data-stu-id="90115-171">So, if you are copying 10 files that are 1 TB each, at most 10 mappers are allocated.</span></span> <span data-ttu-id="90115-172">Also, if you have lots of files with mappers assigned, initially the mappers work in parallel to move large files.</span><span class="sxs-lookup"><span data-stu-id="90115-172">Also, if you have lots of files with mappers assigned, initially the mappers work in parallel to move large files.</span></span> <span data-ttu-id="90115-173">However, as the job starts to wind down only a few mappers remain allocated and you can be stuck with a single mapper assigned to a large file.</span><span class="sxs-lookup"><span data-stu-id="90115-173">However, as the job starts to wind down only a few mappers remain allocated and you can be stuck with a single mapper assigned to a large file.</span></span> <span data-ttu-id="90115-174">Microsoft has submitted improvements to Distcp to address this issue in future Hadoop versions.</span><span class="sxs-lookup"><span data-stu-id="90115-174">Microsoft has submitted improvements to Distcp to address this issue in future Hadoop versions.</span></span>  

<span data-ttu-id="90115-175">Another example to consider is when using Azure Data Lake Analytics with Data Lake Storage Gen1.</span><span class="sxs-lookup"><span data-stu-id="90115-175">Another example to consider is when using Azure Data Lake Analytics with Data Lake Storage Gen1.</span></span> <span data-ttu-id="90115-176">Depending on the processing done by the extractor, some files that cannot be split (for example, XML, JSON) could suffer in performance when greater than 2 GB.</span><span class="sxs-lookup"><span data-stu-id="90115-176">Depending on the processing done by the extractor, some files that cannot be split (for example, XML, JSON) could suffer in performance when greater than 2 GB.</span></span> <span data-ttu-id="90115-177">In cases where files can be split by an extractor (for example, CSV), large files are preferred.</span><span class="sxs-lookup"><span data-stu-id="90115-177">In cases where files can be split by an extractor (for example, CSV), large files are preferred.</span></span>

### <a name="capacity-plan-for-your-workload"></a><span data-ttu-id="90115-178">Capacity plan for your workload</span><span class="sxs-lookup"><span data-stu-id="90115-178">Capacity plan for your workload</span></span> 

<span data-ttu-id="90115-179">Azure Data Lake Storage Gen1 removes the hard IO throttling limits that are placed on Blob storage accounts.</span><span class="sxs-lookup"><span data-stu-id="90115-179">Azure Data Lake Storage Gen1 removes the hard IO throttling limits that are placed on Blob storage accounts.</span></span> <span data-ttu-id="90115-180">However, there are still soft limits that need to be considered.</span><span class="sxs-lookup"><span data-stu-id="90115-180">However, there are still soft limits that need to be considered.</span></span> <span data-ttu-id="90115-181">The default ingress/egress throttling limits meet the needs of most scenarios.</span><span class="sxs-lookup"><span data-stu-id="90115-181">The default ingress/egress throttling limits meet the needs of most scenarios.</span></span> <span data-ttu-id="90115-182">If your workload needs to have the limits increased, work with Microsoft support.</span><span class="sxs-lookup"><span data-stu-id="90115-182">If your workload needs to have the limits increased, work with Microsoft support.</span></span> <span data-ttu-id="90115-183">Also, look at the limits during the proof-of-concept stage so that IO throttling limits are not hit during production.</span><span class="sxs-lookup"><span data-stu-id="90115-183">Also, look at the limits during the proof-of-concept stage so that IO throttling limits are not hit during production.</span></span> <span data-ttu-id="90115-184">If that happens, it might require waiting for a manual increase from the Microsoft engineering team.</span><span class="sxs-lookup"><span data-stu-id="90115-184">If that happens, it might require waiting for a manual increase from the Microsoft engineering team.</span></span> <span data-ttu-id="90115-185">If IO throttling occurs, Azure Data Lake Storage Gen1 returns an error code of 429, and ideally should be retried with an appropriate exponential backoff policy.</span><span class="sxs-lookup"><span data-stu-id="90115-185">If IO throttling occurs, Azure Data Lake Storage Gen1 returns an error code of 429, and ideally should be retried with an appropriate exponential backoff policy.</span></span> 

### <a name="optimize-writes-with-the-data-lake-storage-gen1-driver-buffer"></a><span data-ttu-id="90115-186">Optimize “writes” with the Data Lake Storage Gen1 driver buffer</span><span class="sxs-lookup"><span data-stu-id="90115-186">Optimize “writes” with the Data Lake Storage Gen1 driver buffer</span></span> 

<span data-ttu-id="90115-187">To optimize performance and reduce IOPS when writing to Data Lake Storage Gen1 from Hadoop, perform write operations as close to the Data Lake Storage Gen1 driver buffer size as possible.</span><span class="sxs-lookup"><span data-stu-id="90115-187">To optimize performance and reduce IOPS when writing to Data Lake Storage Gen1 from Hadoop, perform write operations as close to the Data Lake Storage Gen1 driver buffer size as possible.</span></span> <span data-ttu-id="90115-188">Try not to exceed the buffer size before flushing, such as when streaming using Apache Storm or Spark streaming workloads.</span><span class="sxs-lookup"><span data-stu-id="90115-188">Try not to exceed the buffer size before flushing, such as when streaming using Apache Storm or Spark streaming workloads.</span></span> <span data-ttu-id="90115-189">When writing to Data Lake Storage Gen1 from HDInsight/Hadoop, it is important to know that Data Lake Storage Gen1 has a driver with a 4-MB buffer.</span><span class="sxs-lookup"><span data-stu-id="90115-189">When writing to Data Lake Storage Gen1 from HDInsight/Hadoop, it is important to know that Data Lake Storage Gen1 has a driver with a 4-MB buffer.</span></span> <span data-ttu-id="90115-190">Like many file system drivers, this buffer can be manually flushed before reaching the 4-MB size.</span><span class="sxs-lookup"><span data-stu-id="90115-190">Like many file system drivers, this buffer can be manually flushed before reaching the 4-MB size.</span></span> <span data-ttu-id="90115-191">If not, it is immediately flushed to storage if the next write exceeds the buffer’s maximum size.</span><span class="sxs-lookup"><span data-stu-id="90115-191">If not, it is immediately flushed to storage if the next write exceeds the buffer’s maximum size.</span></span> <span data-ttu-id="90115-192">Where possible, you must avoid an overrun or a significant underrun of the buffer when syncing/flushing policy by count or time window.</span><span class="sxs-lookup"><span data-stu-id="90115-192">Where possible, you must avoid an overrun or a significant underrun of the buffer when syncing/flushing policy by count or time window.</span></span>

## <a name="resiliency-considerations"></a><span data-ttu-id="90115-193">Resiliency considerations</span><span class="sxs-lookup"><span data-stu-id="90115-193">Resiliency considerations</span></span> 

<span data-ttu-id="90115-194">When architecting a system with Data Lake Storage Gen1 or any cloud service, you must consider your availability requirements and how to respond to potential interruptions in the service.</span><span class="sxs-lookup"><span data-stu-id="90115-194">When architecting a system with Data Lake Storage Gen1 or any cloud service, you must consider your availability requirements and how to respond to potential interruptions in the service.</span></span> <span data-ttu-id="90115-195">An issue could be localized to the specific instance or even region-wide, so having a plan for both is important.</span><span class="sxs-lookup"><span data-stu-id="90115-195">An issue could be localized to the specific instance or even region-wide, so having a plan for both is important.</span></span> <span data-ttu-id="90115-196">Depending on the **recovery time objective** and the **recovery point objective** SLAs for your workload, you might choose a more or less aggressive strategy for high availability and disaster recovery.</span><span class="sxs-lookup"><span data-stu-id="90115-196">Depending on the **recovery time objective** and the **recovery point objective** SLAs for your workload, you might choose a more or less aggressive strategy for high availability and disaster recovery.</span></span>

### <a name="high-availability-and-disaster-recovery"></a><span data-ttu-id="90115-197">High availability and disaster recovery</span><span class="sxs-lookup"><span data-stu-id="90115-197">High availability and disaster recovery</span></span> 

<span data-ttu-id="90115-198">High availability (HA) and disaster recovery (DR) can sometimes be combined together, although each has a slightly different strategy, especially when it comes to data.</span><span class="sxs-lookup"><span data-stu-id="90115-198">High availability (HA) and disaster recovery (DR) can sometimes be combined together, although each has a slightly different strategy, especially when it comes to data.</span></span> <span data-ttu-id="90115-199">Data Lake Storage Gen1 already handles 3x replication under the hood to guard against localized hardware failures.</span><span class="sxs-lookup"><span data-stu-id="90115-199">Data Lake Storage Gen1 already handles 3x replication under the hood to guard against localized hardware failures.</span></span> <span data-ttu-id="90115-200">However, since replication across regions is not built in, you must manage this yourself.</span><span class="sxs-lookup"><span data-stu-id="90115-200">However, since replication across regions is not built in, you must manage this yourself.</span></span> <span data-ttu-id="90115-201">When building a plan for HA, in the event of a service interruption the workload needs access to the latest data as quickly as possible by switching over to a separately replicated instance locally or in a new region.</span><span class="sxs-lookup"><span data-stu-id="90115-201">When building a plan for HA, in the event of a service interruption the workload needs access to the latest data as quickly as possible by switching over to a separately replicated instance locally or in a new region.</span></span>  

<span data-ttu-id="90115-202">In a DR strategy, to prepare for the unlikely event of a catastrophic failure of a region, it is also important to have data replicated to a different region.</span><span class="sxs-lookup"><span data-stu-id="90115-202">In a DR strategy, to prepare for the unlikely event of a catastrophic failure of a region, it is also important to have data replicated to a different region.</span></span> <span data-ttu-id="90115-203">This data might initially be the same as the replicated HA data.</span><span class="sxs-lookup"><span data-stu-id="90115-203">This data might initially be the same as the replicated HA data.</span></span> <span data-ttu-id="90115-204">However, you must also consider your requirements for edge cases such as data corruption where you may want to create periodic snapshots to fall back to.</span><span class="sxs-lookup"><span data-stu-id="90115-204">However, you must also consider your requirements for edge cases such as data corruption where you may want to create periodic snapshots to fall back to.</span></span> <span data-ttu-id="90115-205">Depending on the importance and size of the data, consider rolling delta snapshots of 1-, 6-, and 24-hour periods on the local and/or secondary store, according to risk tolerances.</span><span class="sxs-lookup"><span data-stu-id="90115-205">Depending on the importance and size of the data, consider rolling delta snapshots of 1-, 6-, and 24-hour periods on the local and/or secondary store, according to risk tolerances.</span></span> 

<span data-ttu-id="90115-206">For data resiliency with Data Lake Storage Gen1, it is recommended to geo-replicate your data to a separate region with a frequency that satisfies your HA/DR requirements, ideally every hour.</span><span class="sxs-lookup"><span data-stu-id="90115-206">For data resiliency with Data Lake Storage Gen1, it is recommended to geo-replicate your data to a separate region with a frequency that satisfies your HA/DR requirements, ideally every hour.</span></span> <span data-ttu-id="90115-207">This frequency of replication minimizes massive data movements that can have competing throughput needs with the main system and a better recovery point objective (RPO).</span><span class="sxs-lookup"><span data-stu-id="90115-207">This frequency of replication minimizes massive data movements that can have competing throughput needs with the main system and a better recovery point objective (RPO).</span></span> <span data-ttu-id="90115-208">Additionally, you should consider ways for the application using Data Lake Storage Gen1 to automatically fail over to the secondary account through monitoring triggers or length of failed attempts, or at least send a notification to admins for manual intervention.</span><span class="sxs-lookup"><span data-stu-id="90115-208">Additionally, you should consider ways for the application using Data Lake Storage Gen1 to automatically fail over to the secondary account through monitoring triggers or length of failed attempts, or at least send a notification to admins for manual intervention.</span></span> <span data-ttu-id="90115-209">Keep in mind that there is tradeoff of failing over versus waiting for a service to come back online.</span><span class="sxs-lookup"><span data-stu-id="90115-209">Keep in mind that there is tradeoff of failing over versus waiting for a service to come back online.</span></span> <span data-ttu-id="90115-210">If the data hasn't finished replicating, a failover could cause potential data loss, inconsistency, or complex merging of the data.</span><span class="sxs-lookup"><span data-stu-id="90115-210">If the data hasn't finished replicating, a failover could cause potential data loss, inconsistency, or complex merging of the data.</span></span> 

<span data-ttu-id="90115-211">Below are the top three recommended options for orchestrating replication between Data Lake Storage Gen1 accounts, and key differences between each of them.</span><span class="sxs-lookup"><span data-stu-id="90115-211">Below are the top three recommended options for orchestrating replication between Data Lake Storage Gen1 accounts, and key differences between each of them.</span></span>


|  |<span data-ttu-id="90115-212">Distcp</span><span class="sxs-lookup"><span data-stu-id="90115-212">Distcp</span></span>  |<span data-ttu-id="90115-213">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="90115-213">Azure Data Factory</span></span>  |<span data-ttu-id="90115-214">AdlCopy</span><span class="sxs-lookup"><span data-stu-id="90115-214">AdlCopy</span></span>  |
|---------|---------|---------|---------|
|<span data-ttu-id="90115-215">**Scale limits**</span><span class="sxs-lookup"><span data-stu-id="90115-215">**Scale limits**</span></span>     | <span data-ttu-id="90115-216">Bounded by worker nodes</span><span class="sxs-lookup"><span data-stu-id="90115-216">Bounded by worker nodes</span></span>        | <span data-ttu-id="90115-217">Limited by Max Cloud Data Movement units</span><span class="sxs-lookup"><span data-stu-id="90115-217">Limited by Max Cloud Data Movement units</span></span>        | <span data-ttu-id="90115-218">Bound by Analytics units</span><span class="sxs-lookup"><span data-stu-id="90115-218">Bound by Analytics units</span></span>        |
|<span data-ttu-id="90115-219">**Supports copying deltas**</span><span class="sxs-lookup"><span data-stu-id="90115-219">**Supports copying deltas**</span></span>     |   <span data-ttu-id="90115-220">Yes</span><span class="sxs-lookup"><span data-stu-id="90115-220">Yes</span></span>      | <span data-ttu-id="90115-221">No</span><span class="sxs-lookup"><span data-stu-id="90115-221">No</span></span>         | <span data-ttu-id="90115-222">No</span><span class="sxs-lookup"><span data-stu-id="90115-222">No</span></span>         |
|<span data-ttu-id="90115-223">**Built-in orchestration**</span><span class="sxs-lookup"><span data-stu-id="90115-223">**Built-in orchestration**</span></span>     |  <span data-ttu-id="90115-224">No (use Oozie Airflow or cron jobs)</span><span class="sxs-lookup"><span data-stu-id="90115-224">No (use Oozie Airflow or cron jobs)</span></span>       | <span data-ttu-id="90115-225">Yes</span><span class="sxs-lookup"><span data-stu-id="90115-225">Yes</span></span>        | <span data-ttu-id="90115-226">No (Use Azure Automation or Windows Task Scheduler)</span><span class="sxs-lookup"><span data-stu-id="90115-226">No (Use Azure Automation or Windows Task Scheduler)</span></span>         |
|<span data-ttu-id="90115-227">**Supported file systems**</span><span class="sxs-lookup"><span data-stu-id="90115-227">**Supported file systems**</span></span>     | <span data-ttu-id="90115-228">ADL, HDFS, WASB, S3, GS, CFS</span><span class="sxs-lookup"><span data-stu-id="90115-228">ADL, HDFS, WASB, S3, GS, CFS</span></span>        |<span data-ttu-id="90115-229">Numerous, see [Connectors](../data-factory/connector-azure-blob-storage.md).</span><span class="sxs-lookup"><span data-stu-id="90115-229">Numerous, see [Connectors](../data-factory/connector-azure-blob-storage.md).</span></span>         | <span data-ttu-id="90115-230">ADL to ADL, WASB to ADL (same region only)</span><span class="sxs-lookup"><span data-stu-id="90115-230">ADL to ADL, WASB to ADL (same region only)</span></span>        |
|<span data-ttu-id="90115-231">**OS support**</span><span class="sxs-lookup"><span data-stu-id="90115-231">**OS support**</span></span>     |<span data-ttu-id="90115-232">Any OS running Hadoop</span><span class="sxs-lookup"><span data-stu-id="90115-232">Any OS running Hadoop</span></span>         | <span data-ttu-id="90115-233">N/A</span><span class="sxs-lookup"><span data-stu-id="90115-233">N/A</span></span>          | <span data-ttu-id="90115-234">Windows 10</span><span class="sxs-lookup"><span data-stu-id="90115-234">Windows 10</span></span>         |
   

### <a name="use-distcp-for-data-movement-between-two-locations"></a><span data-ttu-id="90115-235">Use Distcp for data movement between two locations</span><span class="sxs-lookup"><span data-stu-id="90115-235">Use Distcp for data movement between two locations</span></span> 

<span data-ttu-id="90115-236">Short for distributed copy, Distcp is a Linux command-line tool that comes with Hadoop and provides distributed data movement between two locations.</span><span class="sxs-lookup"><span data-stu-id="90115-236">Short for distributed copy, Distcp is a Linux command-line tool that comes with Hadoop and provides distributed data movement between two locations.</span></span> <span data-ttu-id="90115-237">The two locations can be Data Lake Storage Gen1, HDFS, WASB, or S3.</span><span class="sxs-lookup"><span data-stu-id="90115-237">The two locations can be Data Lake Storage Gen1, HDFS, WASB, or S3.</span></span> <span data-ttu-id="90115-238">This tool uses MapReduce jobs on a Hadoop cluster (for example, HDInsight) to scale out on all the nodes.</span><span class="sxs-lookup"><span data-stu-id="90115-238">This tool uses MapReduce jobs on a Hadoop cluster (for example, HDInsight) to scale out on all the nodes.</span></span> <span data-ttu-id="90115-239">Distcp is considered the fastest way to move big data without special network compression appliances.</span><span class="sxs-lookup"><span data-stu-id="90115-239">Distcp is considered the fastest way to move big data without special network compression appliances.</span></span> <span data-ttu-id="90115-240">Distcp also provides an option to only update deltas between two locations, handles automatic retries, as well as dynamic scaling of compute.</span><span class="sxs-lookup"><span data-stu-id="90115-240">Distcp also provides an option to only update deltas between two locations, handles automatic retries, as well as dynamic scaling of compute.</span></span> <span data-ttu-id="90115-241">This approach is incredibly efficient when it comes to replicating things like Hive/Spark tables that can have many large files in a single directory and you only want to copy over the modified data.</span><span class="sxs-lookup"><span data-stu-id="90115-241">This approach is incredibly efficient when it comes to replicating things like Hive/Spark tables that can have many large files in a single directory and you only want to copy over the modified data.</span></span> <span data-ttu-id="90115-242">For these reasons, Distcp is the most recommended tool for copying data between big data stores.</span><span class="sxs-lookup"><span data-stu-id="90115-242">For these reasons, Distcp is the most recommended tool for copying data between big data stores.</span></span> 

<span data-ttu-id="90115-243">Copy jobs can be triggered by Apache Oozie workflows using frequency or data triggers, as well as Linux cron jobs.</span><span class="sxs-lookup"><span data-stu-id="90115-243">Copy jobs can be triggered by Apache Oozie workflows using frequency or data triggers, as well as Linux cron jobs.</span></span> <span data-ttu-id="90115-244">For intensive replication jobs, it is recommended to spin up a separate HDInsight Hadoop cluster that can be tuned and scaled specifically for the copy jobs.</span><span class="sxs-lookup"><span data-stu-id="90115-244">For intensive replication jobs, it is recommended to spin up a separate HDInsight Hadoop cluster that can be tuned and scaled specifically for the copy jobs.</span></span> <span data-ttu-id="90115-245">This ensures that copy jobs do not interfere with critical jobs.</span><span class="sxs-lookup"><span data-stu-id="90115-245">This ensures that copy jobs do not interfere with critical jobs.</span></span> <span data-ttu-id="90115-246">If running replication on a wide enough frequency, the cluster can even be taken down between each job.</span><span class="sxs-lookup"><span data-stu-id="90115-246">If running replication on a wide enough frequency, the cluster can even be taken down between each job.</span></span> <span data-ttu-id="90115-247">If failing over to secondary region, make sure that another cluster is also spun up in the secondary region to replicate new data back to the primary Data Lake Storage Gen1 account once it comes back up.</span><span class="sxs-lookup"><span data-stu-id="90115-247">If failing over to secondary region, make sure that another cluster is also spun up in the secondary region to replicate new data back to the primary Data Lake Storage Gen1 account once it comes back up.</span></span> <span data-ttu-id="90115-248">For examples of using Distcp, see [Use Distcp to copy data between Azure Storage Blobs and Data Lake Storage Gen1](data-lake-store-copy-data-wasb-distcp.md).</span><span class="sxs-lookup"><span data-stu-id="90115-248">For examples of using Distcp, see [Use Distcp to copy data between Azure Storage Blobs and Data Lake Storage Gen1](data-lake-store-copy-data-wasb-distcp.md).</span></span>

### <a name="use-azure-data-factory-to-schedule-copy-jobs"></a><span data-ttu-id="90115-249">Use Azure Data Factory to schedule copy jobs</span><span class="sxs-lookup"><span data-stu-id="90115-249">Use Azure Data Factory to schedule copy jobs</span></span> 

<span data-ttu-id="90115-250">Azure Data Factory can also be used to schedule copy jobs using a **Copy Activity**, and can even be set up on a frequency via the **Copy Wizard**.</span><span class="sxs-lookup"><span data-stu-id="90115-250">Azure Data Factory can also be used to schedule copy jobs using a **Copy Activity**, and can even be set up on a frequency via the **Copy Wizard**.</span></span> <span data-ttu-id="90115-251">Keep in mind that Azure Data Factory has a limit of cloud data movement units (DMUs), and eventually caps the throughput/compute for large data workloads.</span><span class="sxs-lookup"><span data-stu-id="90115-251">Keep in mind that Azure Data Factory has a limit of cloud data movement units (DMUs), and eventually caps the throughput/compute for large data workloads.</span></span> <span data-ttu-id="90115-252">Additionally, Azure Data Factory currently does not offer delta updates between Data Lake Storage Gen1 accounts, so folders like Hive tables would require a complete copy to replicate.</span><span class="sxs-lookup"><span data-stu-id="90115-252">Additionally, Azure Data Factory currently does not offer delta updates between Data Lake Storage Gen1 accounts, so folders like Hive tables would require a complete copy to replicate.</span></span> <span data-ttu-id="90115-253">Refer to the [Copy Activity tuning guide](../data-factory/copy-activity-performance.md) for more information on copying with Data Factory.</span><span class="sxs-lookup"><span data-stu-id="90115-253">Refer to the [Copy Activity tuning guide](../data-factory/copy-activity-performance.md) for more information on copying with Data Factory.</span></span> 

### <a name="adlcopy"></a><span data-ttu-id="90115-254">AdlCopy</span><span class="sxs-lookup"><span data-stu-id="90115-254">AdlCopy</span></span>

<span data-ttu-id="90115-255">AdlCopy is a Windows command-line tool that allows you to copy data between two Data Lake Storage Gen1 accounts only within the same region.</span><span class="sxs-lookup"><span data-stu-id="90115-255">AdlCopy is a Windows command-line tool that allows you to copy data between two Data Lake Storage Gen1 accounts only within the same region.</span></span> <span data-ttu-id="90115-256">The AdlCopy tool provides a standalone option or the option to use an Azure Data Lake Analytics account to run your copy job.</span><span class="sxs-lookup"><span data-stu-id="90115-256">The AdlCopy tool provides a standalone option or the option to use an Azure Data Lake Analytics account to run your copy job.</span></span> <span data-ttu-id="90115-257">Though it was originally built for on-demand copies as opposed to a robust replication, it provides another option to do distributed copying across Data Lake Storage Gen1 accounts within the same region.</span><span class="sxs-lookup"><span data-stu-id="90115-257">Though it was originally built for on-demand copies as opposed to a robust replication, it provides another option to do distributed copying across Data Lake Storage Gen1 accounts within the same region.</span></span> <span data-ttu-id="90115-258">For reliability, it’s recommended to use the premium Data Lake Analytics option for any production workload.</span><span class="sxs-lookup"><span data-stu-id="90115-258">For reliability, it’s recommended to use the premium Data Lake Analytics option for any production workload.</span></span> <span data-ttu-id="90115-259">The standalone version can return busy responses and has limited scale and monitoring.</span><span class="sxs-lookup"><span data-stu-id="90115-259">The standalone version can return busy responses and has limited scale and monitoring.</span></span> 

<span data-ttu-id="90115-260">Like Distcp, the AdlCopy needs to be orchestrated by something like Azure Automation or Windows Task Scheduler.</span><span class="sxs-lookup"><span data-stu-id="90115-260">Like Distcp, the AdlCopy needs to be orchestrated by something like Azure Automation or Windows Task Scheduler.</span></span> <span data-ttu-id="90115-261">As with Data Factory, AdlCopy does not support copying only updated files, but recopies and overwrite existing files.</span><span class="sxs-lookup"><span data-stu-id="90115-261">As with Data Factory, AdlCopy does not support copying only updated files, but recopies and overwrite existing files.</span></span> <span data-ttu-id="90115-262">For more information and examples of using AdlCopy, see [Copy data from Azure Storage Blobs to Data Lake Storage Gen1](data-lake-store-copy-data-azure-storage-blob.md).</span><span class="sxs-lookup"><span data-stu-id="90115-262">For more information and examples of using AdlCopy, see [Copy data from Azure Storage Blobs to Data Lake Storage Gen1](data-lake-store-copy-data-azure-storage-blob.md).</span></span>

## <a name="monitoring-considerations"></a><span data-ttu-id="90115-263">Monitoring considerations</span><span class="sxs-lookup"><span data-stu-id="90115-263">Monitoring considerations</span></span> 

<span data-ttu-id="90115-264">Data Lake Storage Gen1 provides detailed diagnostic logs and auditing.</span><span class="sxs-lookup"><span data-stu-id="90115-264">Data Lake Storage Gen1 provides detailed diagnostic logs and auditing.</span></span> <span data-ttu-id="90115-265">Data Lake Storage Gen1 provides some basic metrics in the Azure portal under the Data Lake Storage Gen1 account and in Azure Monitor.</span><span class="sxs-lookup"><span data-stu-id="90115-265">Data Lake Storage Gen1 provides some basic metrics in the Azure portal under the Data Lake Storage Gen1 account and in Azure Monitor.</span></span> <span data-ttu-id="90115-266">Availability of Data Lake Storage Gen1 is displayed in the Azure portal.</span><span class="sxs-lookup"><span data-stu-id="90115-266">Availability of Data Lake Storage Gen1 is displayed in the Azure portal.</span></span> <span data-ttu-id="90115-267">However, this metric is refreshed every seven minutes and cannot be queried through a publicly exposed API.</span><span class="sxs-lookup"><span data-stu-id="90115-267">However, this metric is refreshed every seven minutes and cannot be queried through a publicly exposed API.</span></span> <span data-ttu-id="90115-268">To get the most up-to-date availability of a Data Lake Storage Gen1 account, you must run your own synthetic tests to validate availability.</span><span class="sxs-lookup"><span data-stu-id="90115-268">To get the most up-to-date availability of a Data Lake Storage Gen1 account, you must run your own synthetic tests to validate availability.</span></span> <span data-ttu-id="90115-269">Other metrics such as total storage utilization, read/write requests, and ingress/egress can take up to 24 hours to refresh.</span><span class="sxs-lookup"><span data-stu-id="90115-269">Other metrics such as total storage utilization, read/write requests, and ingress/egress can take up to 24 hours to refresh.</span></span> <span data-ttu-id="90115-270">So, more up-to-date metrics must be calculated manually through Hadoop command-line tools or aggregating log information.</span><span class="sxs-lookup"><span data-stu-id="90115-270">So, more up-to-date metrics must be calculated manually through Hadoop command-line tools or aggregating log information.</span></span> <span data-ttu-id="90115-271">The quickest way to get the most recent storage utilization is running this HDFS command from a Hadoop cluster node (for example, head node):</span><span class="sxs-lookup"><span data-stu-id="90115-271">The quickest way to get the most recent storage utilization is running this HDFS command from a Hadoop cluster node (for example, head node):</span></span>   

    hdfs dfs -du -s -h adl://<adlsg1_account_name>.azuredatalakestore.net:443/

### <a name="export-data-lake-storage-gen1-diagnostics"></a><span data-ttu-id="90115-272">Export Data Lake Storage Gen1 diagnostics</span><span class="sxs-lookup"><span data-stu-id="90115-272">Export Data Lake Storage Gen1 diagnostics</span></span> 

<span data-ttu-id="90115-273">One of the quickest ways to get access to searchable logs from Data Lake Storage Gen1 is to enable log shipping to **Log Analytics** under the **Diagnostics** blade for the Data Lake Storage Gen1 account.</span><span class="sxs-lookup"><span data-stu-id="90115-273">One of the quickest ways to get access to searchable logs from Data Lake Storage Gen1 is to enable log shipping to **Log Analytics** under the **Diagnostics** blade for the Data Lake Storage Gen1 account.</span></span> <span data-ttu-id="90115-274">This provides immediate access to incoming logs with time and content filters, along with alerting options (email/webhook) triggered within 15-minute intervals.</span><span class="sxs-lookup"><span data-stu-id="90115-274">This provides immediate access to incoming logs with time and content filters, along with alerting options (email/webhook) triggered within 15-minute intervals.</span></span> <span data-ttu-id="90115-275">For instructions, see [Accessing diagnostic logs for Azure Data Lake Storage Gen1](data-lake-store-diagnostic-logs.md).</span><span class="sxs-lookup"><span data-stu-id="90115-275">For instructions, see [Accessing diagnostic logs for Azure Data Lake Storage Gen1](data-lake-store-diagnostic-logs.md).</span></span> 

<span data-ttu-id="90115-276">For more real-time alerting and more control on where to land the logs, consider exporting logs to Azure EventHub where content can be analyzed individually or over a time window in order to submit real-time notifications to a queue.</span><span class="sxs-lookup"><span data-stu-id="90115-276">For more real-time alerting and more control on where to land the logs, consider exporting logs to Azure EventHub where content can be analyzed individually or over a time window in order to submit real-time notifications to a queue.</span></span> <span data-ttu-id="90115-277">A separate application such as a [Logic App](../connectors/connectors-create-api-azure-event-hubs.md) can then consume and communicate the alerts to the appropriate channel, as well as submit metrics to monitoring tools like NewRelic, Datadog, or AppDynamics.</span><span class="sxs-lookup"><span data-stu-id="90115-277">A separate application such as a [Logic App](../connectors/connectors-create-api-azure-event-hubs.md) can then consume and communicate the alerts to the appropriate channel, as well as submit metrics to monitoring tools like NewRelic, Datadog, or AppDynamics.</span></span> <span data-ttu-id="90115-278">Alternatively, if you are using a third-party tool such as ElasticSearch, you can export the logs to Blob Storage and use the [Azure Logstash plugin](https://github.com/Azure/azure-diagnostics-tools/tree/master/Logstash/logstash-input-azureblob) to consume the data into your Elasticsearch, Kibana, and Logstash (ELK) stack.</span><span class="sxs-lookup"><span data-stu-id="90115-278">Alternatively, if you are using a third-party tool such as ElasticSearch, you can export the logs to Blob Storage and use the [Azure Logstash plugin](https://github.com/Azure/azure-diagnostics-tools/tree/master/Logstash/logstash-input-azureblob) to consume the data into your Elasticsearch, Kibana, and Logstash (ELK) stack.</span></span>

### <a name="turn-on-debug-level-logging-in-hdinsight"></a><span data-ttu-id="90115-279">Turn on debug-level logging in HDInsight</span><span class="sxs-lookup"><span data-stu-id="90115-279">Turn on debug-level logging in HDInsight</span></span> 

<span data-ttu-id="90115-280">If Data Lake Storage Gen1 log shipping is not turned on, Azure HDInsight also provides a way to turn on [client-side logging for Data Lake Storage Gen1](data-lake-store-performance-tuning-mapreduce.md) via log4j.</span><span class="sxs-lookup"><span data-stu-id="90115-280">If Data Lake Storage Gen1 log shipping is not turned on, Azure HDInsight also provides a way to turn on [client-side logging for Data Lake Storage Gen1](data-lake-store-performance-tuning-mapreduce.md) via log4j.</span></span> <span data-ttu-id="90115-281">You must set the following property in **Ambari** > **YARN** > **Config** > **Advanced yarn-log4j configurations**:</span><span class="sxs-lookup"><span data-stu-id="90115-281">You must set the following property in **Ambari** > **YARN** > **Config** > **Advanced yarn-log4j configurations**:</span></span> 

    log4j.logger.com.microsoft.azure.datalake.store=DEBUG 

<span data-ttu-id="90115-282">Once the property is set and the nodes are restarted, Data Lake Storage Gen1 diagnostics is written to the YARN logs on the nodes (/tmp/<user>/yarn.log), and important details like errors or throttling (HTTP 429 error code) can be monitored.</span><span class="sxs-lookup"><span data-stu-id="90115-282">Once the property is set and the nodes are restarted, Data Lake Storage Gen1 diagnostics is written to the YARN logs on the nodes (/tmp/<user>/yarn.log), and important details like errors or throttling (HTTP 429 error code) can be monitored.</span></span> <span data-ttu-id="90115-283">This same information can also be monitored in Log Analytics or wherever logs are shipped to in the [Diagnostics](data-lake-store-diagnostic-logs.md) blade of the Data Lake Storage Gen1 account.</span><span class="sxs-lookup"><span data-stu-id="90115-283">This same information can also be monitored in Log Analytics or wherever logs are shipped to in the [Diagnostics](data-lake-store-diagnostic-logs.md) blade of the Data Lake Storage Gen1 account.</span></span> <span data-ttu-id="90115-284">It is recommended to at least have client-side logging turned on or utilize the log shipping option with Data Lake Storage Gen1 for operational visibility and easier debugging.</span><span class="sxs-lookup"><span data-stu-id="90115-284">It is recommended to at least have client-side logging turned on or utilize the log shipping option with Data Lake Storage Gen1 for operational visibility and easier debugging.</span></span>

### <a name="run-synthetic-transactions"></a><span data-ttu-id="90115-285">Run synthetic transactions</span><span class="sxs-lookup"><span data-stu-id="90115-285">Run synthetic transactions</span></span> 

<span data-ttu-id="90115-286">Currently, the service availability metric for Data Lake Storage Gen1 in the Azure portal has 7-minute refresh window.</span><span class="sxs-lookup"><span data-stu-id="90115-286">Currently, the service availability metric for Data Lake Storage Gen1 in the Azure portal has 7-minute refresh window.</span></span> <span data-ttu-id="90115-287">Also, it cannot  be queried using a publicly exposed API.</span><span class="sxs-lookup"><span data-stu-id="90115-287">Also, it cannot  be queried using a publicly exposed API.</span></span> <span data-ttu-id="90115-288">Hence, it is recommended to build a basic application that does synthetic transactions to Data Lake Storage Gen1 that can provide up to the minute availability.</span><span class="sxs-lookup"><span data-stu-id="90115-288">Hence, it is recommended to build a basic application that does synthetic transactions to Data Lake Storage Gen1 that can provide up to the minute availability.</span></span> <span data-ttu-id="90115-289">An example might be creating a WebJob, Logic App, or Azure Function App to perform a read, create, and update against Data Lake Storage Gen1 and send the results to your monitoring solution.</span><span class="sxs-lookup"><span data-stu-id="90115-289">An example might be creating a WebJob, Logic App, or Azure Function App to perform a read, create, and update against Data Lake Storage Gen1 and send the results to your monitoring solution.</span></span> <span data-ttu-id="90115-290">The operations can be done in a temporary folder and then deleted after the test, which might be run every 30-60 seconds, depending on requirements.</span><span class="sxs-lookup"><span data-stu-id="90115-290">The operations can be done in a temporary folder and then deleted after the test, which might be run every 30-60 seconds, depending on requirements.</span></span>

## <a name="directory-layout-considerations"></a><span data-ttu-id="90115-291">Directory layout considerations</span><span class="sxs-lookup"><span data-stu-id="90115-291">Directory layout considerations</span></span>

<span data-ttu-id="90115-292">When landing data into a data lake, it’s important to pre-plan the structure of the data so that security, partitioning, and processing can be utilized effectively.</span><span class="sxs-lookup"><span data-stu-id="90115-292">When landing data into a data lake, it’s important to pre-plan the structure of the data so that security, partitioning, and processing can be utilized effectively.</span></span> <span data-ttu-id="90115-293">Many of the following recommendations can be used whether it’s with Azure Data Lake Storage Gen1, Blob Storage, or HDFS.</span><span class="sxs-lookup"><span data-stu-id="90115-293">Many of the following recommendations can be used whether it’s with Azure Data Lake Storage Gen1, Blob Storage, or HDFS.</span></span> <span data-ttu-id="90115-294">Every workload has different requirements on how the data is consumed, but below are some common layouts to consider when working with IoT and batch scenarios.</span><span class="sxs-lookup"><span data-stu-id="90115-294">Every workload has different requirements on how the data is consumed, but below are some common layouts to consider when working with IoT and batch scenarios.</span></span>

### <a name="iot-structure"></a><span data-ttu-id="90115-295">IoT structure</span><span class="sxs-lookup"><span data-stu-id="90115-295">IoT structure</span></span> 

<span data-ttu-id="90115-296">In IoT workloads, there can be a great deal of data being landed in the data store that spans across numerous products, devices, organizations, and customers.</span><span class="sxs-lookup"><span data-stu-id="90115-296">In IoT workloads, there can be a great deal of data being landed in the data store that spans across numerous products, devices, organizations, and customers.</span></span> <span data-ttu-id="90115-297">It’s important to pre-plan the directory layout for organization, security, and efficient processing of the data for down-stream consumers.</span><span class="sxs-lookup"><span data-stu-id="90115-297">It’s important to pre-plan the directory layout for organization, security, and efficient processing of the data for down-stream consumers.</span></span> <span data-ttu-id="90115-298">A general template to consider might be the following layout:</span><span class="sxs-lookup"><span data-stu-id="90115-298">A general template to consider might be the following layout:</span></span> 

    {Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/ 

<span data-ttu-id="90115-299">For example, landing telemetry for an airplane engine within the UK might look like the following structure:</span><span class="sxs-lookup"><span data-stu-id="90115-299">For example, landing telemetry for an airplane engine within the UK might look like the following structure:</span></span> 

    UK/Planes/BA1293/Engine1/2017/08/11/12/ 

<span data-ttu-id="90115-300">There's an important reason to put the date at the end of the folder structure.</span><span class="sxs-lookup"><span data-stu-id="90115-300">There's an important reason to put the date at the end of the folder structure.</span></span> <span data-ttu-id="90115-301">If you want to lock down certain regions or subject matters to users/groups, then you can easily do so with the POSIX permissions.</span><span class="sxs-lookup"><span data-stu-id="90115-301">If you want to lock down certain regions or subject matters to users/groups, then you can easily do so with the POSIX permissions.</span></span> <span data-ttu-id="90115-302">Otherwise, if there was a need to restrict a certain security group to viewing just the UK data or certain planes, with the date structure in front a separate permission would be required for numerous folders under every hour folder.</span><span class="sxs-lookup"><span data-stu-id="90115-302">Otherwise, if there was a need to restrict a certain security group to viewing just the UK data or certain planes, with the date structure in front a separate permission would be required for numerous folders under every hour folder.</span></span> <span data-ttu-id="90115-303">Additionally, having the date structure in front would exponentially increase the number of folders as time went on.</span><span class="sxs-lookup"><span data-stu-id="90115-303">Additionally, having the date structure in front would exponentially increase the number of folders as time went on.</span></span>

### <a name="batch-jobs-structure"></a><span data-ttu-id="90115-304">Batch jobs structure</span><span class="sxs-lookup"><span data-stu-id="90115-304">Batch jobs structure</span></span> 

<span data-ttu-id="90115-305">From a high-level, a commonly used approach in batch processing is to land data in an “in” folder.</span><span class="sxs-lookup"><span data-stu-id="90115-305">From a high-level, a commonly used approach in batch processing is to land data in an “in” folder.</span></span> <span data-ttu-id="90115-306">Then, once the data is processed, put the new data into an “out” folder for downstream processes to consume.</span><span class="sxs-lookup"><span data-stu-id="90115-306">Then, once the data is processed, put the new data into an “out” folder for downstream processes to consume.</span></span> <span data-ttu-id="90115-307">This directory structure is seen sometimes for jobs that require processing on individual files and might not require massively parallel processing over large datasets.</span><span class="sxs-lookup"><span data-stu-id="90115-307">This directory structure is seen sometimes for jobs that require processing on individual files and might not require massively parallel processing over large datasets.</span></span> <span data-ttu-id="90115-308">Like the IoT structure recommended above, a good directory structure has the parent-level folders for things such as region and subject matters (for example, organization, product/producer).</span><span class="sxs-lookup"><span data-stu-id="90115-308">Like the IoT structure recommended above, a good directory structure has the parent-level folders for things such as region and subject matters (for example, organization, product/producer).</span></span> <span data-ttu-id="90115-309">This structure helps with securing the data across your organization and better management of the data in your workloads.</span><span class="sxs-lookup"><span data-stu-id="90115-309">This structure helps with securing the data across your organization and better management of the data in your workloads.</span></span> <span data-ttu-id="90115-310">Furthermore, consider date and time in the structure to allow better organization, filtered searches, security, and automation in the processing.</span><span class="sxs-lookup"><span data-stu-id="90115-310">Furthermore, consider date and time in the structure to allow better organization, filtered searches, security, and automation in the processing.</span></span> <span data-ttu-id="90115-311">The level of granularity for the date structure is determined by the interval on which the data is uploaded or processed, such as hourly, daily, or even monthly.</span><span class="sxs-lookup"><span data-stu-id="90115-311">The level of granularity for the date structure is determined by the interval on which the data is uploaded or processed, such as hourly, daily, or even monthly.</span></span> 

<span data-ttu-id="90115-312">Sometimes file processing is unsuccessful due to data corruption or unexpected formats.</span><span class="sxs-lookup"><span data-stu-id="90115-312">Sometimes file processing is unsuccessful due to data corruption or unexpected formats.</span></span> <span data-ttu-id="90115-313">In such cases, directory structure might benefit from a **/bad** folder to move the files to for further inspection.</span><span class="sxs-lookup"><span data-stu-id="90115-313">In such cases, directory structure might benefit from a **/bad** folder to move the files to for further inspection.</span></span> <span data-ttu-id="90115-314">The batch job might also handle the reporting or notification of these *bad* files for manual intervention.</span><span class="sxs-lookup"><span data-stu-id="90115-314">The batch job might also handle the reporting or notification of these *bad* files for manual intervention.</span></span> <span data-ttu-id="90115-315">Consider the following template structure:</span><span class="sxs-lookup"><span data-stu-id="90115-315">Consider the following template structure:</span></span> 

    {Region}/{SubjectMatter(s)}/In/{yyyy}/{mm}/{dd}/{hh}/ 
    {Region}/{SubjectMatter(s)}/Out/{yyyy}/{mm}/{dd}/{hh}/ 
    {Region}/{SubjectMatter(s)}/Bad/{yyyy}/{mm}/{dd}/{hh}/ 

<span data-ttu-id="90115-316">For example, a marketing firm receives daily data extracts of customer updates from their clients in North America.</span><span class="sxs-lookup"><span data-stu-id="90115-316">For example, a marketing firm receives daily data extracts of customer updates from their clients in North America.</span></span> <span data-ttu-id="90115-317">It might look like the following snippet before and after being processed:</span><span class="sxs-lookup"><span data-stu-id="90115-317">It might look like the following snippet before and after being processed:</span></span> 

    NA/Extracts/ACMEPaperCo/In/2017/08/14/updates_08142017.csv 
    NA/Extracts/ACMEPaperCo/Out/2017/08/14/processed_updates_08142017.csv 
 
<span data-ttu-id="90115-318">In the common case of batch data being processed directly into databases such as Hive or traditional SQL databases, there isn’t a need for an **/in** or **/out** folder since the output already goes into a separate folder for the Hive table or external database.</span><span class="sxs-lookup"><span data-stu-id="90115-318">In the common case of batch data being processed directly into databases such as Hive or traditional SQL databases, there isn’t a need for an **/in** or **/out** folder since the output already goes into a separate folder for the Hive table or external database.</span></span> <span data-ttu-id="90115-319">For example, daily extracts from customers would land into their respective folders, and orchestration by something like Azure Data Factory, Apache Oozie, or Apache Airflow would trigger a daily Hive or Spark job to process and write the data into a Hive table.</span><span class="sxs-lookup"><span data-stu-id="90115-319">For example, daily extracts from customers would land into their respective folders, and orchestration by something like Azure Data Factory, Apache Oozie, or Apache Airflow would trigger a daily Hive or Spark job to process and write the data into a Hive table.</span></span>

## <a name="next-steps"></a><span data-ttu-id="90115-320">Next steps</span><span class="sxs-lookup"><span data-stu-id="90115-320">Next steps</span></span>     

* [<span data-ttu-id="90115-321">Overview of Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-321">Overview of Azure Data Lake Storage Gen1</span></span>](data-lake-store-overview.md) 
* [<span data-ttu-id="90115-322">Access Control in Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-322">Access Control in Azure Data Lake Storage Gen1</span></span>](data-lake-store-access-control.md) 
* [<span data-ttu-id="90115-323">Security in Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-323">Security in Azure Data Lake Storage Gen1</span></span>](data-lake-store-security-overview.md)
* [<span data-ttu-id="90115-324">Tuning Azure Data Lake Storage Gen1 for performance</span><span class="sxs-lookup"><span data-stu-id="90115-324">Tuning Azure Data Lake Storage Gen1 for performance</span></span>](data-lake-store-performance-tuning-guidance.md)
* [<span data-ttu-id="90115-325">Performance tuning guidance for using HDInsight Spark with Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-325">Performance tuning guidance for using HDInsight Spark with Azure Data Lake Storage Gen1</span></span>](data-lake-store-performance-tuning-spark.md)
* [<span data-ttu-id="90115-326">Performance tuning guidance for using HDInsight Hive with Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-326">Performance tuning guidance for using HDInsight Hive with Azure Data Lake Storage Gen1</span></span>](data-lake-store-performance-tuning-hive.md)
* [<span data-ttu-id="90115-327">Data Orchestration using Azure Data Factory for Azure Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-327">Data Orchestration using Azure Data Factory for Azure Data Lake Storage Gen1</span></span>](https://mix.office.com/watch/1oa7le7t2u4ka)
* [<span data-ttu-id="90115-328">Create HDInsight clusters with Data Lake Storage Gen1</span><span class="sxs-lookup"><span data-stu-id="90115-328">Create HDInsight clusters with Data Lake Storage Gen1</span></span>](data-lake-store-hdinsight-hadoop-use-portal.md) 
