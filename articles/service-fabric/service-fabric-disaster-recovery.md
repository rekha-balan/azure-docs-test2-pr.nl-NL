---
title: Azure Service Fabric disaster recovery | Microsoft Docs
description: Azure Service Fabric offers the capabilities necessary to deal with all types of disasters. This article describes the types of disasters that can occur and how to deal with them.
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: ''
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: conceptual
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: d096de79ada6e15aa5a383e8b0897bd470c22b45
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44797737"
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="b202b-104">Disaster recovery in Azure Service Fabric</span><span class="sxs-lookup"><span data-stu-id="b202b-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="b202b-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="b202b-106">This is especially important for failures that are unplanned and outside of your control.</span><span class="sxs-lookup"><span data-stu-id="b202b-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="b202b-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span><span class="sxs-lookup"><span data-stu-id="b202b-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="b202b-108">It also discuss mitigations and actions to take if a disaster happened anyway.</span><span class="sxs-lookup"><span data-stu-id="b202b-108">It also discuss mitigations and actions to take if a disaster happened anyway.</span></span> <span data-ttu-id="b202b-109">The goal is to limit or eliminate the risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span><span class="sxs-lookup"><span data-stu-id="b202b-109">The goal is to limit or eliminate the risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="b202b-110">Avoiding disaster</span><span class="sxs-lookup"><span data-stu-id="b202b-110">Avoiding disaster</span></span>
<span data-ttu-id="b202b-111">Service Fabric's primary goal is to help you model both your environment and your services in such a way that common failure types are not disasters.</span><span class="sxs-lookup"><span data-stu-id="b202b-111">Service Fabric's primary goal is to help you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="b202b-112">In general there are two types of disaster/failure scenarios:</span><span class="sxs-lookup"><span data-stu-id="b202b-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="b202b-113">Hardware or software faults</span><span class="sxs-lookup"><span data-stu-id="b202b-113">Hardware or software faults</span></span>
2. <span data-ttu-id="b202b-114">Operational faults</span><span class="sxs-lookup"><span data-stu-id="b202b-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="b202b-115">Hardware and software faults</span><span class="sxs-lookup"><span data-stu-id="b202b-115">Hardware and software faults</span></span>
<span data-ttu-id="b202b-116">Hardware and software faults are unpredictable.</span><span class="sxs-lookup"><span data-stu-id="b202b-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="b202b-117">The easiest way to survive faults is running more copies of the service  spanned across hardware or software fault boundaries.</span><span class="sxs-lookup"><span data-stu-id="b202b-117">The easiest way to survive faults is running more copies of the service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="b202b-118">For example, if your service is running only on one particular machine, then the failure of that one machine is a disaster for that service.</span><span class="sxs-lookup"><span data-stu-id="b202b-118">For example, if your service is running only on one particular machine, then the failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="b202b-119">The simple way to avoid this disaster is to ensure that the service is actually running on multiple machines.</span><span class="sxs-lookup"><span data-stu-id="b202b-119">The simple way to avoid this disaster is to ensure that the service is actually running on multiple machines.</span></span> <span data-ttu-id="b202b-120">Testing is also necessary to ensure the failure of one machine doesn't disrupt the running service.</span><span class="sxs-lookup"><span data-stu-id="b202b-120">Testing is also necessary to ensure the failure of one machine doesn't disrupt the running service.</span></span> <span data-ttu-id="b202b-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload the remaining services.</span><span class="sxs-lookup"><span data-stu-id="b202b-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload the remaining services.</span></span> <span data-ttu-id="b202b-122">The same pattern works regardless of what you're trying to avoid the failure of.</span><span class="sxs-lookup"><span data-stu-id="b202b-122">The same pattern works regardless of what you're trying to avoid the failure of.</span></span> <span data-ttu-id="b202b-123">For example.</span><span class="sxs-lookup"><span data-stu-id="b202b-123">For example.</span></span> <span data-ttu-id="b202b-124">if you're concerned about the failure of a SAN, you run across multiple SANs.</span><span class="sxs-lookup"><span data-stu-id="b202b-124">if you're concerned about the failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="b202b-125">If you're concerned about the loss of a rack of servers, you run across multiple racks.</span><span class="sxs-lookup"><span data-stu-id="b202b-125">If you're concerned about the loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="b202b-126">If you're worried about the loss of datacenters, your service should run across multiple Azure regions or datacenters.</span><span class="sxs-lookup"><span data-stu-id="b202b-126">If you're worried about the loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="b202b-127">When running in this type of spanned mode, you're still subject to some types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span><span class="sxs-lookup"><span data-stu-id="b202b-127">When running in this type of spanned mode, you're still subject to some types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="b202b-128">Service Fabric provides many mechanisms for expanding the cluster and handles bringing failed nodes and services back.</span><span class="sxs-lookup"><span data-stu-id="b202b-128">Service Fabric provides many mechanisms for expanding the cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="b202b-129">Service Fabric also allows running many instances of your services in order to avoid these types of unplanned failures from turning into real disasters.</span><span class="sxs-lookup"><span data-stu-id="b202b-129">Service Fabric also allows running many instances of your services in order to avoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="b202b-130">There may be reasons why running a deployment large enough to span over failures is not feasible.</span><span class="sxs-lookup"><span data-stu-id="b202b-130">There may be reasons why running a deployment large enough to span over failures is not feasible.</span></span> <span data-ttu-id="b202b-131">For example, it may take more hardware resources than you're willing to pay for relative to the chance of failure.</span><span class="sxs-lookup"><span data-stu-id="b202b-131">For example, it may take more hardware resources than you're willing to pay for relative to the chance of failure.</span></span> <span data-ttu-id="b202b-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span><span class="sxs-lookup"><span data-stu-id="b202b-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="b202b-133">Where this line is drawn differs for each application.</span><span class="sxs-lookup"><span data-stu-id="b202b-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="b202b-134">For software faults specifically, the fault could be in the service that you are trying to scale.</span><span class="sxs-lookup"><span data-stu-id="b202b-134">For software faults specifically, the fault could be in the service that you are trying to scale.</span></span> <span data-ttu-id="b202b-135">In this case more copies don't prevent the disaster, since the failure condition is correlated across all the instances.</span><span class="sxs-lookup"><span data-stu-id="b202b-135">In this case more copies don't prevent the disaster, since the failure condition is correlated across all the instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="b202b-136">Operational faults</span><span class="sxs-lookup"><span data-stu-id="b202b-136">Operational faults</span></span>
<span data-ttu-id="b202b-137">Even if your service is spanned across the globe with many redundancies, it can still experience disastrous events.</span><span class="sxs-lookup"><span data-stu-id="b202b-137">Even if your service is spanned across the globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="b202b-138">For example, if someone accidentally reconfigures the dns name for the service, or deletes it outright.</span><span class="sxs-lookup"><span data-stu-id="b202b-138">For example, if someone accidentally reconfigures the dns name for the service, or deletes it outright.</span></span> <span data-ttu-id="b202b-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span><span class="sxs-lookup"><span data-stu-id="b202b-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="b202b-140">Unless there's some other mitigation, that service and all of the state it had is now gone.</span><span class="sxs-lookup"><span data-stu-id="b202b-140">Unless there's some other mitigation, that service and all of the state it had is now gone.</span></span> <span data-ttu-id="b202b-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="b202b-142">The best ways to avoid these types of operational faults are to</span><span class="sxs-lookup"><span data-stu-id="b202b-142">The best ways to avoid these types of operational faults are to</span></span>
1. <span data-ttu-id="b202b-143">restrict operational access to the environment</span><span class="sxs-lookup"><span data-stu-id="b202b-143">restrict operational access to the environment</span></span>
2. <span data-ttu-id="b202b-144">strictly audit dangerous operations</span><span class="sxs-lookup"><span data-stu-id="b202b-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="b202b-145">impose automation, prevent manual or out of band changes, and validate specific changes against the actual environment before enacting them</span><span class="sxs-lookup"><span data-stu-id="b202b-145">impose automation, prevent manual or out of band changes, and validate specific changes against the actual environment before enacting them</span></span>
4. <span data-ttu-id="b202b-146">ensure that destructive operations are "soft".</span><span class="sxs-lookup"><span data-stu-id="b202b-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="b202b-147">Soft operations don't take effect immediately or can be undone within some time window</span><span class="sxs-lookup"><span data-stu-id="b202b-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="b202b-148">Service Fabric provides some mechanisms to prevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span><span class="sxs-lookup"><span data-stu-id="b202b-148">Service Fabric provides some mechanisms to prevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="b202b-149">However, most of these operational faults require organizational efforts and other systems.</span><span class="sxs-lookup"><span data-stu-id="b202b-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="b202b-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span><span class="sxs-lookup"><span data-stu-id="b202b-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="b202b-151">Managing failures</span><span class="sxs-lookup"><span data-stu-id="b202b-151">Managing failures</span></span>
<span data-ttu-id="b202b-152">The goal of Service Fabric is almost always automatic management of failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-152">The goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="b202b-153">However, in order to handle some types of failures, services must have additional code.</span><span class="sxs-lookup"><span data-stu-id="b202b-153">However, in order to handle some types of failures, services must have additional code.</span></span> <span data-ttu-id="b202b-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span><span class="sxs-lookup"><span data-stu-id="b202b-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="b202b-155">Handling single failures</span><span class="sxs-lookup"><span data-stu-id="b202b-155">Handling single failures</span></span>
<span data-ttu-id="b202b-156">Single machines can fail for all sorts of reasons.</span><span class="sxs-lookup"><span data-stu-id="b202b-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="b202b-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="b202b-158">Other failures are in software.</span><span class="sxs-lookup"><span data-stu-id="b202b-158">Other failures are in software.</span></span> <span data-ttu-id="b202b-159">These include failures of the actual operating system and the service itself.</span><span class="sxs-lookup"><span data-stu-id="b202b-159">These include failures of the actual operating system and the service itself.</span></span> <span data-ttu-id="b202b-160">Service Fabric automatically detects these types of failures, including cases where the machine becomes isolated from other machines due to network issues.</span><span class="sxs-lookup"><span data-stu-id="b202b-160">Service Fabric automatically detects these types of failures, including cases where the machine becomes isolated from other machines due to network issues.</span></span>

<span data-ttu-id="b202b-161">Regardless of the type of service, running a single instance results in downtime for that service if that single copy of the code fails for any reason.</span><span class="sxs-lookup"><span data-stu-id="b202b-161">Regardless of the type of service, running a single instance results in downtime for that service if that single copy of the code fails for any reason.</span></span> 

<span data-ttu-id="b202b-162">In order to handle any single failure, the simplest thing you can do is to ensure that your services run on more than one node by default.</span><span class="sxs-lookup"><span data-stu-id="b202b-162">In order to handle any single failure, the simplest thing you can do is to ensure that your services run on more than one node by default.</span></span> <span data-ttu-id="b202b-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span><span class="sxs-lookup"><span data-stu-id="b202b-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="b202b-164">For stateful services, the minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span><span class="sxs-lookup"><span data-stu-id="b202b-164">For stateful services, the minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="b202b-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span><span class="sxs-lookup"><span data-stu-id="b202b-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="b202b-166">Handling coordinated failures</span><span class="sxs-lookup"><span data-stu-id="b202b-166">Handling coordinated failures</span></span>
<span data-ttu-id="b202b-167">Coordinated failures can happen in a cluster due to either planned or unplanned infrastructure failures and changes, or planned software changes.</span><span class="sxs-lookup"><span data-stu-id="b202b-167">Coordinated failures can happen in a cluster due to either planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="b202b-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span><span class="sxs-lookup"><span data-stu-id="b202b-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="b202b-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span><span class="sxs-lookup"><span data-stu-id="b202b-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="b202b-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span><span class="sxs-lookup"><span data-stu-id="b202b-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="b202b-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span><span class="sxs-lookup"><span data-stu-id="b202b-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="b202b-172">By default, Service Fabric tries to ensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span><span class="sxs-lookup"><span data-stu-id="b202b-172">By default, Service Fabric tries to ensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="b202b-173">For example, let's say that failure of a power source causes a rack of machines to fail simultaneously.</span><span class="sxs-lookup"><span data-stu-id="b202b-173">For example, let's say that failure of a power source causes a rack of machines to fail simultaneously.</span></span> <span data-ttu-id="b202b-174">With multiple copies of the service running the loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span><span class="sxs-lookup"><span data-stu-id="b202b-174">With multiple copies of the service running the loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="b202b-175">This is why managing fault domains is critical to ensuring high availability of your services.</span><span class="sxs-lookup"><span data-stu-id="b202b-175">This is why managing fault domains is critical to ensuring high availability of your services.</span></span> <span data-ttu-id="b202b-176">When running Service Fabric in Azure, fault domains are managed automatically.</span><span class="sxs-lookup"><span data-stu-id="b202b-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="b202b-177">In other environments, they may not be.</span><span class="sxs-lookup"><span data-stu-id="b202b-177">In other environments, they may not be.</span></span> <span data-ttu-id="b202b-178">If you're building your own clusters on premises, be sure to map and plan your fault domain layout correctly.</span><span class="sxs-lookup"><span data-stu-id="b202b-178">If you're building your own clusters on premises, be sure to map and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="b202b-179">Upgrade Domains are useful for modeling areas where software is going to be upgraded at the same time.</span><span class="sxs-lookup"><span data-stu-id="b202b-179">Upgrade Domains are useful for modeling areas where software is going to be upgraded at the same time.</span></span> <span data-ttu-id="b202b-180">Because of this, Upgrade Domains also often define the boundaries where software is taken down during planned upgrades.</span><span class="sxs-lookup"><span data-stu-id="b202b-180">Because of this, Upgrade Domains also often define the boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="b202b-181">Upgrades of both Service Fabric and your services follow the same model.</span><span class="sxs-lookup"><span data-stu-id="b202b-181">Upgrades of both Service Fabric and your services follow the same model.</span></span> <span data-ttu-id="b202b-182">For more on rolling upgrades, upgrade domains, and the Service Fabric health model that helps prevent unintended changes from impacting the cluster and your service, see these documents:</span><span class="sxs-lookup"><span data-stu-id="b202b-182">For more on rolling upgrades, upgrade domains, and the Service Fabric health model that helps prevent unintended changes from impacting the cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="b202b-183">Application Upgrade</span><span class="sxs-lookup"><span data-stu-id="b202b-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="b202b-184">Application Upgrade Tutorial</span><span class="sxs-lookup"><span data-stu-id="b202b-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="b202b-185">Service Fabric Health Model</span><span class="sxs-lookup"><span data-stu-id="b202b-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="b202b-186">You can visualize the layout of your cluster using the cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="b202b-186">You can visualize the layout of your cluster using the cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="b202b-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="b202b-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="b202b-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules to ensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of the features that Service Fabric provides in order to keep normal operational issues and failures from turning into disasters.</span><span class="sxs-lookup"><span data-stu-id="b202b-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules to ensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of the features that Service Fabric provides in order to keep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="b202b-189">Handling simultaneous hardware or software failures</span><span class="sxs-lookup"><span data-stu-id="b202b-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="b202b-190">Above we talked about single failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-190">Above we talked about single failures.</span></span> <span data-ttu-id="b202b-191">As you can see, are easy to handle for both stateless and stateful services just by keeping more copies of the code (and state) running across fault and upgrade domains.</span><span class="sxs-lookup"><span data-stu-id="b202b-191">As you can see, are easy to handle for both stateless and stateful services just by keeping more copies of the code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="b202b-192">Multiple simultaneous random failures can also happen.</span><span class="sxs-lookup"><span data-stu-id="b202b-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="b202b-193">These are more likely to lead to an actual disaster.</span><span class="sxs-lookup"><span data-stu-id="b202b-193">These are more likely to lead to an actual disaster.</span></span>


### <a name="random-failures-leading-to-service-failures"></a><span data-ttu-id="b202b-194">Random failures leading to service failures</span><span class="sxs-lookup"><span data-stu-id="b202b-194">Random failures leading to service failures</span></span>
<span data-ttu-id="b202b-195">Let's say that the service had an `InstanceCount` of 5, and several nodes running those instances all failed at the same time.</span><span class="sxs-lookup"><span data-stu-id="b202b-195">Let's say that the service had an `InstanceCount` of 5, and several nodes running those instances all failed at the same time.</span></span> <span data-ttu-id="b202b-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span><span class="sxs-lookup"><span data-stu-id="b202b-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="b202b-197">It will continue creating replacements until the service is back to its desired instance count.</span><span class="sxs-lookup"><span data-stu-id="b202b-197">It will continue creating replacements until the service is back to its desired instance count.</span></span> <span data-ttu-id="b202b-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in the cluster.</span><span class="sxs-lookup"><span data-stu-id="b202b-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in the cluster.</span></span> <span data-ttu-id="b202b-199">Let's say that some of those instances were to fail.</span><span class="sxs-lookup"><span data-stu-id="b202b-199">Let's say that some of those instances were to fail.</span></span> <span data-ttu-id="b202b-200">In this case, Service Fabric notices that the service is not in its desired state, and tries to create the instances on the nodes where they are missing.</span><span class="sxs-lookup"><span data-stu-id="b202b-200">In this case, Service Fabric notices that the service is not in its desired state, and tries to create the instances on the nodes where they are missing.</span></span> 

<span data-ttu-id="b202b-201">For stateful services the situation depends on whether the service has persisted state or not.</span><span class="sxs-lookup"><span data-stu-id="b202b-201">For stateful services the situation depends on whether the service has persisted state or not.</span></span> <span data-ttu-id="b202b-202">It also depends on how many replicas the service had and how many failed.</span><span class="sxs-lookup"><span data-stu-id="b202b-202">It also depends on how many replicas the service had and how many failed.</span></span> <span data-ttu-id="b202b-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span><span class="sxs-lookup"><span data-stu-id="b202b-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="b202b-204">Determining if there has been quorum loss or not</span><span class="sxs-lookup"><span data-stu-id="b202b-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="b202b-205">A quorum loss is any time a majority of the replicas of a stateful service are down at the same time, including the Primary.</span><span class="sxs-lookup"><span data-stu-id="b202b-205">A quorum loss is any time a majority of the replicas of a stateful service are down at the same time, including the Primary.</span></span>
2. <span data-ttu-id="b202b-206">Determining if the quorum loss is permanent or not</span><span class="sxs-lookup"><span data-stu-id="b202b-206">Determining if the quorum loss is permanent or not</span></span>
 - <span data-ttu-id="b202b-207">Most of the time, failures are transient.</span><span class="sxs-lookup"><span data-stu-id="b202b-207">Most of the time, failures are transient.</span></span> <span data-ttu-id="b202b-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span><span class="sxs-lookup"><span data-stu-id="b202b-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="b202b-209">Sometimes though, failures are permanent.</span><span class="sxs-lookup"><span data-stu-id="b202b-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="b202b-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span><span class="sxs-lookup"><span data-stu-id="b202b-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="b202b-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds to step 3 by declaring (potential) data loss.</span><span class="sxs-lookup"><span data-stu-id="b202b-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds to step 3 by declaring (potential) data loss.</span></span> <span data-ttu-id="b202b-212">Proceeding to data loss makes sense because Service Fabric knows that there's no point in waiting for the replicas to come back, because even if they were recovered they would be empty.</span><span class="sxs-lookup"><span data-stu-id="b202b-212">Proceeding to data loss makes sense because Service Fabric knows that there's no point in waiting for the replicas to come back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="b202b-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric to start waiting for the replicas to come back and restore quorum.</span><span class="sxs-lookup"><span data-stu-id="b202b-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric to start waiting for the replicas to come back and restore quorum.</span></span> <span data-ttu-id="b202b-214">This results in a service outage for any _writes_ to the affected partition (or "replica set") of the service.</span><span class="sxs-lookup"><span data-stu-id="b202b-214">This results in a service outage for any _writes_ to the affected partition (or "replica set") of the service.</span></span> <span data-ttu-id="b202b-215">However, reads may still be possible with reduced consistency guarantees.</span><span class="sxs-lookup"><span data-stu-id="b202b-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="b202b-216">The default amount of time that Service Fabric waits for quorum to be restored is infinite, since proceeding is a (potential) data loss event and carries other risks.</span><span class="sxs-lookup"><span data-stu-id="b202b-216">The default amount of time that Service Fabric waits for quorum to be restored is infinite, since proceeding is a (potential) data loss event and carries other risks.</span></span> <span data-ttu-id="b202b-217">Overriding the default `QuorumLossWaitDuration` value is possible but is not recommended.</span><span class="sxs-lookup"><span data-stu-id="b202b-217">Overriding the default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="b202b-218">Instead at this time, all efforts should be made to restore the down replicas.</span><span class="sxs-lookup"><span data-stu-id="b202b-218">Instead at this time, all efforts should be made to restore the down replicas.</span></span> <span data-ttu-id="b202b-219">This requires bringing the nodes that are down back up, and ensuring that they can remount the drives where they stored the local persistent state.</span><span class="sxs-lookup"><span data-stu-id="b202b-219">This requires bringing the nodes that are down back up, and ensuring that they can remount the drives where they stored the local persistent state.</span></span> <span data-ttu-id="b202b-220">If the quorum loss is caused by process failure, Service Fabric automatically tries to recreate the processes and restart the replicas inside them.</span><span class="sxs-lookup"><span data-stu-id="b202b-220">If the quorum loss is caused by process failure, Service Fabric automatically tries to recreate the processes and restart the replicas inside them.</span></span> <span data-ttu-id="b202b-221">If this fails, Service Fabric reports health errors.</span><span class="sxs-lookup"><span data-stu-id="b202b-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="b202b-222">If these can be resolved then the replicas usually come back.</span><span class="sxs-lookup"><span data-stu-id="b202b-222">If these can be resolved then the replicas usually come back.</span></span> <span data-ttu-id="b202b-223">Sometimes, though, the replicas can't be brought back.</span><span class="sxs-lookup"><span data-stu-id="b202b-223">Sometimes, though, the replicas can't be brought back.</span></span> <span data-ttu-id="b202b-224">For example, the drives may all have failed, or the machines physically destroyed somehow.</span><span class="sxs-lookup"><span data-stu-id="b202b-224">For example, the drives may all have failed, or the machines physically destroyed somehow.</span></span> <span data-ttu-id="b202b-225">In these cases, we now have a permanent quorum loss event.</span><span class="sxs-lookup"><span data-stu-id="b202b-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="b202b-226">To tell Service Fabric to stop waiting for the down replicas to come back, a cluster administrator must determine which partitions of which services are affected and call the `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span><span class="sxs-lookup"><span data-stu-id="b202b-226">To tell Service Fabric to stop waiting for the down replicas to come back, a cluster administrator must determine which partitions of which services are affected and call the `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="b202b-227">This API allows specifying the ID of the partition to move out of QuorumLoss and into potential dataloss.</span><span class="sxs-lookup"><span data-stu-id="b202b-227">This API allows specifying the ID of the partition to move out of QuorumLoss and into potential dataloss.</span></span>

  > [!NOTE]
  > <span data-ttu-id="b202b-228">It is _never_ safe to use this API other than in a targeted way against specific partitions.</span><span class="sxs-lookup"><span data-stu-id="b202b-228">It is _never_ safe to use this API other than in a targeted way against specific partitions.</span></span> 
  >

3. <span data-ttu-id="b202b-229">Determining if there has been actual data loss, and restoring from backups</span><span class="sxs-lookup"><span data-stu-id="b202b-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="b202b-230">When Service Fabric calls the `OnDataLossAsync` method, it is always because of _suspected_ data loss.</span><span class="sxs-lookup"><span data-stu-id="b202b-230">When Service Fabric calls the `OnDataLossAsync` method, it is always because of _suspected_ data loss.</span></span> <span data-ttu-id="b202b-231">Service Fabric ensures that this call is delivered to the _best_ remaining replica.</span><span class="sxs-lookup"><span data-stu-id="b202b-231">Service Fabric ensures that this call is delivered to the _best_ remaining replica.</span></span> <span data-ttu-id="b202b-232">This is whichever replica has made the most progress.</span><span class="sxs-lookup"><span data-stu-id="b202b-232">This is whichever replica has made the most progress.</span></span> <span data-ttu-id="b202b-233">The reason we always say _suspected_ data loss is that it is possible that the remaining replica actually has all same state as the Primary did when it went down.</span><span class="sxs-lookup"><span data-stu-id="b202b-233">The reason we always say _suspected_ data loss is that it is possible that the remaining replica actually has all same state as the Primary did when it went down.</span></span> <span data-ttu-id="b202b-234">However, without that state to compare it to, there's no good way for Service Fabric or operators to know for sure.</span><span class="sxs-lookup"><span data-stu-id="b202b-234">However, without that state to compare it to, there's no good way for Service Fabric or operators to know for sure.</span></span> <span data-ttu-id="b202b-235">At this point, Service Fabric also knows the other replicas are not coming back.</span><span class="sxs-lookup"><span data-stu-id="b202b-235">At this point, Service Fabric also knows the other replicas are not coming back.</span></span> <span data-ttu-id="b202b-236">That was the decision made when we stopped waiting for the quorum loss to resolve itself.</span><span class="sxs-lookup"><span data-stu-id="b202b-236">That was the decision made when we stopped waiting for the quorum loss to resolve itself.</span></span> <span data-ttu-id="b202b-237">The best course of action for the service is usually to freeze and wait for specific administrative intervention.</span><span class="sxs-lookup"><span data-stu-id="b202b-237">The best course of action for the service is usually to freeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="b202b-238">So what does a typical implementation of the `OnDataLossAsync` method do?</span><span class="sxs-lookup"><span data-stu-id="b202b-238">So what does a typical implementation of the `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="b202b-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span><span class="sxs-lookup"><span data-stu-id="b202b-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="b202b-240">Usually at this point, to pause and wait for further decisions and manual actions to be taken.</span><span class="sxs-lookup"><span data-stu-id="b202b-240">Usually at this point, to pause and wait for further decisions and manual actions to be taken.</span></span> <span data-ttu-id="b202b-241">This is because even if backups are available they may need to be prepared.</span><span class="sxs-lookup"><span data-stu-id="b202b-241">This is because even if backups are available they may need to be prepared.</span></span> <span data-ttu-id="b202b-242">For example, if two different services coordinate information, those backups may need to be modified in order to ensure that once the restore happens that the information those two services care about is consistent.</span><span class="sxs-lookup"><span data-stu-id="b202b-242">For example, if two different services coordinate information, those backups may need to be modified in order to ensure that once the restore happens that the information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="b202b-243">Often there is also some other telemetry or exhaust from the service.</span><span class="sxs-lookup"><span data-stu-id="b202b-243">Often there is also some other telemetry or exhaust from the service.</span></span> <span data-ttu-id="b202b-244">This metadata may be contained in other services or in logs.</span><span class="sxs-lookup"><span data-stu-id="b202b-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="b202b-245">This information can be used needed to determine if there were any calls received and processed at the primary that were not present in the backup or replicated to this particular replica.</span><span class="sxs-lookup"><span data-stu-id="b202b-245">This information can be used needed to determine if there were any calls received and processed at the primary that were not present in the backup or replicated to this particular replica.</span></span> <span data-ttu-id="b202b-246">These may need to be replayed or added to the backup before restoration is feasible.</span><span class="sxs-lookup"><span data-stu-id="b202b-246">These may need to be replayed or added to the backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="b202b-247">Comparisons of the remaining replica's state to that contained in any backups that are available.</span><span class="sxs-lookup"><span data-stu-id="b202b-247">Comparisons of the remaining replica's state to that contained in any backups that are available.</span></span> <span data-ttu-id="b202b-248">If using the Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span><span class="sxs-lookup"><span data-stu-id="b202b-248">If using the Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="b202b-249">The goal is to see if the state within the replica is sufficient, or also what the backup may be missing.</span><span class="sxs-lookup"><span data-stu-id="b202b-249">The goal is to see if the state within the replica is sufficient, or also what the backup may be missing.</span></span>
  - <span data-ttu-id="b202b-250">Once the comparison is done, and if necessary the restore completed, the service code should return true if any state changes were made.</span><span class="sxs-lookup"><span data-stu-id="b202b-250">Once the comparison is done, and if necessary the restore completed, the service code should return true if any state changes were made.</span></span> <span data-ttu-id="b202b-251">If the replica determined that it was the best available copy of the state and made no changes, then return false.</span><span class="sxs-lookup"><span data-stu-id="b202b-251">If the replica determined that it was the best available copy of the state and made no changes, then return false.</span></span> <span data-ttu-id="b202b-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span><span class="sxs-lookup"><span data-stu-id="b202b-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="b202b-253">They will be dropped and rebuilt from this replica.</span><span class="sxs-lookup"><span data-stu-id="b202b-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="b202b-254">False indicates that no state changes were made, so the other replicas can keep what they have.</span><span class="sxs-lookup"><span data-stu-id="b202b-254">False indicates that no state changes were made, so the other replicas can keep what they have.</span></span> 

<span data-ttu-id="b202b-255">It is critically important that service authors practice potential data loss and failure scenarios before services are ever deployed in production.</span><span class="sxs-lookup"><span data-stu-id="b202b-255">It is critically important that service authors practice potential data loss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="b202b-256">To protect against the possibility of data loss, it is important to periodically [back up the state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services to a geo-redundant store.</span><span class="sxs-lookup"><span data-stu-id="b202b-256">To protect against the possibility of data loss, it is important to periodically [back up the state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services to a geo-redundant store.</span></span> <span data-ttu-id="b202b-257">You must also ensure that you have the ability to restore it.</span><span class="sxs-lookup"><span data-stu-id="b202b-257">You must also ensure that you have the ability to restore it.</span></span> <span data-ttu-id="b202b-258">Since backups of many different services are taken at different times, you need to ensure that after a restore your services have a consistent view of each other.</span><span class="sxs-lookup"><span data-stu-id="b202b-258">Since backups of many different services are taken at different times, you need to ensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="b202b-259">For example, consider a situation where one service generates a number and stores it, then sends it to another service that also stores it.</span><span class="sxs-lookup"><span data-stu-id="b202b-259">For example, consider a situation where one service generates a number and stores it, then sends it to another service that also stores it.</span></span> <span data-ttu-id="b202b-260">After a restore, you might discover that the second service has the number but the first does not, because it's backup didn't include that operation.</span><span class="sxs-lookup"><span data-stu-id="b202b-260">After a restore, you might discover that the second service has the number but the first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="b202b-261">If you find out that the remaining replicas are insufficient to continue from in a data loss scenario, and you can't reconstruct service state from telemetry or exhaust, the frequency of your backups determines your best possible recovery point objective (RPO).</span><span class="sxs-lookup"><span data-stu-id="b202b-261">If you find out that the remaining replicas are insufficient to continue from in a data loss scenario, and you can't reconstruct service state from telemetry or exhaust, the frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="b202b-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and data loss requiring restoration from a backup.</span><span class="sxs-lookup"><span data-stu-id="b202b-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and data loss requiring restoration from a backup.</span></span> <span data-ttu-id="b202b-263">These scenarios are included as a part of Service Fabric's testability tools, managed by the Fault Analysis Service.</span><span class="sxs-lookup"><span data-stu-id="b202b-263">These scenarios are included as a part of Service Fabric's testability tools, managed by the Fault Analysis Service.</span></span> <span data-ttu-id="b202b-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span><span class="sxs-lookup"><span data-stu-id="b202b-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="b202b-265">System services can also suffer quorum loss, with the impact being specific to the service in question.</span><span class="sxs-lookup"><span data-stu-id="b202b-265">System services can also suffer quorum loss, with the impact being specific to the service in question.</span></span> <span data-ttu-id="b202b-266">For instance, quorum loss in the naming service impacts name resolution, whereas quorum loss in the failover manager service blocks new service creation and failovers.</span><span class="sxs-lookup"><span data-stu-id="b202b-266">For instance, quorum loss in the naming service impacts name resolution, whereas quorum loss in the failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="b202b-267">While the Service Fabric system services follow the same pattern as your services for state management, it is not recommended that you should attempt to move them out of Quorum Loss and into potential data loss.</span><span class="sxs-lookup"><span data-stu-id="b202b-267">While the Service Fabric system services follow the same pattern as your services for state management, it is not recommended that you should attempt to move them out of Quorum Loss and into potential data loss.</span></span> <span data-ttu-id="b202b-268">The recommendation is instead to [seek support](service-fabric-support.md) to determine a solution that is targeted to your specific situation.</span><span class="sxs-lookup"><span data-stu-id="b202b-268">The recommendation is instead to [seek support](service-fabric-support.md) to determine a solution that is targeted to your specific situation.</span></span>  <span data-ttu-id="b202b-269">Usually it is preferable to simply wait until the down replicas return.</span><span class="sxs-lookup"><span data-stu-id="b202b-269">Usually it is preferable to simply wait until the down replicas return.</span></span>
>

## <a name="availability-of-the-service-fabric-cluster"></a><span data-ttu-id="b202b-270">Availability of the Service Fabric cluster</span><span class="sxs-lookup"><span data-stu-id="b202b-270">Availability of the Service Fabric cluster</span></span>
<span data-ttu-id="b202b-271">Generally speaking, the Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span><span class="sxs-lookup"><span data-stu-id="b202b-271">Generally speaking, the Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="b202b-272">A failure of any one node will not cause availability or reliability issues for the cluster, primarily because the Service Fabric system services follow the same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span><span class="sxs-lookup"><span data-stu-id="b202b-272">A failure of any one node will not cause availability or reliability issues for the cluster, primarily because the Service Fabric system services follow the same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="b202b-273">The underlying Service Fabric networking and failure detection layers are fully distributed.</span><span class="sxs-lookup"><span data-stu-id="b202b-273">The underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="b202b-274">Most system services can be rebuilt from metadata in the cluster, or know how to resynchronize their state from other places.</span><span class="sxs-lookup"><span data-stu-id="b202b-274">Most system services can be rebuilt from metadata in the cluster, or know how to resynchronize their state from other places.</span></span> <span data-ttu-id="b202b-275">The availability of the cluster can become compromised if system services get into quorum loss situations like those described above.</span><span class="sxs-lookup"><span data-stu-id="b202b-275">The availability of the cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="b202b-276">In these cases you may not be able to perform certain operations on the cluster like starting an upgrade or deploying new services, but the cluster itself is still up.</span><span class="sxs-lookup"><span data-stu-id="b202b-276">In these cases you may not be able to perform certain operations on the cluster like starting an upgrade or deploying new services, but the cluster itself is still up.</span></span> <span data-ttu-id="b202b-277">Services on already running will remain running in these conditions unless they require writes to the system services to continue functioning.</span><span class="sxs-lookup"><span data-stu-id="b202b-277">Services on already running will remain running in these conditions unless they require writes to the system services to continue functioning.</span></span> <span data-ttu-id="b202b-278">For example, if the Failover Manager is in quorum loss all services will continue to run, but any services that fail will not be able to automatically restart, since this requires the involvement of the Failover Manager.</span><span class="sxs-lookup"><span data-stu-id="b202b-278">For example, if the Failover Manager is in quorum loss all services will continue to run, but any services that fail will not be able to automatically restart, since this requires the involvement of the Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="b202b-279">Failures of a datacenter or Azure region</span><span class="sxs-lookup"><span data-stu-id="b202b-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="b202b-280">In rare cases, a physical data center can become temporarily unavailable due to loss of power or network connectivity.</span><span class="sxs-lookup"><span data-stu-id="b202b-280">In rare cases, a physical data center can become temporarily unavailable due to loss of power or network connectivity.</span></span> <span data-ttu-id="b202b-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span><span class="sxs-lookup"><span data-stu-id="b202b-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="b202b-282">However, _your data is preserved_.</span><span class="sxs-lookup"><span data-stu-id="b202b-282">However, _your data is preserved_.</span></span> <span data-ttu-id="b202b-283">For clusters running in Azure, you can view updates on outages on the [Azure status page][azure-status-dashboard].</span><span class="sxs-lookup"><span data-stu-id="b202b-283">For clusters running in Azure, you can view updates on outages on the [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="b202b-284">In the highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or the services inside them could be lost.</span><span class="sxs-lookup"><span data-stu-id="b202b-284">In the highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or the services inside them could be lost.</span></span> <span data-ttu-id="b202b-285">This includes any state not backed up outside of that datacenter or region.</span><span class="sxs-lookup"><span data-stu-id="b202b-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="b202b-286">There's two different strategies for surviving the permanent or sustained failure of a single datacenter or region.</span><span class="sxs-lookup"><span data-stu-id="b202b-286">There's two different strategies for surviving the permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="b202b-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span><span class="sxs-lookup"><span data-stu-id="b202b-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="b202b-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span><span class="sxs-lookup"><span data-stu-id="b202b-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="b202b-289">This also requires coordination of backups from the services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span><span class="sxs-lookup"><span data-stu-id="b202b-289">This also requires coordination of backups from the services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="b202b-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span><span class="sxs-lookup"><span data-stu-id="b202b-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="b202b-291">The minimum supported configuration for this is three datacenters or regions.</span><span class="sxs-lookup"><span data-stu-id="b202b-291">The minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="b202b-292">The recommended number of regions or datacenters is five.</span><span class="sxs-lookup"><span data-stu-id="b202b-292">The recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="b202b-293">This requires a more complex cluster topology.</span><span class="sxs-lookup"><span data-stu-id="b202b-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="b202b-294">However, the benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span><span class="sxs-lookup"><span data-stu-id="b202b-294">However, the benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="b202b-295">These failures can be handled by the mechanisms that work for clusters within a single region.</span><span class="sxs-lookup"><span data-stu-id="b202b-295">These failures can be handled by the mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="b202b-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="b202b-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="b202b-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-to-cluster-failures"></a><span data-ttu-id="b202b-298">Random failures leading to cluster failures</span><span class="sxs-lookup"><span data-stu-id="b202b-298">Random failures leading to cluster failures</span></span>
<span data-ttu-id="b202b-299">Service Fabric has the concept of Seed Nodes.</span><span class="sxs-lookup"><span data-stu-id="b202b-299">Service Fabric has the concept of Seed Nodes.</span></span> <span data-ttu-id="b202b-300">These are nodes that maintain the availability of the underlying cluster.</span><span class="sxs-lookup"><span data-stu-id="b202b-300">These are nodes that maintain the availability of the underlying cluster.</span></span> <span data-ttu-id="b202b-301">These nodes help to ensure the cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span><span class="sxs-lookup"><span data-stu-id="b202b-301">These nodes help to ensure the cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="b202b-302">If random failures remove a majority of the seed nodes in the cluster and they are not brought back, the cluster automatically shuts down.</span><span class="sxs-lookup"><span data-stu-id="b202b-302">If random failures remove a majority of the seed nodes in the cluster and they are not brought back, the cluster automatically shuts down.</span></span> <span data-ttu-id="b202b-303">In Azure, Seed Nodes are automatically managed: they are distributed over the available fault and upgrade domains, and if a single seed node is removed from the cluster another one will be created in its place.</span><span class="sxs-lookup"><span data-stu-id="b202b-303">In Azure, Seed Nodes are automatically managed: they are distributed over the available fault and upgrade domains, and if a single seed node is removed from the cluster another one will be created in its place.</span></span> 

<span data-ttu-id="b202b-304">In both standalone Service Fabric clusters and Azure, the "Primary Node Type" is the one that runs the seeds.</span><span class="sxs-lookup"><span data-stu-id="b202b-304">In both standalone Service Fabric clusters and Azure, the "Primary Node Type" is the one that runs the seeds.</span></span> <span data-ttu-id="b202b-305">When defining a primary node type, Service Fabric will automatically take advantage of the number of nodes provided by creating up to 9 seed nodes and 9 replicas of each of the system services.</span><span class="sxs-lookup"><span data-stu-id="b202b-305">When defining a primary node type, Service Fabric will automatically take advantage of the number of nodes provided by creating up to 9 seed nodes and 9 replicas of each of the system services.</span></span> <span data-ttu-id="b202b-306">If a set of random failures takes out a majority of those system service replicas simultaneously, the system services will enter quorum loss, as we described above.</span><span class="sxs-lookup"><span data-stu-id="b202b-306">If a set of random failures takes out a majority of those system service replicas simultaneously, the system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="b202b-307">If a majority of the seed nodes are lost, the cluster will shut down soon after.</span><span class="sxs-lookup"><span data-stu-id="b202b-307">If a majority of the seed nodes are lost, the cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="b202b-308">Next steps</span><span class="sxs-lookup"><span data-stu-id="b202b-308">Next steps</span></span>
- <span data-ttu-id="b202b-309">Learn how to simulate various failures using the [testability framework](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="b202b-309">Learn how to simulate various failures using the [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="b202b-310">Read other disaster-recovery and high-availability resources.</span><span class="sxs-lookup"><span data-stu-id="b202b-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="b202b-311">Microsoft has published a large amount of guidance on these topics.</span><span class="sxs-lookup"><span data-stu-id="b202b-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="b202b-312">While some of these documents refer to specific techniques for use in other products, they contain many general best practices you can apply in the Service Fabric context as well:</span><span class="sxs-lookup"><span data-stu-id="b202b-312">While some of these documents refer to specific techniques for use in other products, they contain many general best practices you can apply in the Service Fabric context as well:</span></span>
  - [<span data-ttu-id="b202b-313">Availability checklist</span><span class="sxs-lookup"><span data-stu-id="b202b-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="b202b-314">Performing a disaster recovery drill</span><span class="sxs-lookup"><span data-stu-id="b202b-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="b202b-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="b202b-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="b202b-316">Learn about [Service Fabric support options](service-fabric-support.md)</span><span class="sxs-lookup"><span data-stu-id="b202b-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
