---
title: Emotion API overview | Microsoft Docs
description: Use the Microsoft cutting-edge, cloud-based emotion recognition algorithm to build more personalized apps, with the Emotion API in Cognitive Services.
services: cognitive-services
author: v-royhar
manager: yutkuo
ms.service: cognitive-services
ms.technology: emotion
ms.topic: article
ms.date: 02/06/2017
ms.author: anroth
ms.openlocfilehash: 2f756ecd61fa7092645dab46c1db14bc3e323080
ms.sourcegitcommit: 5b9d839c0c0a94b293fdafe1d6e5429506c07e05
ms.translationtype: HT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/02/2018
ms.locfileid: "44549454"
---
# <a name="emotion-api"></a><span data-ttu-id="fd777-103">Emotion API</span><span class="sxs-lookup"><span data-stu-id="fd777-103">Emotion API</span></span>

<span data-ttu-id="fd777-104">Welcome to the Microsoft Emotion API, which allows you to build more personalized apps with Microsoft’s cutting edge cloud-based emotion recognition algorithm.</span><span class="sxs-lookup"><span data-stu-id="fd777-104">Welcome to the Microsoft Emotion API, which allows you to build more personalized apps with Microsoft’s cutting edge cloud-based emotion recognition algorithm.</span></span>

### <a name="emotion-recognition"></a><span data-ttu-id="fd777-105">Emotion Recognition</span><span class="sxs-lookup"><span data-stu-id="fd777-105">Emotion Recognition</span></span>

<span data-ttu-id="fd777-106">The Emotion API beta takes an image as an input, and returns the confidence across a set of emotions for each face in the image, as well as bounding box for the face, from the Face API.</span><span class="sxs-lookup"><span data-stu-id="fd777-106">The Emotion API beta takes an image as an input, and returns the confidence across a set of emotions for each face in the image, as well as bounding box for the face, from the Face API.</span></span> <span data-ttu-id="fd777-107">The emotions detected are happiness, sadness, surprise, anger, fear, contempt, disgust or neutral.</span><span class="sxs-lookup"><span data-stu-id="fd777-107">The emotions detected are happiness, sadness, surprise, anger, fear, contempt, disgust or neutral.</span></span> <span data-ttu-id="fd777-108">These emotions are communicated cross-culturally and universally via the same basic facial expressions, where are identified by Emotion API.</span><span class="sxs-lookup"><span data-stu-id="fd777-108">These emotions are communicated cross-culturally and universally via the same basic facial expressions, where are identified by Emotion API.</span></span> 

<span data-ttu-id="fd777-109">**Interpreting Results:**</span><span class="sxs-lookup"><span data-stu-id="fd777-109">**Interpreting Results:**</span></span> 

<span data-ttu-id="fd777-110">In interpreting results from the Emotion API, the emotion detected should be interpreted as the emotion with the highest score, as scores are normalized to sum to one.</span><span class="sxs-lookup"><span data-stu-id="fd777-110">In interpreting results from the Emotion API, the emotion detected should be interpreted as the emotion with the highest score, as scores are normalized to sum to one.</span></span> <span data-ttu-id="fd777-111">Users may choose to set a higher confidence threshold within their application, depending on their needs.</span><span class="sxs-lookup"><span data-stu-id="fd777-111">Users may choose to set a higher confidence threshold within their application, depending on their needs.</span></span> 

<span data-ttu-id="fd777-112">For more details about emotion detection, please refer to the API Reference:</span><span class="sxs-lookup"><span data-stu-id="fd777-112">For more details about emotion detection, please refer to the API Reference:</span></span> 
  * <span data-ttu-id="fd777-113">Basic: If a user has already called the Face API, they can submit the face rectangle as an input and use the basic tier.</span><span class="sxs-lookup"><span data-stu-id="fd777-113">Basic: If a user has already called the Face API, they can submit the face rectangle as an input and use the basic tier.</span></span> [<span data-ttu-id="fd777-114">API Reference</span><span class="sxs-lookup"><span data-stu-id="fd777-114">API Reference</span></span>](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/56f23eb019845524ec61c4d7)
  * <span data-ttu-id="fd777-115">Standard: If a user does not submit a face rectangle, they should use standard mode.</span><span class="sxs-lookup"><span data-stu-id="fd777-115">Standard: If a user does not submit a face rectangle, they should use standard mode.</span></span>  [<span data-ttu-id="fd777-116">API Reference</span><span class="sxs-lookup"><span data-stu-id="fd777-116">API Reference</span></span>](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89/operations/563b31ea778daf121cc3a5fa)

### <a name="emotion-in-video"></a><span data-ttu-id="fd777-117">Emotion in Video</span><span class="sxs-lookup"><span data-stu-id="fd777-117">Emotion in Video</span></span>

<span data-ttu-id="fd777-118">The Emotion API for Video takes a video as an input, and returns the confidence across a set of emotions for the group of faces in the image over a period of time.</span><span class="sxs-lookup"><span data-stu-id="fd777-118">The Emotion API for Video takes a video as an input, and returns the confidence across a set of emotions for the group of faces in the image over a period of time.</span></span> <span data-ttu-id="fd777-119">The emotions detected are happiness, sadness, surprise, anger, fear, contempt, disgust or neutral.</span><span class="sxs-lookup"><span data-stu-id="fd777-119">The emotions detected are happiness, sadness, surprise, anger, fear, contempt, disgust or neutral.</span></span> <span data-ttu-id="fd777-120">These emotions are communicated cross-culturally and universally via the same basic facial expressions, where are identified by Emotion API.</span><span class="sxs-lookup"><span data-stu-id="fd777-120">These emotions are communicated cross-culturally and universally via the same basic facial expressions, where are identified by Emotion API.</span></span>

<span data-ttu-id="fd777-121">**Interpreting Results:**</span><span class="sxs-lookup"><span data-stu-id="fd777-121">**Interpreting Results:**</span></span>

<span data-ttu-id="fd777-122">Emotion API for Video provides two types of aggregate results for the emotions of faces in a frame.</span><span class="sxs-lookup"><span data-stu-id="fd777-122">Emotion API for Video provides two types of aggregate results for the emotions of faces in a frame.</span></span> <span data-ttu-id="fd777-123">The API first calculates emotion scores for each face in a video, smoothing the results over time for higher accuracy.</span><span class="sxs-lookup"><span data-stu-id="fd777-123">The API first calculates emotion scores for each face in a video, smoothing the results over time for higher accuracy.</span></span> <span data-ttu-id="fd777-124">It returns two types of aggregates: *windowMeanScores* gives a mean score for all of the faces detected in a frame for each emotion.</span><span class="sxs-lookup"><span data-stu-id="fd777-124">It returns two types of aggregates: *windowMeanScores* gives a mean score for all of the faces detected in a frame for each emotion.</span></span> <span data-ttu-id="fd777-125">The emotion detected should be interpreted as the emotion with the highest score, as scores are normalized to sum to one.</span><span class="sxs-lookup"><span data-stu-id="fd777-125">The emotion detected should be interpreted as the emotion with the highest score, as scores are normalized to sum to one.</span></span> <span data-ttu-id="fd777-126">Users may choose to set a higher confidence threshold within their application, depending on their needs.</span><span class="sxs-lookup"><span data-stu-id="fd777-126">Users may choose to set a higher confidence threshold within their application, depending on their needs.</span></span> <span data-ttu-id="fd777-127">*windowFaceDistribution* gives the distribution of faces with each emotion as the dominant emotion for that face.</span><span class="sxs-lookup"><span data-stu-id="fd777-127">*windowFaceDistribution* gives the distribution of faces with each emotion as the dominant emotion for that face.</span></span> <span data-ttu-id="fd777-128">Dominant emotions for each face have been determined based on the emotion with the highest score for that face.</span><span class="sxs-lookup"><span data-stu-id="fd777-128">Dominant emotions for each face have been determined based on the emotion with the highest score for that face.</span></span> 

<span data-ttu-id="fd777-129">Because emotions are smoothed over time, if you ever build a visualization to overlay your results on top of the original video, subtract 250 milliseconds from the provided timestamps.</span><span class="sxs-lookup"><span data-stu-id="fd777-129">Because emotions are smoothed over time, if you ever build a visualization to overlay your results on top of the original video, subtract 250 milliseconds from the provided timestamps.</span></span> 

<span data-ttu-id="fd777-130">For more detail on how to parse the format of Emotion for Video API results, you may also want to view  <link to Glossary from Video API> For more details about emotion detection in video, please refer to the [API Reference](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89).</span><span class="sxs-lookup"><span data-stu-id="fd777-130">For more detail on how to parse the format of Emotion for Video API results, you may also want to view  <link to Glossary from Video API> For more details about emotion detection in video, please refer to the [API Reference](https://westus.dev.cognitive.microsoft.com/docs/services/5639d931ca73072154c1ce89).</span></span>

