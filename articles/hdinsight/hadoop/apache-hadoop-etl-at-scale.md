---
title: Extract, transform, and load (ETL) at Scale - Azure HDInsight
description: Learn how ETL is used in HDInsight with Hadoop.
services: hdinsight
author: ashishthaps
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: conceptual
ms.date: 11/14/2017
ms.author: ashishth
ms.openlocfilehash: 6af55bf8311efc14687ee16c3226988fd730081d
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44870616"
---
# <a name="extract-transform-and-load-etl-at-scale"></a><span data-ttu-id="251b6-103">Extract, transform, and load (ETL) at scale</span><span class="sxs-lookup"><span data-stu-id="251b6-103">Extract, transform, and load (ETL) at scale</span></span>

<span data-ttu-id="251b6-104">Extract, transform, and load (ETL) is the process by which data is acquired from various sources, collected in a standard location, cleaned and processed, and ultimately loaded into a datastore from which it can be queried.</span><span class="sxs-lookup"><span data-stu-id="251b6-104">Extract, transform, and load (ETL) is the process by which data is acquired from various sources, collected in a standard location, cleaned and processed, and ultimately loaded into a datastore from which it can be queried.</span></span> <span data-ttu-id="251b6-105">Legacy ETL processes import data, clean it in place, and then store it in a relational data engine.</span><span class="sxs-lookup"><span data-stu-id="251b6-105">Legacy ETL processes import data, clean it in place, and then store it in a relational data engine.</span></span> <span data-ttu-id="251b6-106">With HDInsight, a wide variety of Hadoop ecosystem components support performing ETL at scale.</span><span class="sxs-lookup"><span data-stu-id="251b6-106">With HDInsight, a wide variety of Hadoop ecosystem components support performing ETL at scale.</span></span> 

<span data-ttu-id="251b6-107">The use of HDInsight in the ETL process can be summarized by this pipeline:</span><span class="sxs-lookup"><span data-stu-id="251b6-107">The use of HDInsight in the ETL process can be summarized by this pipeline:</span></span>

![HDInsight ETL Overview](./media/apache-hadoop-etl-at-scale/hdinsight-etl-at-scale-overview.png)

<span data-ttu-id="251b6-109">The following sections explore each of the ETL phases and their associated components.</span><span class="sxs-lookup"><span data-stu-id="251b6-109">The following sections explore each of the ETL phases and their associated components.</span></span>

## <a name="orchestration"></a><span data-ttu-id="251b6-110">Orchestration</span><span class="sxs-lookup"><span data-stu-id="251b6-110">Orchestration</span></span>

<span data-ttu-id="251b6-111">Orchestration spans across all phases of the ETL pipeline.</span><span class="sxs-lookup"><span data-stu-id="251b6-111">Orchestration spans across all phases of the ETL pipeline.</span></span> <span data-ttu-id="251b6-112">ETL jobs in HDInsight often involve several different products working in conjunction with each other.</span><span class="sxs-lookup"><span data-stu-id="251b6-112">ETL jobs in HDInsight often involve several different products working in conjunction with each other.</span></span>  <span data-ttu-id="251b6-113">You might use Hive to clean some portion of the data, while Pig cleans another portion.</span><span class="sxs-lookup"><span data-stu-id="251b6-113">You might use Hive to clean some portion of the data, while Pig cleans another portion.</span></span>  <span data-ttu-id="251b6-114">You might use Azure Data Factory to load data into Azure SQL Database from Azure Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="251b6-114">You might use Azure Data Factory to load data into Azure SQL Database from Azure Data Lake Store.</span></span>

<span data-ttu-id="251b6-115">Orchestration is needed to run the appropriate job at the appropriate time.</span><span class="sxs-lookup"><span data-stu-id="251b6-115">Orchestration is needed to run the appropriate job at the appropriate time.</span></span>

### <a name="oozie"></a><span data-ttu-id="251b6-116">Oozie</span><span class="sxs-lookup"><span data-stu-id="251b6-116">Oozie</span></span>

<span data-ttu-id="251b6-117">Apache Oozie is a workflow coordination system that manages Hadoop jobs.</span><span class="sxs-lookup"><span data-stu-id="251b6-117">Apache Oozie is a workflow coordination system that manages Hadoop jobs.</span></span> <span data-ttu-id="251b6-118">Oozie runs within an HDInsight cluster and is integrated with the Hadoop stack.</span><span class="sxs-lookup"><span data-stu-id="251b6-118">Oozie runs within an HDInsight cluster and is integrated with the Hadoop stack.</span></span> <span data-ttu-id="251b6-119">Oozie supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop.</span><span class="sxs-lookup"><span data-stu-id="251b6-119">Oozie supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop.</span></span> <span data-ttu-id="251b6-120">Oozie can also be used to schedule jobs that are specific to a system, such as Java programs or shell scripts.</span><span class="sxs-lookup"><span data-stu-id="251b6-120">Oozie can also be used to schedule jobs that are specific to a system, such as Java programs or shell scripts.</span></span>

<span data-ttu-id="251b6-121">For more information, see [Use Oozie with Hadoop to define and run a workflow on HDInsight](../hdinsight-use-oozie-linux-mac.md) For a deep dive showing how to use Oozie to drive an end-to-end pipeline, see [Operationalize the Data Pipeline](../hdinsight-operationalize-data-pipeline.md).</span><span class="sxs-lookup"><span data-stu-id="251b6-121">For more information, see [Use Oozie with Hadoop to define and run a workflow on HDInsight](../hdinsight-use-oozie-linux-mac.md) For a deep dive showing how to use Oozie to drive an end-to-end pipeline, see [Operationalize the Data Pipeline](../hdinsight-operationalize-data-pipeline.md).</span></span> 

### <a name="azure-data-factory"></a><span data-ttu-id="251b6-122">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="251b6-122">Azure Data Factory</span></span>

<span data-ttu-id="251b6-123">Azure Data Factory provides orchestration capabilities in the form of platform-as-a-service.</span><span class="sxs-lookup"><span data-stu-id="251b6-123">Azure Data Factory provides orchestration capabilities in the form of platform-as-a-service.</span></span> <span data-ttu-id="251b6-124">It is a cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation.</span><span class="sxs-lookup"><span data-stu-id="251b6-124">It is a cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation.</span></span> 

<span data-ttu-id="251b6-125">Using Azure Data Factory, you can:</span><span class="sxs-lookup"><span data-stu-id="251b6-125">Using Azure Data Factory, you can:</span></span>

1. <span data-ttu-id="251b6-126">Create and schedule data-driven workflows (called pipelines) that ingest data from disparate data stores.</span><span class="sxs-lookup"><span data-stu-id="251b6-126">Create and schedule data-driven workflows (called pipelines) that ingest data from disparate data stores.</span></span>
2. <span data-ttu-id="251b6-127">Process and transform the data using compute services such as Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, Azure Batch, and Azure Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="251b6-127">Process and transform the data using compute services such as Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, Azure Batch, and Azure Machine Learning.</span></span>
3. <span data-ttu-id="251b6-128">Publish output data to data stores such as Azure SQL Data Warehouse for business intelligence (BI) applications to consume.</span><span class="sxs-lookup"><span data-stu-id="251b6-128">Publish output data to data stores such as Azure SQL Data Warehouse for business intelligence (BI) applications to consume.</span></span>

<span data-ttu-id="251b6-129">For more information on Azure Data Factory, see the [documentation](../../data-factory/introduction.md).</span><span class="sxs-lookup"><span data-stu-id="251b6-129">For more information on Azure Data Factory, see the [documentation](../../data-factory/introduction.md).</span></span>

## <a name="ingest-file-storage-and-result-storage"></a><span data-ttu-id="251b6-130">Ingest file storage and result storage</span><span class="sxs-lookup"><span data-stu-id="251b6-130">Ingest file storage and result storage</span></span>

<span data-ttu-id="251b6-131">Source data files are typically loaded into a location in Azure Storage or Azure Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="251b6-131">Source data files are typically loaded into a location in Azure Storage or Azure Data Lake Store.</span></span> <span data-ttu-id="251b6-132">Files can be in any format, but typically they are flat files like CSVs.</span><span class="sxs-lookup"><span data-stu-id="251b6-132">Files can be in any format, but typically they are flat files like CSVs.</span></span> 

### <a name="azure-storage"></a><span data-ttu-id="251b6-133">Azure Storage</span><span class="sxs-lookup"><span data-stu-id="251b6-133">Azure Storage</span></span> 

<span data-ttu-id="251b6-134">[Azure Storage](https://azure.microsoft.com/services/storage/blobs/) has [specific scalability targets](../../storage/common/storage-scalability-targets.md).</span><span class="sxs-lookup"><span data-stu-id="251b6-134">[Azure Storage](https://azure.microsoft.com/services/storage/blobs/) has [specific scalability targets](../../storage/common/storage-scalability-targets.md).</span></span>  <span data-ttu-id="251b6-135">For most analytic nodes, Azure Storage scales best when dealing with many smaller files.</span><span class="sxs-lookup"><span data-stu-id="251b6-135">For most analytic nodes, Azure Storage scales best when dealing with many smaller files.</span></span>  <span data-ttu-id="251b6-136">Azure Storage guarantees the same performance, no matter how many files or how large the files (as long as you are within your limits).</span><span class="sxs-lookup"><span data-stu-id="251b6-136">Azure Storage guarantees the same performance, no matter how many files or how large the files (as long as you are within your limits).</span></span>  <span data-ttu-id="251b6-137">This means that you can store terabytes of data and still get consistent performance, whether you are using a subset of the data or all of the data.</span><span class="sxs-lookup"><span data-stu-id="251b6-137">This means that you can store terabytes of data and still get consistent performance, whether you are using a subset of the data or all of the data.</span></span>

<span data-ttu-id="251b6-138">Azure Storage has several different types of blobs.</span><span class="sxs-lookup"><span data-stu-id="251b6-138">Azure Storage has several different types of blobs.</span></span>  <span data-ttu-id="251b6-139">An *append blob* is a great option for storing web logs or sensor data.</span><span class="sxs-lookup"><span data-stu-id="251b6-139">An *append blob* is a great option for storing web logs or sensor data.</span></span>  

<span data-ttu-id="251b6-140">Multiple blobs can be distributed across many servers to scale out access to them, but a single blob can only be served by a single server.</span><span class="sxs-lookup"><span data-stu-id="251b6-140">Multiple blobs can be distributed across many servers to scale out access to them, but a single blob can only be served by a single server.</span></span> <span data-ttu-id="251b6-141">While blobs can be logically grouped in blob containers, there are no partitioning implications from this grouping.</span><span class="sxs-lookup"><span data-stu-id="251b6-141">While blobs can be logically grouped in blob containers, there are no partitioning implications from this grouping.</span></span>

<span data-ttu-id="251b6-142">Azure Storage also has a WebHDFS API layer for the blob storage.</span><span class="sxs-lookup"><span data-stu-id="251b6-142">Azure Storage also has a WebHDFS API layer for the blob storage.</span></span>  <span data-ttu-id="251b6-143">All the services in HDInsight can access files in Azure Blob Storage for data cleaning and data processing, similarly to how those services would use Hadoop Distributed Files System (HDFS).</span><span class="sxs-lookup"><span data-stu-id="251b6-143">All the services in HDInsight can access files in Azure Blob Storage for data cleaning and data processing, similarly to how those services would use Hadoop Distributed Files System (HDFS).</span></span>

<span data-ttu-id="251b6-144">Data is typically ingested into Azure Storage using either PowerShell, the Azure Storage SDK, or AZCopy.</span><span class="sxs-lookup"><span data-stu-id="251b6-144">Data is typically ingested into Azure Storage using either PowerShell, the Azure Storage SDK, or AZCopy.</span></span>

### <a name="azure-data-lake-store"></a><span data-ttu-id="251b6-145">Azure Data Lake Store</span><span class="sxs-lookup"><span data-stu-id="251b6-145">Azure Data Lake Store</span></span>

<span data-ttu-id="251b6-146">Azure Data Lake Store (ADLS) is a managed, hyperscale repository for analytics data that is compatible with HDFS.</span><span class="sxs-lookup"><span data-stu-id="251b6-146">Azure Data Lake Store (ADLS) is a managed, hyperscale repository for analytics data that is compatible with HDFS.</span></span>  <span data-ttu-id="251b6-147">ADLS uses a design paradigm that is similar to HDFS, and offers unlimited scalability in terms of total capacity and the size of individual files.</span><span class="sxs-lookup"><span data-stu-id="251b6-147">ADLS uses a design paradigm that is similar to HDFS, and offers unlimited scalability in terms of total capacity and the size of individual files.</span></span> <span data-ttu-id="251b6-148">ADLS is very good when working with large files, since a large file can be stored across multiple nodes.</span><span class="sxs-lookup"><span data-stu-id="251b6-148">ADLS is very good when working with large files, since a large file can be stored across multiple nodes.</span></span>  <span data-ttu-id="251b6-149">Partitioning data in ADLS is done behind the scenes.</span><span class="sxs-lookup"><span data-stu-id="251b6-149">Partitioning data in ADLS is done behind the scenes.</span></span>  <span data-ttu-id="251b6-150">You get massive throughput to run analytic jobs with thousands of concurrent executors that efficiently read and write hundreds of terabytes of data.</span><span class="sxs-lookup"><span data-stu-id="251b6-150">You get massive throughput to run analytic jobs with thousands of concurrent executors that efficiently read and write hundreds of terabytes of data.</span></span>

<span data-ttu-id="251b6-151">Data is typically ingested into ADLS using Azure Data Factory, ADLS SDKs, AdlCopy Service, Apache DistCp, or Apache Sqoop.</span><span class="sxs-lookup"><span data-stu-id="251b6-151">Data is typically ingested into ADLS using Azure Data Factory, ADLS SDKs, AdlCopy Service, Apache DistCp, or Apache Sqoop.</span></span>  <span data-ttu-id="251b6-152">Which of these services to use largely depends on where the data is.</span><span class="sxs-lookup"><span data-stu-id="251b6-152">Which of these services to use largely depends on where the data is.</span></span>  <span data-ttu-id="251b6-153">If the data is currently in an existing Hadoop cluster, you might use Apache DistCp, AdlCopy Service, or Azure Data Factory.</span><span class="sxs-lookup"><span data-stu-id="251b6-153">If the data is currently in an existing Hadoop cluster, you might use Apache DistCp, AdlCopy Service, or Azure Data Factory.</span></span>  <span data-ttu-id="251b6-154">If it's in Azure Blob Storage, you might use Azure Data Lake Store .NET SDK, Azure PowerShell, or Azure Data Factory.</span><span class="sxs-lookup"><span data-stu-id="251b6-154">If it's in Azure Blob Storage, you might use Azure Data Lake Store .NET SDK, Azure PowerShell, or Azure Data Factory.</span></span>

<span data-ttu-id="251b6-155">ADLS is also optimized for event ingestion using Azure Event Hub or Apache Storm.</span><span class="sxs-lookup"><span data-stu-id="251b6-155">ADLS is also optimized for event ingestion using Azure Event Hub or Apache Storm.</span></span>

#### <a name="considerations-for-both-storage-options"></a><span data-ttu-id="251b6-156">Considerations for both storage options</span><span class="sxs-lookup"><span data-stu-id="251b6-156">Considerations for both storage options</span></span>

<span data-ttu-id="251b6-157">For uploading datasets in the terabyte range, network latency can be a major problem, particularly if the data is coming from an on-premises location.</span><span class="sxs-lookup"><span data-stu-id="251b6-157">For uploading datasets in the terabyte range, network latency can be a major problem, particularly if the data is coming from an on-premises location.</span></span>  <span data-ttu-id="251b6-158">In such cases, you can use the options below:</span><span class="sxs-lookup"><span data-stu-id="251b6-158">In such cases, you can use the options below:</span></span>

* <span data-ttu-id="251b6-159">Azure ExpressRoute:  Azure ExpressRoute lets you create private connections between Azure datacenters and your on-premises infrastructure.</span><span class="sxs-lookup"><span data-stu-id="251b6-159">Azure ExpressRoute:  Azure ExpressRoute lets you create private connections between Azure datacenters and your on-premises infrastructure.</span></span> <span data-ttu-id="251b6-160">These connections provide a reliable option for transferring large amounts of data.</span><span class="sxs-lookup"><span data-stu-id="251b6-160">These connections provide a reliable option for transferring large amounts of data.</span></span> <span data-ttu-id="251b6-161">For more information, see [Azure ExpressRoute documentation](../../expressroute/expressroute-introduction.md).</span><span class="sxs-lookup"><span data-stu-id="251b6-161">For more information, see [Azure ExpressRoute documentation](../../expressroute/expressroute-introduction.md).</span></span>

* <span data-ttu-id="251b6-162">"Offline" upload of data.</span><span class="sxs-lookup"><span data-stu-id="251b6-162">"Offline" upload of data.</span></span> <span data-ttu-id="251b6-163">You can use [Azure Import/Export service](../../storage/common/storage-import-export-service.md) to ship hard disk drives with your data to an Azure data center.</span><span class="sxs-lookup"><span data-stu-id="251b6-163">You can use [Azure Import/Export service](../../storage/common/storage-import-export-service.md) to ship hard disk drives with your data to an Azure data center.</span></span> <span data-ttu-id="251b6-164">Your data is first uploaded to Azure Storage Blobs.</span><span class="sxs-lookup"><span data-stu-id="251b6-164">Your data is first uploaded to Azure Storage Blobs.</span></span> <span data-ttu-id="251b6-165">You can then use [Azure Data Factory](../../data-factory/connector-azure-data-lake-store.md) or the [AdlCopy](../../data-lake-store/data-lake-store-copy-data-azure-storage-blob.md) tool to copy data from Azure Storage blobs to Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="251b6-165">You can then use [Azure Data Factory](../../data-factory/connector-azure-data-lake-store.md) or the [AdlCopy](../../data-lake-store/data-lake-store-copy-data-azure-storage-blob.md) tool to copy data from Azure Storage blobs to Data Lake Store.</span></span>

### <a name="azure-sql-data-warehouse"></a><span data-ttu-id="251b6-166">Azure SQL Data Warehouse</span><span class="sxs-lookup"><span data-stu-id="251b6-166">Azure SQL Data Warehouse</span></span>

<span data-ttu-id="251b6-167">Azure SQL DW is a great choice to store cleaned and prepared results for future analytics.</span><span class="sxs-lookup"><span data-stu-id="251b6-167">Azure SQL DW is a great choice to store cleaned and prepared results for future analytics.</span></span>  <span data-ttu-id="251b6-168">Azure HDInsight can be used to perform those services for Azure SQL DW.</span><span class="sxs-lookup"><span data-stu-id="251b6-168">Azure HDInsight can be used to perform those services for Azure SQL DW.</span></span>

<span data-ttu-id="251b6-169">Azure SQL Data Warehouse (SQL DW) is a relational database store optimized for analytic workloads.</span><span class="sxs-lookup"><span data-stu-id="251b6-169">Azure SQL Data Warehouse (SQL DW) is a relational database store optimized for analytic workloads.</span></span>  <span data-ttu-id="251b6-170">Azure SQL DW scales based on partitioned tables.</span><span class="sxs-lookup"><span data-stu-id="251b6-170">Azure SQL DW scales based on partitioned tables.</span></span>  <span data-ttu-id="251b6-171">Tables can be partitioned across multiple nodes.</span><span class="sxs-lookup"><span data-stu-id="251b6-171">Tables can be partitioned across multiple nodes.</span></span>  <span data-ttu-id="251b6-172">Azure SQL DW nodes are selected at the time of creation.</span><span class="sxs-lookup"><span data-stu-id="251b6-172">Azure SQL DW nodes are selected at the time of creation.</span></span>  <span data-ttu-id="251b6-173">They can scale after the fact, but that's an active process that might require data movement.</span><span class="sxs-lookup"><span data-stu-id="251b6-173">They can scale after the fact, but that's an active process that might require data movement.</span></span> <span data-ttu-id="251b6-174">See [SQL Data Warehouse - Manage Compute](../../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md) for more information.</span><span class="sxs-lookup"><span data-stu-id="251b6-174">See [SQL Data Warehouse - Manage Compute](../../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md) for more information.</span></span>

### <a name="hbase"></a><span data-ttu-id="251b6-175">HBase</span><span class="sxs-lookup"><span data-stu-id="251b6-175">HBase</span></span>

<span data-ttu-id="251b6-176">Apache HBase is a key-value store available in Azure HDInsight.</span><span class="sxs-lookup"><span data-stu-id="251b6-176">Apache HBase is a key-value store available in Azure HDInsight.</span></span>  <span data-ttu-id="251b6-177">Apache HBase is an open-source, NoSQL database that is built on Hadoop and modeled after Google BigTable.</span><span class="sxs-lookup"><span data-stu-id="251b6-177">Apache HBase is an open-source, NoSQL database that is built on Hadoop and modeled after Google BigTable.</span></span> <span data-ttu-id="251b6-178">HBase provides performant random access and strong consistency for large amounts of unstructured and semistructured data in a schemaless database organized by column families.</span><span class="sxs-lookup"><span data-stu-id="251b6-178">HBase provides performant random access and strong consistency for large amounts of unstructured and semistructured data in a schemaless database organized by column families.</span></span>

<span data-ttu-id="251b6-179">Data is stored in the rows of a table, and data within a row is grouped by column family.</span><span class="sxs-lookup"><span data-stu-id="251b6-179">Data is stored in the rows of a table, and data within a row is grouped by column family.</span></span> <span data-ttu-id="251b6-180">HBase is a schemaless database in the sense that neither the columns nor the type of data stored in them need to be defined before using them.</span><span class="sxs-lookup"><span data-stu-id="251b6-180">HBase is a schemaless database in the sense that neither the columns nor the type of data stored in them need to be defined before using them.</span></span> <span data-ttu-id="251b6-181">The open-source code scales linearly to handle petabytes of data on thousands of nodes.</span><span class="sxs-lookup"><span data-stu-id="251b6-181">The open-source code scales linearly to handle petabytes of data on thousands of nodes.</span></span> <span data-ttu-id="251b6-182">HBase can rely on data redundancy, batch processing, and other features that are provided by distributed applications in the Hadoop ecosystem.</span><span class="sxs-lookup"><span data-stu-id="251b6-182">HBase can rely on data redundancy, batch processing, and other features that are provided by distributed applications in the Hadoop ecosystem.</span></span>   

<span data-ttu-id="251b6-183">HBase is an excellent destination for sensor and log data for future analysis.</span><span class="sxs-lookup"><span data-stu-id="251b6-183">HBase is an excellent destination for sensor and log data for future analysis.</span></span>

<span data-ttu-id="251b6-184">HBase scalability is dependent on the number of nodes in the HDInsight cluster.</span><span class="sxs-lookup"><span data-stu-id="251b6-184">HBase scalability is dependent on the number of nodes in the HDInsight cluster.</span></span>

### <a name="azure-sql-database-and-azure-database"></a><span data-ttu-id="251b6-185">Azure SQL Database and Azure Database</span><span class="sxs-lookup"><span data-stu-id="251b6-185">Azure SQL Database and Azure Database</span></span>

<span data-ttu-id="251b6-186">Azure offers three different relational databases as platform-as-a-service (PAAS).</span><span class="sxs-lookup"><span data-stu-id="251b6-186">Azure offers three different relational databases as platform-as-a-service (PAAS).</span></span>

* <span data-ttu-id="251b6-187">[Azure SQL Database](../../sql-database/sql-database-technical-overview.md) is an implementation of Microsoft SQL Server.</span><span class="sxs-lookup"><span data-stu-id="251b6-187">[Azure SQL Database](../../sql-database/sql-database-technical-overview.md) is an implementation of Microsoft SQL Server.</span></span> <span data-ttu-id="251b6-188">For more information on performance, see [Tuning Performance in Azure SQL Database](../../sql-database/sql-database-performance-guidance.md).</span><span class="sxs-lookup"><span data-stu-id="251b6-188">For more information on performance, see [Tuning Performance in Azure SQL Database](../../sql-database/sql-database-performance-guidance.md).</span></span>
* <span data-ttu-id="251b6-189">[Azure Database for MySQL](../../mysql/overview.md)  is an implementation of Oracle MySQL.</span><span class="sxs-lookup"><span data-stu-id="251b6-189">[Azure Database for MySQL](../../mysql/overview.md)  is an implementation of Oracle MySQL.</span></span>
* <span data-ttu-id="251b6-190">[Azure Database for PostgreSQL](../../postgresql/quickstart-create-server-database-portal.md) is an implementation of PostgreSQL.</span><span class="sxs-lookup"><span data-stu-id="251b6-190">[Azure Database for PostgreSQL](../../postgresql/quickstart-create-server-database-portal.md) is an implementation of PostgreSQL.</span></span>

<span data-ttu-id="251b6-191">These products scale up, which means that they are scaled by adding more CPU and memory.</span><span class="sxs-lookup"><span data-stu-id="251b6-191">These products scale up, which means that they are scaled by adding more CPU and memory.</span></span>  <span data-ttu-id="251b6-192">You can also choose to use premium disks with the products for better I/O performance.</span><span class="sxs-lookup"><span data-stu-id="251b6-192">You can also choose to use premium disks with the products for better I/O performance.</span></span>

## <a name="azure-analysis-services"></a><span data-ttu-id="251b6-193">Azure Analysis Services</span><span class="sxs-lookup"><span data-stu-id="251b6-193">Azure Analysis Services</span></span> 

<span data-ttu-id="251b6-194">Azure Analysis Services (AAS) is an analytical data engine used in decision support and business analytics, providing the analytical data for business reports and client applications such as Power BI, Excel, Reporting Services reports, and other data visualization tools.</span><span class="sxs-lookup"><span data-stu-id="251b6-194">Azure Analysis Services (AAS) is an analytical data engine used in decision support and business analytics, providing the analytical data for business reports and client applications such as Power BI, Excel, Reporting Services reports, and other data visualization tools.</span></span>

<span data-ttu-id="251b6-195">Analysis cubes can scale by changing tiers for each individual cube.</span><span class="sxs-lookup"><span data-stu-id="251b6-195">Analysis cubes can scale by changing tiers for each individual cube.</span></span>  <span data-ttu-id="251b6-196">For more information, see [Azure Analysis Services Pricing](https://azure.microsoft.com/pricing/details/analysis-services/).</span><span class="sxs-lookup"><span data-stu-id="251b6-196">For more information, see [Azure Analysis Services Pricing](https://azure.microsoft.com/pricing/details/analysis-services/).</span></span>

## <a name="extract-and-load"></a><span data-ttu-id="251b6-197">Extract and Load</span><span class="sxs-lookup"><span data-stu-id="251b6-197">Extract and Load</span></span>

<span data-ttu-id="251b6-198">Once the data exists in Azure, you can use many services to extract and load it into other products.</span><span class="sxs-lookup"><span data-stu-id="251b6-198">Once the data exists in Azure, you can use many services to extract and load it into other products.</span></span>  <span data-ttu-id="251b6-199">HDInsight supports Sqoop and Flume.</span><span class="sxs-lookup"><span data-stu-id="251b6-199">HDInsight supports Sqoop and Flume.</span></span> 

### <a name="sqoop"></a><span data-ttu-id="251b6-200">Sqoop</span><span class="sxs-lookup"><span data-stu-id="251b6-200">Sqoop</span></span>

<span data-ttu-id="251b6-201">Apache Sqoop is a tool designed for efficiently transferring data between structured, semi-structured, and unstructured data sources.</span><span class="sxs-lookup"><span data-stu-id="251b6-201">Apache Sqoop is a tool designed for efficiently transferring data between structured, semi-structured, and unstructured data sources.</span></span> 

<span data-ttu-id="251b6-202">Sqoop uses MapReduce to import and export the data, to provide parallel operation and fault tolerance.</span><span class="sxs-lookup"><span data-stu-id="251b6-202">Sqoop uses MapReduce to import and export the data, to provide parallel operation and fault tolerance.</span></span>

### <a name="flume"></a><span data-ttu-id="251b6-203">Flume</span><span class="sxs-lookup"><span data-stu-id="251b6-203">Flume</span></span>

<span data-ttu-id="251b6-204">Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.</span><span class="sxs-lookup"><span data-stu-id="251b6-204">Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.</span></span> <span data-ttu-id="251b6-205">Flume has a simple and flexible architecture based on streaming data flows.</span><span class="sxs-lookup"><span data-stu-id="251b6-205">Flume has a simple and flexible architecture based on streaming data flows.</span></span> <span data-ttu-id="251b6-206">Flume is robust and fault-tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.</span><span class="sxs-lookup"><span data-stu-id="251b6-206">Flume is robust and fault-tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.</span></span> <span data-ttu-id="251b6-207">Flume uses a simple extensible data model that allows for online analytic application.</span><span class="sxs-lookup"><span data-stu-id="251b6-207">Flume uses a simple extensible data model that allows for online analytic application.</span></span>

<span data-ttu-id="251b6-208">Apache Flume cannot be used with Azure HDInsight.</span><span class="sxs-lookup"><span data-stu-id="251b6-208">Apache Flume cannot be used with Azure HDInsight.</span></span>  <span data-ttu-id="251b6-209">An on-premises Hadoop installation can use Flume to send data to either Azure Storage Blobs or Azure Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="251b6-209">An on-premises Hadoop installation can use Flume to send data to either Azure Storage Blobs or Azure Data Lake Store.</span></span>  <span data-ttu-id="251b6-210">For more information, see [Using Apache Flume with HDInsight](https://blogs.msdn.microsoft.com/bigdatasupport/2014/03/18/using-apache-flume-with-hdinsight/).</span><span class="sxs-lookup"><span data-stu-id="251b6-210">For more information, see [Using Apache Flume with HDInsight](https://blogs.msdn.microsoft.com/bigdatasupport/2014/03/18/using-apache-flume-with-hdinsight/).</span></span>

## <a name="transform"></a><span data-ttu-id="251b6-211">Transform</span><span class="sxs-lookup"><span data-stu-id="251b6-211">Transform</span></span>

<span data-ttu-id="251b6-212">Once data exists in the chosen location, you need to clean it, combine it, or prepare it for a specific usage pattern.</span><span class="sxs-lookup"><span data-stu-id="251b6-212">Once data exists in the chosen location, you need to clean it, combine it, or prepare it for a specific usage pattern.</span></span>  <span data-ttu-id="251b6-213">Hive, Pig, and Spark SQL are all good choices for that kind of work.</span><span class="sxs-lookup"><span data-stu-id="251b6-213">Hive, Pig, and Spark SQL are all good choices for that kind of work.</span></span>  <span data-ttu-id="251b6-214">They are all supported on HDInsight.</span><span class="sxs-lookup"><span data-stu-id="251b6-214">They are all supported on HDInsight.</span></span> 

## <a name="next-steps"></a><span data-ttu-id="251b6-215">Next steps</span><span class="sxs-lookup"><span data-stu-id="251b6-215">Next steps</span></span>

* [<span data-ttu-id="251b6-216">Use Pig with Hadoop on HDInsight</span><span class="sxs-lookup"><span data-stu-id="251b6-216">Use Pig with Hadoop on HDInsight</span></span>](hdinsight-use-pig.md)
* [<span data-ttu-id="251b6-217">Using Apache Hive as an ETL Tool</span><span class="sxs-lookup"><span data-stu-id="251b6-217">Using Apache Hive as an ETL Tool</span></span>](apache-hadoop-using-apache-hive-as-an-etl-tool.md) 
