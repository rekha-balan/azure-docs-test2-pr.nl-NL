---
title: Run custom MapReduce programs - Azure HDInsight
description: When and how to run custom MapReduce programs in HDInsight.
services: hdinsight
author: ashishthaps
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: conceptual
ms.date: 12/04/2017
ms.author: ashishth
ms.openlocfilehash: 80f58157e69ff5a6e707408d795889b5bcd677b7
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44868834"
---
# <a name="run-custom-mapreduce-programs"></a><span data-ttu-id="ae5ec-103">Run custom MapReduce programs</span><span class="sxs-lookup"><span data-stu-id="ae5ec-103">Run custom MapReduce programs</span></span>

<span data-ttu-id="ae5ec-104">Hadoop-based big data systems such as HDInsight enable data processing using a wide range of tools and technologies.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-104">Hadoop-based big data systems such as HDInsight enable data processing using a wide range of tools and technologies.</span></span> <span data-ttu-id="ae5ec-105">The following table describes the main advantages and considerations for each one.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-105">The following table describes the main advantages and considerations for each one.</span></span>

| <span data-ttu-id="ae5ec-106">Query mechanism</span><span class="sxs-lookup"><span data-stu-id="ae5ec-106">Query mechanism</span></span> | <span data-ttu-id="ae5ec-107">Advantages</span><span class="sxs-lookup"><span data-stu-id="ae5ec-107">Advantages</span></span> | <span data-ttu-id="ae5ec-108">Considerations</span><span class="sxs-lookup"><span data-stu-id="ae5ec-108">Considerations</span></span> |
| --- | --- | --- |
| <span data-ttu-id="ae5ec-109">**Hive using HiveQL**</span><span class="sxs-lookup"><span data-stu-id="ae5ec-109">**Hive using HiveQL**</span></span> | <ul><li><span data-ttu-id="ae5ec-110">An excellent solution for batch processing and analysis of large amounts of immutable data, for data summarization, and for on demand querying.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-110">An excellent solution for batch processing and analysis of large amounts of immutable data, for data summarization, and for on demand querying.</span></span> <span data-ttu-id="ae5ec-111">It uses a familiar SQL-like syntax.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-111">It uses a familiar SQL-like syntax.</span></span></li><li><span data-ttu-id="ae5ec-112">It can be used to produce persistent tables of data that can be easily partitioned and indexed.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-112">It can be used to produce persistent tables of data that can be easily partitioned and indexed.</span></span></li><li><span data-ttu-id="ae5ec-113">Multiple external tables and views can be created over the same data.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-113">Multiple external tables and views can be created over the same data.</span></span></li><li><span data-ttu-id="ae5ec-114">It supports a simple data warehouse implementation that provides massive scale-out and fault-tolerance capabilities for data storage and processing.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-114">It supports a simple data warehouse implementation that provides massive scale-out and fault-tolerance capabilities for data storage and processing.</span></span></li></ul> | <ul><li><span data-ttu-id="ae5ec-115">It requires the source data to have at least some identifiable structure.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-115">It requires the source data to have at least some identifiable structure.</span></span></li><li><span data-ttu-id="ae5ec-116">It is not suitable for real-time queries and row level updates.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-116">It is not suitable for real-time queries and row level updates.</span></span> <span data-ttu-id="ae5ec-117">It is best used for batch jobs over large sets of data.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-117">It is best used for batch jobs over large sets of data.</span></span></li><li><span data-ttu-id="ae5ec-118">It might not be able to carry out some types of complex processing tasks.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-118">It might not be able to carry out some types of complex processing tasks.</span></span></li></ul> |
| <span data-ttu-id="ae5ec-119">**Pig using Pig Latin**</span><span class="sxs-lookup"><span data-stu-id="ae5ec-119">**Pig using Pig Latin**</span></span> | <ul><li><span data-ttu-id="ae5ec-120">An excellent solution for manipulating data as sets, merging and filtering datasets, applying functions to records or groups of records, and for restructuring data by defining columns, by grouping values, or by converting columns to rows.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-120">An excellent solution for manipulating data as sets, merging and filtering datasets, applying functions to records or groups of records, and for restructuring data by defining columns, by grouping values, or by converting columns to rows.</span></span></li><li><span data-ttu-id="ae5ec-121">It can use a workflow-based approach as a sequence of operations on data.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-121">It can use a workflow-based approach as a sequence of operations on data.</span></span></li></ul> | <ul><li><span data-ttu-id="ae5ec-122">SQL users may find Pig Latin is less familiar and more difficult to use than HiveQL.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-122">SQL users may find Pig Latin is less familiar and more difficult to use than HiveQL.</span></span></li><li><span data-ttu-id="ae5ec-123">The default output is usually a text file and so can be more difficult to use with visualization tools such as Excel.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-123">The default output is usually a text file and so can be more difficult to use with visualization tools such as Excel.</span></span> <span data-ttu-id="ae5ec-124">Typically you will layer a Hive table over the output.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-124">Typically you will layer a Hive table over the output.</span></span></li></ul> |
| <span data-ttu-id="ae5ec-125">**Custom map/reduce**</span><span class="sxs-lookup"><span data-stu-id="ae5ec-125">**Custom map/reduce**</span></span> | <ul><li><span data-ttu-id="ae5ec-126">It provides full control over the map and reduce phases and execution.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-126">It provides full control over the map and reduce phases and execution.</span></span></li><li><span data-ttu-id="ae5ec-127">It allows queries to be optimized to achieve maximum performance from the cluster, or to minimize the load on the servers and the network.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-127">It allows queries to be optimized to achieve maximum performance from the cluster, or to minimize the load on the servers and the network.</span></span></li><li><span data-ttu-id="ae5ec-128">The components can be written in a range of well-known languages.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-128">The components can be written in a range of well-known languages.</span></span></li></ul> | <ul><li><span data-ttu-id="ae5ec-129">It is more difficult than using Pig or Hive because you must create your own map and reduce components.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-129">It is more difficult than using Pig or Hive because you must create your own map and reduce components.</span></span></li><li><span data-ttu-id="ae5ec-130">Processes that require joining sets of data are more difficult to implement.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-130">Processes that require joining sets of data are more difficult to implement.</span></span></li><li><span data-ttu-id="ae5ec-131">Even though there are test frameworks available, debugging code is more complex than a normal application because the code runs as a batch job under the control of the Hadoop job scheduler.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-131">Even though there are test frameworks available, debugging code is more complex than a normal application because the code runs as a batch job under the control of the Hadoop job scheduler.</span></span></li></ul> |
| <span data-ttu-id="ae5ec-132">**HCatalog**</span><span class="sxs-lookup"><span data-stu-id="ae5ec-132">**HCatalog**</span></span> | <ul><li><span data-ttu-id="ae5ec-133">It abstracts the path details of storage, making administration easier and removing the need for users to know where the data is stored.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-133">It abstracts the path details of storage, making administration easier and removing the need for users to know where the data is stored.</span></span></li><li><span data-ttu-id="ae5ec-134">It enables notification of events such as data availability, allowing other tools such as Oozie to detect when operations have occurred.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-134">It enables notification of events such as data availability, allowing other tools such as Oozie to detect when operations have occurred.</span></span></li><li><span data-ttu-id="ae5ec-135">It exposes a relational view of data, including partitioning by key, and makes the data easy to access.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-135">It exposes a relational view of data, including partitioning by key, and makes the data easy to access.</span></span></li></ul> | <ul><li><span data-ttu-id="ae5ec-136">It supports RCFile, CSV text, JSON text, SequenceFile, and ORC file formats by default, but you may need to write a custom SerDe for other formats.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-136">It supports RCFile, CSV text, JSON text, SequenceFile, and ORC file formats by default, but you may need to write a custom SerDe for other formats.</span></span></li><li><span data-ttu-id="ae5ec-137">HCatalog is not thread-safe.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-137">HCatalog is not thread-safe.</span></span></li><li><span data-ttu-id="ae5ec-138">There are some restrictions on the data types for columns when using the HCatalog loader in Pig scripts.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-138">There are some restrictions on the data types for columns when using the HCatalog loader in Pig scripts.</span></span> <span data-ttu-id="ae5ec-139">For more information, see [HCatLoader Data Types](http://cwiki.apache.org/confluence/display/Hive/HCatalog%20LoadStore#HCatalogLoadStore-HCatLoaderDataTypes) in the Apache HCatalog documentation.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-139">For more information, see [HCatLoader Data Types](http://cwiki.apache.org/confluence/display/Hive/HCatalog%20LoadStore#HCatalogLoadStore-HCatLoaderDataTypes) in the Apache HCatalog documentation.</span></span></li></ul> |

<span data-ttu-id="ae5ec-140">Typically, you use the simplest of these approaches that can provide the results you require.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-140">Typically, you use the simplest of these approaches that can provide the results you require.</span></span> <span data-ttu-id="ae5ec-141">For example, you may be able to achieve such results by using just Hive, but for more complex scenarios you may need to use Pig, or even write your own map and reduce components.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-141">For example, you may be able to achieve such results by using just Hive, but for more complex scenarios you may need to use Pig, or even write your own map and reduce components.</span></span> <span data-ttu-id="ae5ec-142">You may also decide, after experimenting with Hive or Pig, that custom map and reduce components can provide better performance by allowing you to fine-tune and optimize the processing.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-142">You may also decide, after experimenting with Hive or Pig, that custom map and reduce components can provide better performance by allowing you to fine-tune and optimize the processing.</span></span>

## <a name="custom-mapreduce-components"></a><span data-ttu-id="ae5ec-143">Custom map/reduce components</span><span class="sxs-lookup"><span data-stu-id="ae5ec-143">Custom map/reduce components</span></span>

<span data-ttu-id="ae5ec-144">Map/reduce code consists of two separate functions implemented as **map** and **reduce** components.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-144">Map/reduce code consists of two separate functions implemented as **map** and **reduce** components.</span></span> <span data-ttu-id="ae5ec-145">The **map** component is run in parallel on multiple cluster nodes, each node applying the mapping to the node's own subset of the data.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-145">The **map** component is run in parallel on multiple cluster nodes, each node applying the mapping to the node's own subset of the data.</span></span> <span data-ttu-id="ae5ec-146">The **reduce** component collates and summarizes the results from all  the map functions.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-146">The **reduce** component collates and summarizes the results from all  the map functions.</span></span> <span data-ttu-id="ae5ec-147">For more information on these two components, see [Use MapReduce in Hadoop on HDInsight](hdinsight-use-mapreduce.md).</span><span class="sxs-lookup"><span data-stu-id="ae5ec-147">For more information on these two components, see [Use MapReduce in Hadoop on HDInsight](hdinsight-use-mapreduce.md).</span></span>

<span data-ttu-id="ae5ec-148">In most HDInsight processing scenarios it is simpler and more efficient to use a higher-level abstraction such as Pig or Hive.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-148">In most HDInsight processing scenarios it is simpler and more efficient to use a higher-level abstraction such as Pig or Hive.</span></span> <span data-ttu-id="ae5ec-149">You can also create custom map and reduce components for use within Hive scripts to perform more sophisticated processing.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-149">You can also create custom map and reduce components for use within Hive scripts to perform more sophisticated processing.</span></span>

<span data-ttu-id="ae5ec-150">Custom map/reduce components are typically written in Java.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-150">Custom map/reduce components are typically written in Java.</span></span> <span data-ttu-id="ae5ec-151">Hadoop provides a streaming interface that also allows components to be used that are developed in other languages such as C#, F#, Visual Basic, Python, and JavaScript.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-151">Hadoop provides a streaming interface that also allows components to be used that are developed in other languages such as C#, F#, Visual Basic, Python, and JavaScript.</span></span>

* <span data-ttu-id="ae5ec-152">For a walkthrough on developing custom Java MapReduce programs, see [Develop Java MapReduce programs for Hadoop on HDInsight](apache-hadoop-develop-deploy-java-mapreduce-linux.md).</span><span class="sxs-lookup"><span data-stu-id="ae5ec-152">For a walkthrough on developing custom Java MapReduce programs, see [Develop Java MapReduce programs for Hadoop on HDInsight](apache-hadoop-develop-deploy-java-mapreduce-linux.md).</span></span>
* <span data-ttu-id="ae5ec-153">To see an example using Python, see [Develop Python streaming MapReduce programs for HDInsight](apache-hadoop-streaming-python.md).</span><span class="sxs-lookup"><span data-stu-id="ae5ec-153">To see an example using Python, see [Develop Python streaming MapReduce programs for HDInsight](apache-hadoop-streaming-python.md).</span></span>

<span data-ttu-id="ae5ec-154">Consider creating your own map and reduce components for the following conditions:</span><span class="sxs-lookup"><span data-stu-id="ae5ec-154">Consider creating your own map and reduce components for the following conditions:</span></span>

* <span data-ttu-id="ae5ec-155">You need to process data that is completely unstructured by parsing the data and using custom logic to obtain structured information from it.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-155">You need to process data that is completely unstructured by parsing the data and using custom logic to obtain structured information from it.</span></span>
* <span data-ttu-id="ae5ec-156">You want to perform complex tasks that are difficult (or impossible) to express in Pig or Hive without resorting to creating a UDF.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-156">You want to perform complex tasks that are difficult (or impossible) to express in Pig or Hive without resorting to creating a UDF.</span></span> <span data-ttu-id="ae5ec-157">For example, you might need to use an external geocoding service to convert latitude and longitude coordinates or IP addresses in the source data to geographical location names.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-157">For example, you might need to use an external geocoding service to convert latitude and longitude coordinates or IP addresses in the source data to geographical location names.</span></span>
* <span data-ttu-id="ae5ec-158">You want to reuse your existing .NET, Python, or JavaScript code in map/reduce components by using the Hadoop streaming interface.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-158">You want to reuse your existing .NET, Python, or JavaScript code in map/reduce components by using the Hadoop streaming interface.</span></span>

## <a name="upload-and-run-your-custom-mapreduce-program"></a><span data-ttu-id="ae5ec-159">Upload and run your custom MapReduce program</span><span class="sxs-lookup"><span data-stu-id="ae5ec-159">Upload and run your custom MapReduce program</span></span>

<span data-ttu-id="ae5ec-160">The most common MapReduce programs are written in Java and compiled to a jar file.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-160">The most common MapReduce programs are written in Java and compiled to a jar file.</span></span>

1. <span data-ttu-id="ae5ec-161">After you have developed, compiled, and tested your MapReduce program, use the `scp` command to upload your jar file to the headnode.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-161">After you have developed, compiled, and tested your MapReduce program, use the `scp` command to upload your jar file to the headnode.</span></span>

    ```bash
    scp mycustomprogram.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net
    ```

    <span data-ttu-id="ae5ec-162">Replace **USERNAME** with the SSH user account for your cluster.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-162">Replace **USERNAME** with the SSH user account for your cluster.</span></span> <span data-ttu-id="ae5ec-163">Replace **CLUSTERNAME** with the cluster name.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-163">Replace **CLUSTERNAME** with the cluster name.</span></span> <span data-ttu-id="ae5ec-164">If you used a password to secure the SSH account, you are prompted to enter the password.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-164">If you used a password to secure the SSH account, you are prompted to enter the password.</span></span> <span data-ttu-id="ae5ec-165">If you used a certificate, you may need to use the `-i` parameter to specify the private key file.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-165">If you used a certificate, you may need to use the `-i` parameter to specify the private key file.</span></span>

2. <span data-ttu-id="ae5ec-166">Connect to the cluster using [SSH](../hdinsight-hadoop-linux-use-ssh-unix.md).</span><span class="sxs-lookup"><span data-stu-id="ae5ec-166">Connect to the cluster using [SSH](../hdinsight-hadoop-linux-use-ssh-unix.md).</span></span>

    ```bash
    ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.net
    ```

3. <span data-ttu-id="ae5ec-167">From the SSH session, execute your MapReduce program through YARN.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-167">From the SSH session, execute your MapReduce program through YARN.</span></span>

    ```bash
    yarn jar mycustomprogram.jar mynamespace.myclass /example/data/sample.log /example/data/logoutput
    ```

    <span data-ttu-id="ae5ec-168">This command submits the MapReduce job to YARN.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-168">This command submits the MapReduce job to YARN.</span></span> <span data-ttu-id="ae5ec-169">The input file is `/example/data/sample.log`, and the output directory is `/example/data/logoutput`.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-169">The input file is `/example/data/sample.log`, and the output directory is `/example/data/logoutput`.</span></span> <span data-ttu-id="ae5ec-170">The input file and any output files are stored to the default storage for the cluster.</span><span class="sxs-lookup"><span data-stu-id="ae5ec-170">The input file and any output files are stored to the default storage for the cluster.</span></span>

## <a name="next-steps"></a><span data-ttu-id="ae5ec-171">Next steps</span><span class="sxs-lookup"><span data-stu-id="ae5ec-171">Next steps</span></span>

* [<span data-ttu-id="ae5ec-172">Use C# with MapReduce streaming on Hadoop in HDInsight</span><span class="sxs-lookup"><span data-stu-id="ae5ec-172">Use C# with MapReduce streaming on Hadoop in HDInsight</span></span>](apache-hadoop-dotnet-csharp-mapreduce-streaming.md)
* [<span data-ttu-id="ae5ec-173">Develop Java MapReduce programs for Hadoop on HDInsight</span><span class="sxs-lookup"><span data-stu-id="ae5ec-173">Develop Java MapReduce programs for Hadoop on HDInsight</span></span>](apache-hadoop-develop-deploy-java-mapreduce-linux.md)
* [<span data-ttu-id="ae5ec-174">Develop Python streaming MapReduce programs for HDInsight</span><span class="sxs-lookup"><span data-stu-id="ae5ec-174">Develop Python streaming MapReduce programs for HDInsight</span></span>](apache-hadoop-streaming-python.md)
* [<span data-ttu-id="ae5ec-175">Use Azure Toolkit for Eclipse to create Spark applications for an HDInsight cluster</span><span class="sxs-lookup"><span data-stu-id="ae5ec-175">Use Azure Toolkit for Eclipse to create Spark applications for an HDInsight cluster</span></span>](../spark/apache-spark-eclipse-tool-plugin.md)
* [<span data-ttu-id="ae5ec-176">Use Python User Defined Functions (UDF) with Hive and Pig in HDInsight</span><span class="sxs-lookup"><span data-stu-id="ae5ec-176">Use Python User Defined Functions (UDF) with Hive and Pig in HDInsight</span></span>](python-udf-hdinsight.md)
