---
title: Compute context options for R Server on HDInsight | Microsoft Docs
description: Learn about the different compute context options available to users with R Server on HDInsight
services: HDInsight
documentationcenter: ''
author: jeffstokes72
manager: jhubbard
editor: cgronlun
ms.assetid: 0deb0b1c-4094-459b-94fc-ec9b774c1f8a
ms.service: HDInsight
ms.custom: hdinsightactive
ms.devlang: R
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: data-services
ms.date: 02/28/2017
ms.author: jeffstok
ms.openlocfilehash: 78635c07f7a15117329f26ffe40d9fc19edb2212
ms.sourcegitcommit: 5b9d839c0c0a94b293fdafe1d6e5429506c07e05
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 08/02/2018
ms.locfileid: "44550317"
---
# <a name="compute-context-options-for-r-server-on-hdinsight"></a><span data-ttu-id="7d3a5-103">Compute context options for R Server on HDInsight</span><span class="sxs-lookup"><span data-stu-id="7d3a5-103">Compute context options for R Server on HDInsight</span></span>
<span data-ttu-id="7d3a5-104">Microsoft R Server on Azure HDInsight provides the latest capabilities for R-based analytics.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-104">Microsoft R Server on Azure HDInsight provides the latest capabilities for R-based analytics.</span></span> <span data-ttu-id="7d3a5-105">It uses data that's stored in HDFS in a container in your [Azure Blob](../storage/storage-introduction.md "Azure Blob storage") storage account, a Data Lake store or the local Linux file system.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-105">It uses data that's stored in HDFS in a container in your [Azure Blob](../storage/storage-introduction.md "Azure Blob storage") storage account, a Data Lake store or the local Linux file system.</span></span> <span data-ttu-id="7d3a5-106">Since R Server is built on open source R, the R-based applications you build can leverage any of the 8000+ open source R packages.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-106">Since R Server is built on open source R, the R-based applications you build can leverage any of the 8000+ open source R packages.</span></span> <span data-ttu-id="7d3a5-107">They can also leverage the routines in [ScaleR](http://www.revolutionanalytics.com/revolution-r-enterprise-scaler "Revolution Analytics ScaleR"), Microsoft’s big data analytics package that's included with R Server.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-107">They can also leverage the routines in [ScaleR](http://www.revolutionanalytics.com/revolution-r-enterprise-scaler "Revolution Analytics ScaleR"), Microsoft’s big data analytics package that's included with R Server.</span></span>  

<span data-ttu-id="7d3a5-108">The edge node of a cluster provides a convenient place to connect to the cluster and run your R scripts.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-108">The edge node of a cluster provides a convenient place to connect to the cluster and run your R scripts.</span></span> <span data-ttu-id="7d3a5-109">With an edge node, you have the option of running ScaleR’s parallelized distributed functions across the cores of the edge node server.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-109">With an edge node, you have the option of running ScaleR’s parallelized distributed functions across the cores of the edge node server.</span></span> <span data-ttu-id="7d3a5-110">You also have the option to run them across the nodes of the cluster by using ScaleR’s Hadoop Map Reduce or Spark compute contexts.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-110">You also have the option to run them across the nodes of the cluster by using ScaleR’s Hadoop Map Reduce or Spark compute contexts.</span></span>

## <a name="compute-contexts-for-an-edge-node"></a><span data-ttu-id="7d3a5-111">Compute contexts for an edge node</span><span class="sxs-lookup"><span data-stu-id="7d3a5-111">Compute contexts for an edge node</span></span>
<span data-ttu-id="7d3a5-112">In general, an R script that's run in R Server on the edge node runs within the R interpreter on that node.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-112">In general, an R script that's run in R Server on the edge node runs within the R interpreter on that node.</span></span> <span data-ttu-id="7d3a5-113">The exceptions are those steps that call a ScaleR function.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-113">The exceptions are those steps that call a ScaleR function.</span></span> <span data-ttu-id="7d3a5-114">The ScaleR calls run in a compute environment that's determined by how you set the ScaleR compute context.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-114">The ScaleR calls run in a compute environment that's determined by how you set the ScaleR compute context.</span></span>  <span data-ttu-id="7d3a5-115">When you run your R script from an edge node, the possible values of the compute context are local sequential (‘local’), local parallel (‘localpar’), Map Reduce, and Spark.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-115">When you run your R script from an edge node, the possible values of the compute context are local sequential (‘local’), local parallel (‘localpar’), Map Reduce, and Spark.</span></span>

<span data-ttu-id="7d3a5-116">The ‘local’ and ‘localpar’ options differ only in how rxExec calls are executed.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-116">The ‘local’ and ‘localpar’ options differ only in how rxExec calls are executed.</span></span> <span data-ttu-id="7d3a5-117">They both execute other rx-function calls in a parallel manner across all available cores unless specified otherwise through use of the ScaleR numCoresToUse option, e.g. rxOptions(numCoresToUse=6).</span><span class="sxs-lookup"><span data-stu-id="7d3a5-117">They both execute other rx-function calls in a parallel manner across all available cores unless specified otherwise through use of the ScaleR numCoresToUse option, e.g. rxOptions(numCoresToUse=6).</span></span> <span data-ttu-id="7d3a5-118">The following summarizes the various compute context options</span><span class="sxs-lookup"><span data-stu-id="7d3a5-118">The following summarizes the various compute context options</span></span>

| <span data-ttu-id="7d3a5-119">Compute context</span><span class="sxs-lookup"><span data-stu-id="7d3a5-119">Compute context</span></span>  | <span data-ttu-id="7d3a5-120">How to set</span><span class="sxs-lookup"><span data-stu-id="7d3a5-120">How to set</span></span>                      | <span data-ttu-id="7d3a5-121">Execution context</span><span class="sxs-lookup"><span data-stu-id="7d3a5-121">Execution context</span></span>                        |
| ---------------- | ------------------------------- | ---------------------------------------- |
| <span data-ttu-id="7d3a5-122">Local sequential</span><span class="sxs-lookup"><span data-stu-id="7d3a5-122">Local sequential</span></span> | <span data-ttu-id="7d3a5-123">rxSetComputeContext(‘local’)</span><span class="sxs-lookup"><span data-stu-id="7d3a5-123">rxSetComputeContext(‘local’)</span></span>    | <span data-ttu-id="7d3a5-124">Parallelized execution across the cores of the edge node server, except for rxExec calls which are executed serially</span><span class="sxs-lookup"><span data-stu-id="7d3a5-124">Parallelized execution across the cores of the edge node server, except for rxExec calls which are executed serially</span></span> |
| <span data-ttu-id="7d3a5-125">Local parallel</span><span class="sxs-lookup"><span data-stu-id="7d3a5-125">Local parallel</span></span>   | <span data-ttu-id="7d3a5-126">rxSetComputeContext(‘localpar’)</span><span class="sxs-lookup"><span data-stu-id="7d3a5-126">rxSetComputeContext(‘localpar’)</span></span> | <span data-ttu-id="7d3a5-127">Parallelized execution across the cores of the edge node server</span><span class="sxs-lookup"><span data-stu-id="7d3a5-127">Parallelized execution across the cores of the edge node server</span></span> |
| <span data-ttu-id="7d3a5-128">Spark</span><span class="sxs-lookup"><span data-stu-id="7d3a5-128">Spark</span></span>            | <span data-ttu-id="7d3a5-129">RxSpark()</span><span class="sxs-lookup"><span data-stu-id="7d3a5-129">RxSpark()</span></span>                       | <span data-ttu-id="7d3a5-130">Parallelized distributed execution via Spark across the nodes of the HDI cluster</span><span class="sxs-lookup"><span data-stu-id="7d3a5-130">Parallelized distributed execution via Spark across the nodes of the HDI cluster</span></span> |
| <span data-ttu-id="7d3a5-131">Map Reduce</span><span class="sxs-lookup"><span data-stu-id="7d3a5-131">Map Reduce</span></span>       | <span data-ttu-id="7d3a5-132">RxHadoopMR()</span><span class="sxs-lookup"><span data-stu-id="7d3a5-132">RxHadoopMR()</span></span>                    | <span data-ttu-id="7d3a5-133">Parallelized distributed execution via Map Reduce across the nodes of the HDI cluster</span><span class="sxs-lookup"><span data-stu-id="7d3a5-133">Parallelized distributed execution via Map Reduce across the nodes of the HDI cluster</span></span> |

<span data-ttu-id="7d3a5-134">Assuming that you’d like parallelized execution for the purposes of performance, then there are three options.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-134">Assuming that you’d like parallelized execution for the purposes of performance, then there are three options.</span></span> <span data-ttu-id="7d3a5-135">Which option you choose depends on the nature of your analytics work, and the size and location of your data.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-135">Which option you choose depends on the nature of your analytics work, and the size and location of your data.</span></span>

## <a name="guidelines-for-deciding-on-a-compute-context"></a><span data-ttu-id="7d3a5-136">Guidelines for deciding on a compute context</span><span class="sxs-lookup"><span data-stu-id="7d3a5-136">Guidelines for deciding on a compute context</span></span>
<span data-ttu-id="7d3a5-137">Currently, there is no formula that tells you which compute context to use.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-137">Currently, there is no formula that tells you which compute context to use.</span></span> <span data-ttu-id="7d3a5-138">There are, however, some guiding principles that can help you make the right choice, or at least help you narrow down your choices before you run a benchmark.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-138">There are, however, some guiding principles that can help you make the right choice, or at least help you narrow down your choices before you run a benchmark.</span></span> <span data-ttu-id="7d3a5-139">These guiding principles include:</span><span class="sxs-lookup"><span data-stu-id="7d3a5-139">These guiding principles include:</span></span>

1. <span data-ttu-id="7d3a5-140">The local Linux file system is faster than HDFS.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-140">The local Linux file system is faster than HDFS.</span></span>
2. <span data-ttu-id="7d3a5-141">Repeated analyses are faster if the data is local, and if it's in XDF.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-141">Repeated analyses are faster if the data is local, and if it's in XDF.</span></span>
3. <span data-ttu-id="7d3a5-142">It's preferable to stream small amounts of data from a text data source; if the amount of data is larger, convert it to XDF prior to analysis.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-142">It's preferable to stream small amounts of data from a text data source; if the amount of data is larger, convert it to XDF prior to analysis.</span></span>
4. <span data-ttu-id="7d3a5-143">The overhead of copying or streaming the data to the edge node for analysis becomes unmanageable for very large amounts of data.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-143">The overhead of copying or streaming the data to the edge node for analysis becomes unmanageable for very large amounts of data.</span></span>
5. <span data-ttu-id="7d3a5-144">Spark is faster than Map Reduce for analysis in Hadoop.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-144">Spark is faster than Map Reduce for analysis in Hadoop.</span></span>

<span data-ttu-id="7d3a5-145">Given these principles, some general rules of thumb for selecting a compute context are:</span><span class="sxs-lookup"><span data-stu-id="7d3a5-145">Given these principles, some general rules of thumb for selecting a compute context are:</span></span>

### <a name="local"></a><span data-ttu-id="7d3a5-146">Local</span><span class="sxs-lookup"><span data-stu-id="7d3a5-146">Local</span></span>
* <span data-ttu-id="7d3a5-147">If the amount of data to analyze is small and does not require repeated analysis, then stream it directly into the analysis routine and use 'local' or 'localpar'.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-147">If the amount of data to analyze is small and does not require repeated analysis, then stream it directly into the analysis routine and use 'local' or 'localpar'.</span></span>
* <span data-ttu-id="7d3a5-148">If the amount of data to analyze is small or medium-sized and requires repeated analysis, then copy it to the local file system, import it to XDF, and analyze it via 'local' or 'localpar'.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-148">If the amount of data to analyze is small or medium-sized and requires repeated analysis, then copy it to the local file system, import it to XDF, and analyze it via 'local' or 'localpar'.</span></span>

### <a name="hadoop-spark"></a><span data-ttu-id="7d3a5-149">Hadoop Spark</span><span class="sxs-lookup"><span data-stu-id="7d3a5-149">Hadoop Spark</span></span>
* <span data-ttu-id="7d3a5-150">If the amount of data to analyze is large, then then import it to a Spark DataFrame using RxHiveData or RxParquetData, or to XDF in HDFS (unless storage is an issue), and analyze it via ‘Spark’.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-150">If the amount of data to analyze is large, then then import it to a Spark DataFrame using RxHiveData or RxParquetData, or to XDF in HDFS (unless storage is an issue), and analyze it via ‘Spark’.</span></span>

### <a name="hadoop-map-reduce"></a><span data-ttu-id="7d3a5-151">Hadoop Map Reduce</span><span class="sxs-lookup"><span data-stu-id="7d3a5-151">Hadoop Map Reduce</span></span>
* <span data-ttu-id="7d3a5-152">Use only if you encounter an insurmountable problem with use of the Spark compute context since generally it will be slower.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-152">Use only if you encounter an insurmountable problem with use of the Spark compute context since generally it will be slower.</span></span>  

## <a name="inline-help-on-rxsetcomputecontext"></a><span data-ttu-id="7d3a5-153">Inline help on rxSetComputeContext</span><span class="sxs-lookup"><span data-stu-id="7d3a5-153">Inline help on rxSetComputeContext</span></span>
<span data-ttu-id="7d3a5-154">For more information and examples of ScaleR compute contexts, see the inline help in R on the rxSetComputeContext method, for example:</span><span class="sxs-lookup"><span data-stu-id="7d3a5-154">For more information and examples of ScaleR compute contexts, see the inline help in R on the rxSetComputeContext method, for example:</span></span>

    > ?rxSetComputeContext

<span data-ttu-id="7d3a5-155">You can also refer to the “[ScaleR Distributed Computing Guide](https://msdn.microsoft.com/microsoft-r/scaler-distributed-computing)” that's available from the [R Server MSDN](https://msdn.microsoft.com/library/mt674634.aspx "R Server on MSDN") library.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-155">You can also refer to the “[ScaleR Distributed Computing Guide](https://msdn.microsoft.com/microsoft-r/scaler-distributed-computing)” that's available from the [R Server MSDN](https://msdn.microsoft.com/library/mt674634.aspx "R Server on MSDN") library.</span></span>

## <a name="next-steps"></a><span data-ttu-id="7d3a5-156">Next steps</span><span class="sxs-lookup"><span data-stu-id="7d3a5-156">Next steps</span></span>
<span data-ttu-id="7d3a5-157">In this article, you learned how to create a new HDInsight cluster that includes R Server.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-157">In this article, you learned how to create a new HDInsight cluster that includes R Server.</span></span> <span data-ttu-id="7d3a5-158">You also learned the basics of using the R console from an SSH session.</span><span class="sxs-lookup"><span data-stu-id="7d3a5-158">You also learned the basics of using the R console from an SSH session.</span></span> <span data-ttu-id="7d3a5-159">Now you can read the following articles to discover other ways of working with R Server on HDInsight:</span><span class="sxs-lookup"><span data-stu-id="7d3a5-159">Now you can read the following articles to discover other ways of working with R Server on HDInsight:</span></span>

* [<span data-ttu-id="7d3a5-160">Overview of R Server for Hadoop</span><span class="sxs-lookup"><span data-stu-id="7d3a5-160">Overview of R Server for Hadoop</span></span>](hdinsight-hadoop-r-server-overview.md)
* [<span data-ttu-id="7d3a5-161">Get started with R Server for Hadoop</span><span class="sxs-lookup"><span data-stu-id="7d3a5-161">Get started with R Server for Hadoop</span></span>](hdinsight-hadoop-r-server-get-started.md)
* [<span data-ttu-id="7d3a5-162">Add RStudio Server to HDInsight (if not added during cluster creation)</span><span class="sxs-lookup"><span data-stu-id="7d3a5-162">Add RStudio Server to HDInsight (if not added during cluster creation)</span></span>](hdinsight-hadoop-r-server-install-r-studio.md)
* [<span data-ttu-id="7d3a5-163">Azure Storage options for R Server on HDInsight</span><span class="sxs-lookup"><span data-stu-id="7d3a5-163">Azure Storage options for R Server on HDInsight</span></span>](hdinsight-hadoop-r-server-storage.md)

