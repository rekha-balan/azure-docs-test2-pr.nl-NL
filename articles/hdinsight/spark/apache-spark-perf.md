---
title: Optimize Spark jobs for performance - Azure HDInsight
description: Shows common strategies for the best performance of Spark clusters.
services: hdinsight
ms.service: hdinsight
author: maxluk
ms.author: maxluk
ms.reviewer: jasonh
ms.custom: hdinsightactive
ms.topic: conceptual
ms.date: 01/11/2018
ms.openlocfilehash: 4a7777be01cc15ed5cc4c9c091230afe1ddfa897
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44856842"
---
# <a name="optimize-spark-jobs"></a><span data-ttu-id="d15d2-103">Optimize Spark jobs</span><span class="sxs-lookup"><span data-stu-id="d15d2-103">Optimize Spark jobs</span></span>

<span data-ttu-id="d15d2-104">Learn how to optimize Spark cluster configuration for your particular workload.</span><span class="sxs-lookup"><span data-stu-id="d15d2-104">Learn how to optimize Spark cluster configuration for your particular workload.</span></span>  <span data-ttu-id="d15d2-105">The most common challenge is memory pressure, due to improper configurations (particularly wrong-sized executors), long-running operations, and tasks that result in Cartesian operations.</span><span class="sxs-lookup"><span data-stu-id="d15d2-105">The most common challenge is memory pressure, due to improper configurations (particularly wrong-sized executors), long-running operations, and tasks that result in Cartesian operations.</span></span> <span data-ttu-id="d15d2-106">You can speed up jobs with appropriate caching, and by allowing for [data skew](#optimize-joins-and-shuffles).</span><span class="sxs-lookup"><span data-stu-id="d15d2-106">You can speed up jobs with appropriate caching, and by allowing for [data skew](#optimize-joins-and-shuffles).</span></span> <span data-ttu-id="d15d2-107">For the best performance, monitor and review long-running and resource-consuming Spark job executions.</span><span class="sxs-lookup"><span data-stu-id="d15d2-107">For the best performance, monitor and review long-running and resource-consuming Spark job executions.</span></span>

<span data-ttu-id="d15d2-108">The following sections describe common Spark job optimizations and recommendations.</span><span class="sxs-lookup"><span data-stu-id="d15d2-108">The following sections describe common Spark job optimizations and recommendations.</span></span>

## <a name="choose-the-data-abstraction"></a><span data-ttu-id="d15d2-109">Choose the data abstraction</span><span class="sxs-lookup"><span data-stu-id="d15d2-109">Choose the data abstraction</span></span>

<span data-ttu-id="d15d2-110">Spark 1.x uses RDDs to abstract data, and then Spark 2.x introduced DataFrames and DataSets.</span><span class="sxs-lookup"><span data-stu-id="d15d2-110">Spark 1.x uses RDDs to abstract data, and then Spark 2.x introduced DataFrames and DataSets.</span></span> <span data-ttu-id="d15d2-111">Consider the following relative merits:</span><span class="sxs-lookup"><span data-stu-id="d15d2-111">Consider the following relative merits:</span></span>

* <span data-ttu-id="d15d2-112">**DataFrames**</span><span class="sxs-lookup"><span data-stu-id="d15d2-112">**DataFrames**</span></span>
    * <span data-ttu-id="d15d2-113">Best choice in most situations</span><span class="sxs-lookup"><span data-stu-id="d15d2-113">Best choice in most situations</span></span>
    * <span data-ttu-id="d15d2-114">Provides query optimization through Catalyst</span><span class="sxs-lookup"><span data-stu-id="d15d2-114">Provides query optimization through Catalyst</span></span>
    * <span data-ttu-id="d15d2-115">Whole-stage code generation</span><span class="sxs-lookup"><span data-stu-id="d15d2-115">Whole-stage code generation</span></span>
    * <span data-ttu-id="d15d2-116">Direct memory access</span><span class="sxs-lookup"><span data-stu-id="d15d2-116">Direct memory access</span></span>
    * <span data-ttu-id="d15d2-117">Low garbage collection (GC) overhead</span><span class="sxs-lookup"><span data-stu-id="d15d2-117">Low garbage collection (GC) overhead</span></span>
    * <span data-ttu-id="d15d2-118">Not as developer-friendly as DataSets, as there are no compile-time checks or domain object programming</span><span class="sxs-lookup"><span data-stu-id="d15d2-118">Not as developer-friendly as DataSets, as there are no compile-time checks or domain object programming</span></span>
* <span data-ttu-id="d15d2-119">**DataSets**</span><span class="sxs-lookup"><span data-stu-id="d15d2-119">**DataSets**</span></span>
    * <span data-ttu-id="d15d2-120">Good in complex ETL pipelines where the performance impact is acceptable</span><span class="sxs-lookup"><span data-stu-id="d15d2-120">Good in complex ETL pipelines where the performance impact is acceptable</span></span>
    * <span data-ttu-id="d15d2-121">Not good in aggregations where the performance impact can be considerable</span><span class="sxs-lookup"><span data-stu-id="d15d2-121">Not good in aggregations where the performance impact can be considerable</span></span>
    * <span data-ttu-id="d15d2-122">Provides query optimization through Catalyst</span><span class="sxs-lookup"><span data-stu-id="d15d2-122">Provides query optimization through Catalyst</span></span>
    * <span data-ttu-id="d15d2-123">Developer-friendly by providing domain object programming and compile-time checks</span><span class="sxs-lookup"><span data-stu-id="d15d2-123">Developer-friendly by providing domain object programming and compile-time checks</span></span>
    * <span data-ttu-id="d15d2-124">Adds serialization/deserialization overhead</span><span class="sxs-lookup"><span data-stu-id="d15d2-124">Adds serialization/deserialization overhead</span></span>
    * <span data-ttu-id="d15d2-125">High GC overhead</span><span class="sxs-lookup"><span data-stu-id="d15d2-125">High GC overhead</span></span>
    * <span data-ttu-id="d15d2-126">Breaks whole-stage code generation</span><span class="sxs-lookup"><span data-stu-id="d15d2-126">Breaks whole-stage code generation</span></span>
* <span data-ttu-id="d15d2-127">**RDDs**</span><span class="sxs-lookup"><span data-stu-id="d15d2-127">**RDDs**</span></span>
    * <span data-ttu-id="d15d2-128">In Spark 2.x, you do not need to use RDDs, unless you need to build a new custom RDD</span><span class="sxs-lookup"><span data-stu-id="d15d2-128">In Spark 2.x, you do not need to use RDDs, unless you need to build a new custom RDD</span></span>
    * <span data-ttu-id="d15d2-129">No query optimization through Catalyst</span><span class="sxs-lookup"><span data-stu-id="d15d2-129">No query optimization through Catalyst</span></span>
    * <span data-ttu-id="d15d2-130">No whole-stage code generation</span><span class="sxs-lookup"><span data-stu-id="d15d2-130">No whole-stage code generation</span></span>
    * <span data-ttu-id="d15d2-131">High GC overhead</span><span class="sxs-lookup"><span data-stu-id="d15d2-131">High GC overhead</span></span>
    * <span data-ttu-id="d15d2-132">Must use Spark 1.x legacy APIs</span><span class="sxs-lookup"><span data-stu-id="d15d2-132">Must use Spark 1.x legacy APIs</span></span>

## <a name="use-optimal-data-format"></a><span data-ttu-id="d15d2-133">Use optimal data format</span><span class="sxs-lookup"><span data-stu-id="d15d2-133">Use optimal data format</span></span>

<span data-ttu-id="d15d2-134">Spark supports many formats, such as csv, json, xml, parquet, orc, and avro.</span><span class="sxs-lookup"><span data-stu-id="d15d2-134">Spark supports many formats, such as csv, json, xml, parquet, orc, and avro.</span></span> <span data-ttu-id="d15d2-135">Spark can be extended to support many more formats with external data sources - for more information, see [Spark packages](https://spark-packages.org).</span><span class="sxs-lookup"><span data-stu-id="d15d2-135">Spark can be extended to support many more formats with external data sources - for more information, see [Spark packages](https://spark-packages.org).</span></span>

<span data-ttu-id="d15d2-136">The best format for performance is parquet with *snappy compression*, which is the default in Spark 2.x.</span><span class="sxs-lookup"><span data-stu-id="d15d2-136">The best format for performance is parquet with *snappy compression*, which is the default in Spark 2.x.</span></span> <span data-ttu-id="d15d2-137">Parquet stores data in columnar format, and is highly optimized in Spark.</span><span class="sxs-lookup"><span data-stu-id="d15d2-137">Parquet stores data in columnar format, and is highly optimized in Spark.</span></span>

## <a name="select-default-storage"></a><span data-ttu-id="d15d2-138">Select default storage</span><span class="sxs-lookup"><span data-stu-id="d15d2-138">Select default storage</span></span>

<span data-ttu-id="d15d2-139">When you create a new Spark cluster, you have the option to select Azure Blob Storage or Azure Data Lake Store as your cluster's default storage.</span><span class="sxs-lookup"><span data-stu-id="d15d2-139">When you create a new Spark cluster, you have the option to select Azure Blob Storage or Azure Data Lake Store as your cluster's default storage.</span></span> <span data-ttu-id="d15d2-140">Both options give you the benefit of long-term storage for transient clusters, so your data does not get automatically deleted when you delete your cluster.</span><span class="sxs-lookup"><span data-stu-id="d15d2-140">Both options give you the benefit of long-term storage for transient clusters, so your data does not get automatically deleted when you delete your cluster.</span></span> <span data-ttu-id="d15d2-141">You can recreate a transient cluster and still access your data.</span><span class="sxs-lookup"><span data-stu-id="d15d2-141">You can recreate a transient cluster and still access your data.</span></span>

| <span data-ttu-id="d15d2-142">Store Type</span><span class="sxs-lookup"><span data-stu-id="d15d2-142">Store Type</span></span> | <span data-ttu-id="d15d2-143">File System</span><span class="sxs-lookup"><span data-stu-id="d15d2-143">File System</span></span> | <span data-ttu-id="d15d2-144">Speed</span><span class="sxs-lookup"><span data-stu-id="d15d2-144">Speed</span></span> | <span data-ttu-id="d15d2-145">Transient</span><span class="sxs-lookup"><span data-stu-id="d15d2-145">Transient</span></span> | <span data-ttu-id="d15d2-146">Use Cases</span><span class="sxs-lookup"><span data-stu-id="d15d2-146">Use Cases</span></span> |
| --- | --- | --- | --- | --- |
| <span data-ttu-id="d15d2-147">Azure Blob Storage</span><span class="sxs-lookup"><span data-stu-id="d15d2-147">Azure Blob Storage</span></span> | <span data-ttu-id="d15d2-148">**wasb:**//url/</span><span class="sxs-lookup"><span data-stu-id="d15d2-148">**wasb:**//url/</span></span> | <span data-ttu-id="d15d2-149">**Standard**</span><span class="sxs-lookup"><span data-stu-id="d15d2-149">**Standard**</span></span> | <span data-ttu-id="d15d2-150">Yes</span><span class="sxs-lookup"><span data-stu-id="d15d2-150">Yes</span></span> | <span data-ttu-id="d15d2-151">Transient cluster</span><span class="sxs-lookup"><span data-stu-id="d15d2-151">Transient cluster</span></span> |
| <span data-ttu-id="d15d2-152">Azure Data Lake Store</span><span class="sxs-lookup"><span data-stu-id="d15d2-152">Azure Data Lake Store</span></span> | <span data-ttu-id="d15d2-153">**adl:**//url/</span><span class="sxs-lookup"><span data-stu-id="d15d2-153">**adl:**//url/</span></span> | <span data-ttu-id="d15d2-154">**Faster**</span><span class="sxs-lookup"><span data-stu-id="d15d2-154">**Faster**</span></span> | <span data-ttu-id="d15d2-155">Yes</span><span class="sxs-lookup"><span data-stu-id="d15d2-155">Yes</span></span> | <span data-ttu-id="d15d2-156">Transient cluster</span><span class="sxs-lookup"><span data-stu-id="d15d2-156">Transient cluster</span></span> |
| <span data-ttu-id="d15d2-157">Local HDFS</span><span class="sxs-lookup"><span data-stu-id="d15d2-157">Local HDFS</span></span> | <span data-ttu-id="d15d2-158">**hdfs:**//url/</span><span class="sxs-lookup"><span data-stu-id="d15d2-158">**hdfs:**//url/</span></span> | <span data-ttu-id="d15d2-159">**Fastest**</span><span class="sxs-lookup"><span data-stu-id="d15d2-159">**Fastest**</span></span> | <span data-ttu-id="d15d2-160">No</span><span class="sxs-lookup"><span data-stu-id="d15d2-160">No</span></span> | <span data-ttu-id="d15d2-161">Interactive 24/7 cluster</span><span class="sxs-lookup"><span data-stu-id="d15d2-161">Interactive 24/7 cluster</span></span> |

## <a name="use-the-cache"></a><span data-ttu-id="d15d2-162">Use the cache</span><span class="sxs-lookup"><span data-stu-id="d15d2-162">Use the cache</span></span>

<span data-ttu-id="d15d2-163">Spark provides its own native caching mechanisms, which can be used through different methods such as `.persist()`, `.cache()`, and `CACHE TABLE`.</span><span class="sxs-lookup"><span data-stu-id="d15d2-163">Spark provides its own native caching mechanisms, which can be used through different methods such as `.persist()`, `.cache()`, and `CACHE TABLE`.</span></span> <span data-ttu-id="d15d2-164">This native caching is effective with small data sets as well as in ETL pipelines where you need to cache intermediate results.</span><span class="sxs-lookup"><span data-stu-id="d15d2-164">This native caching is effective with small data sets as well as in ETL pipelines where you need to cache intermediate results.</span></span> <span data-ttu-id="d15d2-165">However, Spark native caching currently does not work well with partitioning, since a cached table does not retain the partitioning data.</span><span class="sxs-lookup"><span data-stu-id="d15d2-165">However, Spark native caching currently does not work well with partitioning, since a cached table does not retain the partitioning data.</span></span> <span data-ttu-id="d15d2-166">A more generic and reliable caching technique is *storage layer caching*.</span><span class="sxs-lookup"><span data-stu-id="d15d2-166">A more generic and reliable caching technique is *storage layer caching*.</span></span>

* <span data-ttu-id="d15d2-167">Native Spark caching (not recommended)</span><span class="sxs-lookup"><span data-stu-id="d15d2-167">Native Spark caching (not recommended)</span></span>
    * <span data-ttu-id="d15d2-168">Good for small datasets.</span><span class="sxs-lookup"><span data-stu-id="d15d2-168">Good for small datasets.</span></span>
    * <span data-ttu-id="d15d2-169">Does not work with partitioning, which may change in future Spark releases.</span><span class="sxs-lookup"><span data-stu-id="d15d2-169">Does not work with partitioning, which may change in future Spark releases.</span></span>

* <span data-ttu-id="d15d2-170">Storage level caching (recommended)</span><span class="sxs-lookup"><span data-stu-id="d15d2-170">Storage level caching (recommended)</span></span>
    * <span data-ttu-id="d15d2-171">Can be implemented using [Alluxio](http://www.alluxio.org/).</span><span class="sxs-lookup"><span data-stu-id="d15d2-171">Can be implemented using [Alluxio](http://www.alluxio.org/).</span></span>
    * <span data-ttu-id="d15d2-172">Uses in-memory and SSD caching.</span><span class="sxs-lookup"><span data-stu-id="d15d2-172">Uses in-memory and SSD caching.</span></span>

* <span data-ttu-id="d15d2-173">Local HDFS (recommended)</span><span class="sxs-lookup"><span data-stu-id="d15d2-173">Local HDFS (recommended)</span></span>
    * <span data-ttu-id="d15d2-174">`hdfs://mycluster` path.</span><span class="sxs-lookup"><span data-stu-id="d15d2-174">`hdfs://mycluster` path.</span></span>
    * <span data-ttu-id="d15d2-175">Uses SSD caching.</span><span class="sxs-lookup"><span data-stu-id="d15d2-175">Uses SSD caching.</span></span>
    * <span data-ttu-id="d15d2-176">Cached data will be lost when you delete the cluster, requiring a cache rebuild.</span><span class="sxs-lookup"><span data-stu-id="d15d2-176">Cached data will be lost when you delete the cluster, requiring a cache rebuild.</span></span>

## <a name="use-memory-efficiently"></a><span data-ttu-id="d15d2-177">Use memory efficiently</span><span class="sxs-lookup"><span data-stu-id="d15d2-177">Use memory efficiently</span></span>

<span data-ttu-id="d15d2-178">Spark operates by placing data in memory, so managing memory resources is a key aspect of optimizing the execution of Spark jobs.</span><span class="sxs-lookup"><span data-stu-id="d15d2-178">Spark operates by placing data in memory, so managing memory resources is a key aspect of optimizing the execution of Spark jobs.</span></span>  <span data-ttu-id="d15d2-179">There are several techniques you can apply to use your cluster's memory efficiently.</span><span class="sxs-lookup"><span data-stu-id="d15d2-179">There are several techniques you can apply to use your cluster's memory efficiently.</span></span>

* <span data-ttu-id="d15d2-180">Prefer smaller data partitions and account for data size, types, and distribution in your partitioning strategy.</span><span class="sxs-lookup"><span data-stu-id="d15d2-180">Prefer smaller data partitions and account for data size, types, and distribution in your partitioning strategy.</span></span>
* <span data-ttu-id="d15d2-181">Consider the newer, more efficient [Kryo data serialization](https://github.com/EsotericSoftware/kryo), rather than the default Java serialization.</span><span class="sxs-lookup"><span data-stu-id="d15d2-181">Consider the newer, more efficient [Kryo data serialization](https://github.com/EsotericSoftware/kryo), rather than the default Java serialization.</span></span>
* <span data-ttu-id="d15d2-182">Prefer using YARN, as it separates `spark-submit` by batch.</span><span class="sxs-lookup"><span data-stu-id="d15d2-182">Prefer using YARN, as it separates `spark-submit` by batch.</span></span>
* <span data-ttu-id="d15d2-183">Monitor and tune Spark configuration settings.</span><span class="sxs-lookup"><span data-stu-id="d15d2-183">Monitor and tune Spark configuration settings.</span></span>

<span data-ttu-id="d15d2-184">For your reference, the Spark memory structure and some key executor memory parameters are shown in the next image.</span><span class="sxs-lookup"><span data-stu-id="d15d2-184">For your reference, the Spark memory structure and some key executor memory parameters are shown in the next image.</span></span>

### <a name="spark-memory-considerations"></a><span data-ttu-id="d15d2-185">Spark memory considerations</span><span class="sxs-lookup"><span data-stu-id="d15d2-185">Spark memory considerations</span></span>

<span data-ttu-id="d15d2-186">If you are using YARN, then YARN controls the maximum sum of memory used by all containers on each Spark node.</span><span class="sxs-lookup"><span data-stu-id="d15d2-186">If you are using YARN, then YARN controls the maximum sum of memory used by all containers on each Spark node.</span></span>  <span data-ttu-id="d15d2-187">The following diagram shows the key objects and their relationships.</span><span class="sxs-lookup"><span data-stu-id="d15d2-187">The following diagram shows the key objects and their relationships.</span></span>

![YARN Spark Memory Management](./media/apache-spark-perf/yarn-spark-memory.png)

<span data-ttu-id="d15d2-189">To address 'out of memory' messages, try:</span><span class="sxs-lookup"><span data-stu-id="d15d2-189">To address 'out of memory' messages, try:</span></span>

* <span data-ttu-id="d15d2-190">Review DAG Management Shuffles.</span><span class="sxs-lookup"><span data-stu-id="d15d2-190">Review DAG Management Shuffles.</span></span> <span data-ttu-id="d15d2-191">Reduce by map-side reducting, pre-partition (or bucketize) source data, maximize single shuffles, and reduce the amount of data sent.</span><span class="sxs-lookup"><span data-stu-id="d15d2-191">Reduce by map-side reducting, pre-partition (or bucketize) source data, maximize single shuffles, and reduce the amount of data sent.</span></span>
* <span data-ttu-id="d15d2-192">Prefer `ReduceByKey` with its fixed memory limit to `GroupByKey`, which provides aggregations, windowing, and other functions but it has ann unbounded memory limit.</span><span class="sxs-lookup"><span data-stu-id="d15d2-192">Prefer `ReduceByKey` with its fixed memory limit to `GroupByKey`, which provides aggregations, windowing, and other functions but it has ann unbounded memory limit.</span></span>
* <span data-ttu-id="d15d2-193">Prefer `TreeReduce`, which does more work on the executors or partitions, to `Reduce`, which does all work on the driver.</span><span class="sxs-lookup"><span data-stu-id="d15d2-193">Prefer `TreeReduce`, which does more work on the executors or partitions, to `Reduce`, which does all work on the driver.</span></span>
* <span data-ttu-id="d15d2-194">Leverage DataFrames rather than the lower-level RDD objects.</span><span class="sxs-lookup"><span data-stu-id="d15d2-194">Leverage DataFrames rather than the lower-level RDD objects.</span></span>
* <span data-ttu-id="d15d2-195">Create ComplexTypes that encapsulate actions, such as "Top N", various aggregations, or windowing operations.</span><span class="sxs-lookup"><span data-stu-id="d15d2-195">Create ComplexTypes that encapsulate actions, such as "Top N", various aggregations, or windowing operations.</span></span>

## <a name="optimize-data-serialization"></a><span data-ttu-id="d15d2-196">Optimize data serialization</span><span class="sxs-lookup"><span data-stu-id="d15d2-196">Optimize data serialization</span></span>

<span data-ttu-id="d15d2-197">Spark jobs are distributed, so appropriate data serialization is important for the best performance.</span><span class="sxs-lookup"><span data-stu-id="d15d2-197">Spark jobs are distributed, so appropriate data serialization is important for the best performance.</span></span>  <span data-ttu-id="d15d2-198">There are two serialization options for Spark:</span><span class="sxs-lookup"><span data-stu-id="d15d2-198">There are two serialization options for Spark:</span></span>

* <span data-ttu-id="d15d2-199">Java serialization is the default.</span><span class="sxs-lookup"><span data-stu-id="d15d2-199">Java serialization is the default.</span></span>
* <span data-ttu-id="d15d2-200">Kryo serialization is a newer format and can result in faster and more compact serialization than Java.</span><span class="sxs-lookup"><span data-stu-id="d15d2-200">Kryo serialization is a newer format and can result in faster and more compact serialization than Java.</span></span>  <span data-ttu-id="d15d2-201">Kryo requires that you register the classes in your program, and it does not yet support all Serializable types.</span><span class="sxs-lookup"><span data-stu-id="d15d2-201">Kryo requires that you register the classes in your program, and it does not yet support all Serializable types.</span></span>

## <a name="use-bucketing"></a><span data-ttu-id="d15d2-202">Use bucketing</span><span class="sxs-lookup"><span data-stu-id="d15d2-202">Use bucketing</span></span>

<span data-ttu-id="d15d2-203">Bucketing is similar to data partitioning, but each bucket can hold a set of column values rather than just one.</span><span class="sxs-lookup"><span data-stu-id="d15d2-203">Bucketing is similar to data partitioning, but each bucket can hold a set of column values rather than just one.</span></span> <span data-ttu-id="d15d2-204">Bucketing works well for partitioning on large (in the millions or more) numbers of values, such as product identifiers.</span><span class="sxs-lookup"><span data-stu-id="d15d2-204">Bucketing works well for partitioning on large (in the millions or more) numbers of values, such as product identifiers.</span></span> <span data-ttu-id="d15d2-205">A bucket is determined by hashing the bucket key of the row.</span><span class="sxs-lookup"><span data-stu-id="d15d2-205">A bucket is determined by hashing the bucket key of the row.</span></span> <span data-ttu-id="d15d2-206">Bucketed tables offer unique optimizations because they store metadata about how they were bucketed and sorted.</span><span class="sxs-lookup"><span data-stu-id="d15d2-206">Bucketed tables offer unique optimizations because they store metadata about how they were bucketed and sorted.</span></span>

<span data-ttu-id="d15d2-207">Some advanced bucketing features are:</span><span class="sxs-lookup"><span data-stu-id="d15d2-207">Some advanced bucketing features are:</span></span>

* <span data-ttu-id="d15d2-208">Query optimization based on bucketing meta-information</span><span class="sxs-lookup"><span data-stu-id="d15d2-208">Query optimization based on bucketing meta-information</span></span>
* <span data-ttu-id="d15d2-209">Optimized aggregations</span><span class="sxs-lookup"><span data-stu-id="d15d2-209">Optimized aggregations</span></span>
* <span data-ttu-id="d15d2-210">Optimized joins</span><span class="sxs-lookup"><span data-stu-id="d15d2-210">Optimized joins</span></span>

<span data-ttu-id="d15d2-211">You can use partitioning and bucketing at the same time.</span><span class="sxs-lookup"><span data-stu-id="d15d2-211">You can use partitioning and bucketing at the same time.</span></span>

## <a name="optimize-joins-and-shuffles"></a><span data-ttu-id="d15d2-212">Optimize joins and shuffles</span><span class="sxs-lookup"><span data-stu-id="d15d2-212">Optimize joins and shuffles</span></span>

<span data-ttu-id="d15d2-213">If you have slow jobs on a Join or Shuffle, the cause is probably *data skew*, which is asymmetry in your job data.</span><span class="sxs-lookup"><span data-stu-id="d15d2-213">If you have slow jobs on a Join or Shuffle, the cause is probably *data skew*, which is asymmetry in your job data.</span></span> <span data-ttu-id="d15d2-214">For example, a map job may take 20 seconds, but running a job where the data is joined or shuffled takes hours.</span><span class="sxs-lookup"><span data-stu-id="d15d2-214">For example, a map job may take 20 seconds, but running a job where the data is joined or shuffled takes hours.</span></span>   <span data-ttu-id="d15d2-215">To fix data skew, you should salt the entire key, or use an *isolated salt* for  only some subset of keys.</span><span class="sxs-lookup"><span data-stu-id="d15d2-215">To fix data skew, you should salt the entire key, or use an *isolated salt* for  only some subset of keys.</span></span>  <span data-ttu-id="d15d2-216">If you are using an isolated salt, you should further filter to isolate your subset of salted keys in map joins.</span><span class="sxs-lookup"><span data-stu-id="d15d2-216">If you are using an isolated salt, you should further filter to isolate your subset of salted keys in map joins.</span></span> <span data-ttu-id="d15d2-217">Another option is to introduce a bucket column and pre-aggregate in buckets first.</span><span class="sxs-lookup"><span data-stu-id="d15d2-217">Another option is to introduce a bucket column and pre-aggregate in buckets first.</span></span>

<span data-ttu-id="d15d2-218">Another factor causing slow joins could be the join type.</span><span class="sxs-lookup"><span data-stu-id="d15d2-218">Another factor causing slow joins could be the join type.</span></span> <span data-ttu-id="d15d2-219">By default, Spark uses the `SortMerge` join type.</span><span class="sxs-lookup"><span data-stu-id="d15d2-219">By default, Spark uses the `SortMerge` join type.</span></span> <span data-ttu-id="d15d2-220">This type of join is best suited for large data sets, but is otherwise computationally expensive because it must first sort the left and right sides of data before merging them.</span><span class="sxs-lookup"><span data-stu-id="d15d2-220">This type of join is best suited for large data sets, but is otherwise computationally expensive because it must first sort the left and right sides of data before merging them.</span></span>

<span data-ttu-id="d15d2-221">A `Broadcast` join is best suited for smaller data sets, or where one side of the join is much smaller than the other side.</span><span class="sxs-lookup"><span data-stu-id="d15d2-221">A `Broadcast` join is best suited for smaller data sets, or where one side of the join is much smaller than the other side.</span></span> <span data-ttu-id="d15d2-222">This type of join broadcasts one side to all executors, and so requires more memory for broadcasts in general.</span><span class="sxs-lookup"><span data-stu-id="d15d2-222">This type of join broadcasts one side to all executors, and so requires more memory for broadcasts in general.</span></span>

<span data-ttu-id="d15d2-223">You can change the join type in your configuration by setting `spark.sql.autoBroadcastJoinThreshold`, or you can set a join hint using the DataFrame APIs (`dataframe.join(broadcast(df2))`).</span><span class="sxs-lookup"><span data-stu-id="d15d2-223">You can change the join type in your configuration by setting `spark.sql.autoBroadcastJoinThreshold`, or you can set a join hint using the DataFrame APIs (`dataframe.join(broadcast(df2))`).</span></span>

```scala
// Option 1
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 1*1024*1024*1024)

// Option 2
val df1 = spark.table("FactTableA")
val df2 = spark.table("dimMP")
df1.join(broadcast(df2), Seq("PK")).
    createOrReplaceTempView("V_JOIN")
sql("SELECT col1, col2 FROM V_JOIN")
```

<span data-ttu-id="d15d2-224">If you are using bucketed tables, then you have a third join type, the `Merge` join.</span><span class="sxs-lookup"><span data-stu-id="d15d2-224">If you are using bucketed tables, then you have a third join type, the `Merge` join.</span></span> <span data-ttu-id="d15d2-225">A correctly pre-partitioned and pre-sorted dataset will skip the expensive sort phase from a `SortMerge` join.</span><span class="sxs-lookup"><span data-stu-id="d15d2-225">A correctly pre-partitioned and pre-sorted dataset will skip the expensive sort phase from a `SortMerge` join.</span></span>

<span data-ttu-id="d15d2-226">The order of joins matters, particularly in more complex queries.</span><span class="sxs-lookup"><span data-stu-id="d15d2-226">The order of joins matters, particularly in more complex queries.</span></span> <span data-ttu-id="d15d2-227">Start with the most selective joins.</span><span class="sxs-lookup"><span data-stu-id="d15d2-227">Start with the most selective joins.</span></span> <span data-ttu-id="d15d2-228">Also, move joins that increase the number of rows after aggregations when possible.</span><span class="sxs-lookup"><span data-stu-id="d15d2-228">Also, move joins that increase the number of rows after aggregations when possible.</span></span>

<span data-ttu-id="d15d2-229">To manage parallelism, specifically in the case of Cartesian joins, you can add nested structures, windowing, and perhaps skip one or more steps in your Spark Job.</span><span class="sxs-lookup"><span data-stu-id="d15d2-229">To manage parallelism, specifically in the case of Cartesian joins, you can add nested structures, windowing, and perhaps skip one or more steps in your Spark Job.</span></span>

## <a name="customize-cluster-configuration"></a><span data-ttu-id="d15d2-230">Customize cluster configuration</span><span class="sxs-lookup"><span data-stu-id="d15d2-230">Customize cluster configuration</span></span>

<span data-ttu-id="d15d2-231">Depending on your Spark cluster workload, you may determine that a non-default Spark configuration would result in more optimized Spark job execution.</span><span class="sxs-lookup"><span data-stu-id="d15d2-231">Depending on your Spark cluster workload, you may determine that a non-default Spark configuration would result in more optimized Spark job execution.</span></span>  <span data-ttu-id="d15d2-232">Perform benchmark testing with sample workloads to validate any non-default cluster configurations.</span><span class="sxs-lookup"><span data-stu-id="d15d2-232">Perform benchmark testing with sample workloads to validate any non-default cluster configurations.</span></span>

<span data-ttu-id="d15d2-233">Here are some common parameters you can adjust:</span><span class="sxs-lookup"><span data-stu-id="d15d2-233">Here are some common parameters you can adjust:</span></span>

* <span data-ttu-id="d15d2-234">`--num-executors` sets the appropriate number of executors.</span><span class="sxs-lookup"><span data-stu-id="d15d2-234">`--num-executors` sets the appropriate number of executors.</span></span>
* <span data-ttu-id="d15d2-235">`--executor-cores` sets the number of cores for each executor.</span><span class="sxs-lookup"><span data-stu-id="d15d2-235">`--executor-cores` sets the number of cores for each executor.</span></span> <span data-ttu-id="d15d2-236">Typically you should have middle-sized executors, as other processes consume some of the available memory.</span><span class="sxs-lookup"><span data-stu-id="d15d2-236">Typically you should have middle-sized executors, as other processes consume some of the available memory.</span></span>
* <span data-ttu-id="d15d2-237">`--executor-memory` sets the memory size for each executor, which controls the heap size on YARN.</span><span class="sxs-lookup"><span data-stu-id="d15d2-237">`--executor-memory` sets the memory size for each executor, which controls the heap size on YARN.</span></span> <span data-ttu-id="d15d2-238">You should leave some memory for execution overhead.</span><span class="sxs-lookup"><span data-stu-id="d15d2-238">You should leave some memory for execution overhead.</span></span>

### <a name="select-the-correct-executor-size"></a><span data-ttu-id="d15d2-239">Select the correct executor size</span><span class="sxs-lookup"><span data-stu-id="d15d2-239">Select the correct executor size</span></span>

<span data-ttu-id="d15d2-240">When deciding your executor configuration, consider the Java garbage collection (GC) overhead.</span><span class="sxs-lookup"><span data-stu-id="d15d2-240">When deciding your executor configuration, consider the Java garbage collection (GC) overhead.</span></span>

* <span data-ttu-id="d15d2-241">Factors to reduce executor size:</span><span class="sxs-lookup"><span data-stu-id="d15d2-241">Factors to reduce executor size:</span></span>
    1. <span data-ttu-id="d15d2-242">Reduce heap size below 32 GB to keep GC overhead < 10%.</span><span class="sxs-lookup"><span data-stu-id="d15d2-242">Reduce heap size below 32 GB to keep GC overhead < 10%.</span></span>
    2. <span data-ttu-id="d15d2-243">Reduce the number of cores to keep GC overhead < 10%.</span><span class="sxs-lookup"><span data-stu-id="d15d2-243">Reduce the number of cores to keep GC overhead < 10%.</span></span>

* <span data-ttu-id="d15d2-244">Factors to increase executor size:</span><span class="sxs-lookup"><span data-stu-id="d15d2-244">Factors to increase executor size:</span></span>
    1. <span data-ttu-id="d15d2-245">Reduce communication overhead between executors.</span><span class="sxs-lookup"><span data-stu-id="d15d2-245">Reduce communication overhead between executors.</span></span>
    2. <span data-ttu-id="d15d2-246">Reduce the number of open connections between executors (N2) on larger clusters (>100 executors).</span><span class="sxs-lookup"><span data-stu-id="d15d2-246">Reduce the number of open connections between executors (N2) on larger clusters (>100 executors).</span></span>
    3. <span data-ttu-id="d15d2-247">Increase heap size to accommodate for memory-intensive tasks.</span><span class="sxs-lookup"><span data-stu-id="d15d2-247">Increase heap size to accommodate for memory-intensive tasks.</span></span>
    4. <span data-ttu-id="d15d2-248">Optional: Reduce per-executor memory overhead.</span><span class="sxs-lookup"><span data-stu-id="d15d2-248">Optional: Reduce per-executor memory overhead.</span></span>
    5. <span data-ttu-id="d15d2-249">Optional: Increase utilization and concurrency by oversubscribing CPU.</span><span class="sxs-lookup"><span data-stu-id="d15d2-249">Optional: Increase utilization and concurrency by oversubscribing CPU.</span></span>

<span data-ttu-id="d15d2-250">As a general rule of thumb when selecting the executor size:</span><span class="sxs-lookup"><span data-stu-id="d15d2-250">As a general rule of thumb when selecting the executor size:</span></span>
    
1. <span data-ttu-id="d15d2-251">Start with 30 GB per executor and distribute available machine cores.</span><span class="sxs-lookup"><span data-stu-id="d15d2-251">Start with 30 GB per executor and distribute available machine cores.</span></span>
2. <span data-ttu-id="d15d2-252">Increase the number of executor cores for larger clusters (> 100 executors).</span><span class="sxs-lookup"><span data-stu-id="d15d2-252">Increase the number of executor cores for larger clusters (> 100 executors).</span></span>
3. <span data-ttu-id="d15d2-253">Increase or decrease sizes based both on trial runs and on the preceding factors such as GC overhead.</span><span class="sxs-lookup"><span data-stu-id="d15d2-253">Increase or decrease sizes based both on trial runs and on the preceding factors such as GC overhead.</span></span>

<span data-ttu-id="d15d2-254">When running concurrent queries, consider the following:</span><span class="sxs-lookup"><span data-stu-id="d15d2-254">When running concurrent queries, consider the following:</span></span>

1. <span data-ttu-id="d15d2-255">Start with 30 GB per executor and all machine cores.</span><span class="sxs-lookup"><span data-stu-id="d15d2-255">Start with 30 GB per executor and all machine cores.</span></span>
2. <span data-ttu-id="d15d2-256">Create multiple parallel Spark applications by oversubscribing CPU (around 30% latency improvement).</span><span class="sxs-lookup"><span data-stu-id="d15d2-256">Create multiple parallel Spark applications by oversubscribing CPU (around 30% latency improvement).</span></span>
3. <span data-ttu-id="d15d2-257">Distribute queries across parallel applications.</span><span class="sxs-lookup"><span data-stu-id="d15d2-257">Distribute queries across parallel applications.</span></span>
4. <span data-ttu-id="d15d2-258">Increase or decrease sizes based both on trial runs and on the preceding factors such as GC overhead.</span><span class="sxs-lookup"><span data-stu-id="d15d2-258">Increase or decrease sizes based both on trial runs and on the preceding factors such as GC overhead.</span></span>

<span data-ttu-id="d15d2-259">Monitor your query performance for outliers or other performance issues, by looking at the timeline view, SQL graph, job statistics, and so forth.</span><span class="sxs-lookup"><span data-stu-id="d15d2-259">Monitor your query performance for outliers or other performance issues, by looking at the timeline view, SQL graph, job statistics, and so forth.</span></span> <span data-ttu-id="d15d2-260">Sometimes one or a few of the executors are slower than the others, and tasks take much longer to execute.</span><span class="sxs-lookup"><span data-stu-id="d15d2-260">Sometimes one or a few of the executors are slower than the others, and tasks take much longer to execute.</span></span> <span data-ttu-id="d15d2-261">This frequently happens on larger clusters (> 30 nodes).</span><span class="sxs-lookup"><span data-stu-id="d15d2-261">This frequently happens on larger clusters (> 30 nodes).</span></span> <span data-ttu-id="d15d2-262">In this case, divide the work into a larger number of tasks so the scheduler can compensate for slow tasks.</span><span class="sxs-lookup"><span data-stu-id="d15d2-262">In this case, divide the work into a larger number of tasks so the scheduler can compensate for slow tasks.</span></span> <span data-ttu-id="d15d2-263">For example, have at least twice as many tasks as the number of executor cores in the application.</span><span class="sxs-lookup"><span data-stu-id="d15d2-263">For example, have at least twice as many tasks as the number of executor cores in the application.</span></span> <span data-ttu-id="d15d2-264">You can also enable speculative execution of tasks with `conf: spark.speculation = true`.</span><span class="sxs-lookup"><span data-stu-id="d15d2-264">You can also enable speculative execution of tasks with `conf: spark.speculation = true`.</span></span>

## <a name="optimize-job-execution"></a><span data-ttu-id="d15d2-265">Optimize job execution</span><span class="sxs-lookup"><span data-stu-id="d15d2-265">Optimize job execution</span></span>

* <span data-ttu-id="d15d2-266">Cache as necessary, for example if you use the data twice, then cache it.</span><span class="sxs-lookup"><span data-stu-id="d15d2-266">Cache as necessary, for example if you use the data twice, then cache it.</span></span>
* <span data-ttu-id="d15d2-267">Broadcast variables to all executors.</span><span class="sxs-lookup"><span data-stu-id="d15d2-267">Broadcast variables to all executors.</span></span> <span data-ttu-id="d15d2-268">The variables are only serialized once, resulting in faster lookups.</span><span class="sxs-lookup"><span data-stu-id="d15d2-268">The variables are only serialized once, resulting in faster lookups.</span></span>
* <span data-ttu-id="d15d2-269">Use the thread pool on the driver, which results in faster operation for many tasks.</span><span class="sxs-lookup"><span data-stu-id="d15d2-269">Use the thread pool on the driver, which results in faster operation for many tasks.</span></span>

<span data-ttu-id="d15d2-270">Monitor your running jobs regularly for performance issues.</span><span class="sxs-lookup"><span data-stu-id="d15d2-270">Monitor your running jobs regularly for performance issues.</span></span> <span data-ttu-id="d15d2-271">If you need more insight into certain issues, consider one of the following performance profiling tools:</span><span class="sxs-lookup"><span data-stu-id="d15d2-271">If you need more insight into certain issues, consider one of the following performance profiling tools:</span></span>

* <span data-ttu-id="d15d2-272">[Intel PAL Tool](https://github.com/intel-hadoop/PAT) monitors CPU, storage, and network bandwidth utilization.</span><span class="sxs-lookup"><span data-stu-id="d15d2-272">[Intel PAL Tool](https://github.com/intel-hadoop/PAT) monitors CPU, storage, and network bandwidth utilization.</span></span>
* <span data-ttu-id="d15d2-273">[Oracle Java 8 Mission Control](http://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html) profiles Spark and executor code.</span><span class="sxs-lookup"><span data-stu-id="d15d2-273">[Oracle Java 8 Mission Control](http://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html) profiles Spark and executor code.</span></span>

<span data-ttu-id="d15d2-274">Key to Spark 2.x query performance is the Tungsten engine, which depends on whole-stage code generation.</span><span class="sxs-lookup"><span data-stu-id="d15d2-274">Key to Spark 2.x query performance is the Tungsten engine, which depends on whole-stage code generation.</span></span> <span data-ttu-id="d15d2-275">In some cases, whole-stage code generation may be disabled.</span><span class="sxs-lookup"><span data-stu-id="d15d2-275">In some cases, whole-stage code generation may be disabled.</span></span> <span data-ttu-id="d15d2-276">For example, if you use a non-mutable type (`string`) in the aggregation expression, `SortAggregate` appears instead of `HashAggregate`.</span><span class="sxs-lookup"><span data-stu-id="d15d2-276">For example, if you use a non-mutable type (`string`) in the aggregation expression, `SortAggregate` appears instead of `HashAggregate`.</span></span> <span data-ttu-id="d15d2-277">For example, for better performance, try the following and then re-enable code generation:</span><span class="sxs-lookup"><span data-stu-id="d15d2-277">For example, for better performance, try the following and then re-enable code generation:</span></span>

```sql
MAX(AMOUNT) -> MAX(cast(AMOUNT as DOUBLE))
```

## <a name="next-steps"></a><span data-ttu-id="d15d2-278">Next steps</span><span class="sxs-lookup"><span data-stu-id="d15d2-278">Next steps</span></span>

* [<span data-ttu-id="d15d2-279">Debug Spark jobs running on Azure HDInsight</span><span class="sxs-lookup"><span data-stu-id="d15d2-279">Debug Spark jobs running on Azure HDInsight</span></span>](apache-spark-job-debugging.md)
* [<span data-ttu-id="d15d2-280">Manage resources for a Spark cluster on HDInsight</span><span class="sxs-lookup"><span data-stu-id="d15d2-280">Manage resources for a Spark cluster on HDInsight</span></span>](apache-spark-resource-manager.md)
* [<span data-ttu-id="d15d2-281">Use the Spark REST API to submit remote jobs to a Spark cluster</span><span class="sxs-lookup"><span data-stu-id="d15d2-281">Use the Spark REST API to submit remote jobs to a Spark cluster</span></span>](apache-spark-livy-rest-interface.md)
* [<span data-ttu-id="d15d2-282">Tuning Spark</span><span class="sxs-lookup"><span data-stu-id="d15d2-282">Tuning Spark</span></span>](https://spark.apache.org/docs/latest/tuning.html)
* [<span data-ttu-id="d15d2-283">How to Actually Tune Your Spark Jobs So They Work</span><span class="sxs-lookup"><span data-stu-id="d15d2-283">How to Actually Tune Your Spark Jobs So They Work</span></span>](https://www.slideshare.net/ilganeli/how-to-actually-tune-your-spark-jobs-so-they-work)
* [<span data-ttu-id="d15d2-284">Kryo Serialization</span><span class="sxs-lookup"><span data-stu-id="d15d2-284">Kryo Serialization</span></span>](https://github.com/EsotericSoftware/kryo)
