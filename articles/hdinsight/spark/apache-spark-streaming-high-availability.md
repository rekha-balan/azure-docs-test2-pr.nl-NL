---
title: Create highly available Spark Streaming jobs in YARN - Azure HDInsight
description: How to set up Spark Streaming for a high-availability scenario.
services: hdinsight
ms.service: hdinsight
author: jasonwhowell
ms.author: jasonh
ms.reviewer: jasonh
ms.topic: conceptual
ms.custom: hdinsightactive
ms.date: 01/26/2018
ms.openlocfilehash: 0725c70668ca3089028bff5fc1d8374c6bfdecde
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44866053"
---
# <a name="create-high-availability-spark-streaming-jobs-with-yarn"></a><span data-ttu-id="f543a-103">Create high-availability Spark Streaming jobs with YARN</span><span class="sxs-lookup"><span data-stu-id="f543a-103">Create high-availability Spark Streaming jobs with YARN</span></span>

<span data-ttu-id="f543a-104">Spark Streaming enables you to implement scalable, high-throughput, fault-tolerant applications for data streams processing.</span><span class="sxs-lookup"><span data-stu-id="f543a-104">Spark Streaming enables you to implement scalable, high-throughput, fault-tolerant applications for data streams processing.</span></span> <span data-ttu-id="f543a-105">You can connect Spark Streaming applications on a HDInsight Spark cluster to a variety of data sources, such as Azure Event Hubs, Azure IoT Hub, Kafka, Flume, Twitter, ZeroMQ, raw TCP sockets, or by monitoring the HDFS filesystem for changes.</span><span class="sxs-lookup"><span data-stu-id="f543a-105">You can connect Spark Streaming applications on a HDInsight Spark cluster to a variety of data sources, such as Azure Event Hubs, Azure IoT Hub, Kafka, Flume, Twitter, ZeroMQ, raw TCP sockets, or by monitoring the HDFS filesystem for changes.</span></span> <span data-ttu-id="f543a-106">Spark Streaming supports fault tolerance with the guarantee that any given event is processed exactly once, even with a node failure.</span><span class="sxs-lookup"><span data-stu-id="f543a-106">Spark Streaming supports fault tolerance with the guarantee that any given event is processed exactly once, even with a node failure.</span></span>

<span data-ttu-id="f543a-107">Spark Streaming creates long-running jobs during which you are able to apply transformations to the data and then push the results out to filesystems, databases, dashboards, and the console.</span><span class="sxs-lookup"><span data-stu-id="f543a-107">Spark Streaming creates long-running jobs during which you are able to apply transformations to the data and then push the results out to filesystems, databases, dashboards, and the console.</span></span> <span data-ttu-id="f543a-108">Spark Streaming processes micro-batches of data, by first collecting a batch of events over a defined time interval.</span><span class="sxs-lookup"><span data-stu-id="f543a-108">Spark Streaming processes micro-batches of data, by first collecting a batch of events over a defined time interval.</span></span> <span data-ttu-id="f543a-109">Next, that batch is sent on for processing and output.</span><span class="sxs-lookup"><span data-stu-id="f543a-109">Next, that batch is sent on for processing and output.</span></span> <span data-ttu-id="f543a-110">Batch time intervals are typically defined in fractions of a second.</span><span class="sxs-lookup"><span data-stu-id="f543a-110">Batch time intervals are typically defined in fractions of a second.</span></span>

![Spark Streaming](./media/apache-spark-streaming-high-availability/spark-streaming.png)

## <a name="dstreams"></a><span data-ttu-id="f543a-112">DStreams</span><span class="sxs-lookup"><span data-stu-id="f543a-112">DStreams</span></span>

<span data-ttu-id="f543a-113">Spark Streaming represents a continuous stream of data using a *discretized stream* (DStream).</span><span class="sxs-lookup"><span data-stu-id="f543a-113">Spark Streaming represents a continuous stream of data using a *discretized stream* (DStream).</span></span> <span data-ttu-id="f543a-114">This DStream can be created from input sources like Event Hubs or Kafka, or by applying transformations on another DStream.</span><span class="sxs-lookup"><span data-stu-id="f543a-114">This DStream can be created from input sources like Event Hubs or Kafka, or by applying transformations on another DStream.</span></span> <span data-ttu-id="f543a-115">When an event arrives at your Spark Streaming application, the event is stored in a reliable way.</span><span class="sxs-lookup"><span data-stu-id="f543a-115">When an event arrives at your Spark Streaming application, the event is stored in a reliable way.</span></span> <span data-ttu-id="f543a-116">That is, the event data is replicated so that multiple nodes have a copy of it.</span><span class="sxs-lookup"><span data-stu-id="f543a-116">That is, the event data is replicated so that multiple nodes have a copy of it.</span></span> <span data-ttu-id="f543a-117">This ensures that the failure of any single node will not result in the loss of your event.</span><span class="sxs-lookup"><span data-stu-id="f543a-117">This ensures that the failure of any single node will not result in the loss of your event.</span></span>

<span data-ttu-id="f543a-118">The Spark core uses *resilient distributed datasets* (RDDs).</span><span class="sxs-lookup"><span data-stu-id="f543a-118">The Spark core uses *resilient distributed datasets* (RDDs).</span></span> <span data-ttu-id="f543a-119">RDDs distribute data across multiple nodes in the cluster, where each node generally maintains its data completely in-memory for best performance.</span><span class="sxs-lookup"><span data-stu-id="f543a-119">RDDs distribute data across multiple nodes in the cluster, where each node generally maintains its data completely in-memory for best performance.</span></span> <span data-ttu-id="f543a-120">Each RDD represents events collected over a batch interval.</span><span class="sxs-lookup"><span data-stu-id="f543a-120">Each RDD represents events collected over a batch interval.</span></span> <span data-ttu-id="f543a-121">When the batch interval elapses, Spark Streaming produces a new RDD containing all the data in that interval.</span><span class="sxs-lookup"><span data-stu-id="f543a-121">When the batch interval elapses, Spark Streaming produces a new RDD containing all the data in that interval.</span></span> <span data-ttu-id="f543a-122">This continuous set of RDDs is collected into a DStream.</span><span class="sxs-lookup"><span data-stu-id="f543a-122">This continuous set of RDDs is collected into a DStream.</span></span> <span data-ttu-id="f543a-123">A Spark Streaming application processes the data stored in each batch's RDD.</span><span class="sxs-lookup"><span data-stu-id="f543a-123">A Spark Streaming application processes the data stored in each batch's RDD.</span></span>

![Spark DStream](./media/apache-spark-streaming-high-availability/DStream.png)

## <a name="spark-structured-streaming-jobs"></a><span data-ttu-id="f543a-125">Spark Structured Streaming jobs</span><span class="sxs-lookup"><span data-stu-id="f543a-125">Spark Structured Streaming jobs</span></span>

<span data-ttu-id="f543a-126">Spark Structured Streaming was introduced in Spark 2.0 as an analytic engine for use on streaming structured data.</span><span class="sxs-lookup"><span data-stu-id="f543a-126">Spark Structured Streaming was introduced in Spark 2.0 as an analytic engine for use on streaming structured data.</span></span> <span data-ttu-id="f543a-127">Spark Structured Streaming uses the SparkSQL batching engine APIs.</span><span class="sxs-lookup"><span data-stu-id="f543a-127">Spark Structured Streaming uses the SparkSQL batching engine APIs.</span></span> <span data-ttu-id="f543a-128">As with Spark Streaming, Spark Structured Streaming runs its computations over continuously-arriving micro-batches of data.</span><span class="sxs-lookup"><span data-stu-id="f543a-128">As with Spark Streaming, Spark Structured Streaming runs its computations over continuously-arriving micro-batches of data.</span></span> <span data-ttu-id="f543a-129">Spark Structured Streaming represents a stream of data as an Input Table with unlimited rows.</span><span class="sxs-lookup"><span data-stu-id="f543a-129">Spark Structured Streaming represents a stream of data as an Input Table with unlimited rows.</span></span> <span data-ttu-id="f543a-130">That is, the Input Table continues to grow as new data arrives.</span><span class="sxs-lookup"><span data-stu-id="f543a-130">That is, the Input Table continues to grow as new data arrives.</span></span> <span data-ttu-id="f543a-131">This Input Table is continuously processed by a long running query, and the results are written out to an Output Table.</span><span class="sxs-lookup"><span data-stu-id="f543a-131">This Input Table is continuously processed by a long running query, and the results are written out to an Output Table.</span></span>

![Spark Structured Streaming](./media/apache-spark-streaming-high-availability/structured-streaming.png)

<span data-ttu-id="f543a-133">In Structured Streaming, data arrives at the system and is immediately ingested into the Input Table.</span><span class="sxs-lookup"><span data-stu-id="f543a-133">In Structured Streaming, data arrives at the system and is immediately ingested into the Input Table.</span></span> <span data-ttu-id="f543a-134">You write queries that perform operations against this Input Table.</span><span class="sxs-lookup"><span data-stu-id="f543a-134">You write queries that perform operations against this Input Table.</span></span> <span data-ttu-id="f543a-135">The query output yields another table, called the Results Table.</span><span class="sxs-lookup"><span data-stu-id="f543a-135">The query output yields another table, called the Results Table.</span></span> <span data-ttu-id="f543a-136">The Results Table contains results of your query, from which you draw data to send to an external datastore such a relational database.</span><span class="sxs-lookup"><span data-stu-id="f543a-136">The Results Table contains results of your query, from which you draw data to send to an external datastore such a relational database.</span></span> <span data-ttu-id="f543a-137">The *trigger interval* sets the timing for when data is processed from the Input Table.</span><span class="sxs-lookup"><span data-stu-id="f543a-137">The *trigger interval* sets the timing for when data is processed from the Input Table.</span></span> <span data-ttu-id="f543a-138">By default, Structured Streaming processes the data as soon as it arrives.</span><span class="sxs-lookup"><span data-stu-id="f543a-138">By default, Structured Streaming processes the data as soon as it arrives.</span></span> <span data-ttu-id="f543a-139">However, you can also configure the trigger to run on a longer interval, so the streaming data is processed in time-based batches.</span><span class="sxs-lookup"><span data-stu-id="f543a-139">However, you can also configure the trigger to run on a longer interval, so the streaming data is processed in time-based batches.</span></span> <span data-ttu-id="f543a-140">The data in the Results Table may be completely refreshed each time there is new data so that it includes all of the output data since the streaming query began (*complete mode*), or it may only contain just the data that is new since the last time the query was processed (*append mode*).</span><span class="sxs-lookup"><span data-stu-id="f543a-140">The data in the Results Table may be completely refreshed each time there is new data so that it includes all of the output data since the streaming query began (*complete mode*), or it may only contain just the data that is new since the last time the query was processed (*append mode*).</span></span>

## <a name="create-fault-tolerant-spark-streaming-jobs"></a><span data-ttu-id="f543a-141">Create fault-tolerant Spark Streaming jobs</span><span class="sxs-lookup"><span data-stu-id="f543a-141">Create fault-tolerant Spark Streaming jobs</span></span>

<span data-ttu-id="f543a-142">To create a highly-available environment for your Spark Streaming jobs, start by coding your individual jobs for recovery in the event of failure.</span><span class="sxs-lookup"><span data-stu-id="f543a-142">To create a highly-available environment for your Spark Streaming jobs, start by coding your individual jobs for recovery in the event of failure.</span></span> <span data-ttu-id="f543a-143">Such self-recovering jobs are fault-tolerant.</span><span class="sxs-lookup"><span data-stu-id="f543a-143">Such self-recovering jobs are fault-tolerant.</span></span>

<span data-ttu-id="f543a-144">RDDs have several properties that assist highly-available and fault-tolerant Spark Streaming jobs:</span><span class="sxs-lookup"><span data-stu-id="f543a-144">RDDs have several properties that assist highly-available and fault-tolerant Spark Streaming jobs:</span></span>

* <span data-ttu-id="f543a-145">Batches of input data stored in RDDs as a DStream are automatically replicated in memory for fault-tolerance.</span><span class="sxs-lookup"><span data-stu-id="f543a-145">Batches of input data stored in RDDs as a DStream are automatically replicated in memory for fault-tolerance.</span></span>
* <span data-ttu-id="f543a-146">Data lost due to worker failure can be recomputed from replicated input data on different workers, as long as those worker nodes are available.</span><span class="sxs-lookup"><span data-stu-id="f543a-146">Data lost due to worker failure can be recomputed from replicated input data on different workers, as long as those worker nodes are available.</span></span>
* <span data-ttu-id="f543a-147">Fast Fault Recovery can occur within one second, as recovery from faults/stragglers happens via computation in memory.</span><span class="sxs-lookup"><span data-stu-id="f543a-147">Fast Fault Recovery can occur within one second, as recovery from faults/stragglers happens via computation in memory.</span></span>

### <a name="exactly-once-semantics-with-spark-streaming"></a><span data-ttu-id="f543a-148">Exactly-once semantics with Spark Streaming</span><span class="sxs-lookup"><span data-stu-id="f543a-148">Exactly-once semantics with Spark Streaming</span></span>

<span data-ttu-id="f543a-149">To create an application that processes each event once (and only once), consider how all system points of failure restart after having an issue, and how you can avoid data loss.</span><span class="sxs-lookup"><span data-stu-id="f543a-149">To create an application that processes each event once (and only once), consider how all system points of failure restart after having an issue, and how you can avoid data loss.</span></span> <span data-ttu-id="f543a-150">Exactly-once semantics require that no data is lost at any point, and that message processing is restartable, regardless of where the failure occurs.</span><span class="sxs-lookup"><span data-stu-id="f543a-150">Exactly-once semantics require that no data is lost at any point, and that message processing is restartable, regardless of where the failure occurs.</span></span> <span data-ttu-id="f543a-151">See [Create Spark Streaming jobs with exactly-once event processing](apache-spark-streaming-exactly-once.md).</span><span class="sxs-lookup"><span data-stu-id="f543a-151">See [Create Spark Streaming jobs with exactly-once event processing](apache-spark-streaming-exactly-once.md).</span></span>

## <a name="spark-streaming-and-yarn"></a><span data-ttu-id="f543a-152">Spark Streaming and YARN</span><span class="sxs-lookup"><span data-stu-id="f543a-152">Spark Streaming and YARN</span></span>

<span data-ttu-id="f543a-153">In HDInsight, cluster work is coordinated by *Yet Another Resource Negotiator* (YARN).</span><span class="sxs-lookup"><span data-stu-id="f543a-153">In HDInsight, cluster work is coordinated by *Yet Another Resource Negotiator* (YARN).</span></span> <span data-ttu-id="f543a-154">Designing high availability for Spark Streaming includes techniques for Spark Streaming, and also for YARN components.</span><span class="sxs-lookup"><span data-stu-id="f543a-154">Designing high availability for Spark Streaming includes techniques for Spark Streaming, and also for YARN components.</span></span>  <span data-ttu-id="f543a-155">An example configuration using YARN is shown below.</span><span class="sxs-lookup"><span data-stu-id="f543a-155">An example configuration using YARN is shown below.</span></span> 

![YARN Architecture](./media/apache-spark-streaming-high-availability/yarn-arch.png)

<span data-ttu-id="f543a-157">The following sections describe design considerations for this configuration.</span><span class="sxs-lookup"><span data-stu-id="f543a-157">The following sections describe design considerations for this configuration.</span></span>

### <a name="plan-for-failures"></a><span data-ttu-id="f543a-158">Plan for failures</span><span class="sxs-lookup"><span data-stu-id="f543a-158">Plan for failures</span></span>

<span data-ttu-id="f543a-159">To create a YARN configuration for high-availability, you should plan for a possible executor or driver failure.</span><span class="sxs-lookup"><span data-stu-id="f543a-159">To create a YARN configuration for high-availability, you should plan for a possible executor or driver failure.</span></span> <span data-ttu-id="f543a-160">Some Spark Streaming jobs also include data guarantee requirements that need additional configuration and setup.</span><span class="sxs-lookup"><span data-stu-id="f543a-160">Some Spark Streaming jobs also include data guarantee requirements that need additional configuration and setup.</span></span> <span data-ttu-id="f543a-161">For example, a streaming application may have a business requirement for a zero-data-loss guarantee despite any error that occurs in the hosting streaming system or HDInsight cluster.</span><span class="sxs-lookup"><span data-stu-id="f543a-161">For example, a streaming application may have a business requirement for a zero-data-loss guarantee despite any error that occurs in the hosting streaming system or HDInsight cluster.</span></span>

<span data-ttu-id="f543a-162">If an **executor** fails, its tasks and receivers are restarted by Spark automatically, so there is no configuration change needed.</span><span class="sxs-lookup"><span data-stu-id="f543a-162">If an **executor** fails, its tasks and receivers are restarted by Spark automatically, so there is no configuration change needed.</span></span>

<span data-ttu-id="f543a-163">However, if a **driver** fails, then all of its associated executors fail, and all received blocks and computation results are lost.</span><span class="sxs-lookup"><span data-stu-id="f543a-163">However, if a **driver** fails, then all of its associated executors fail, and all received blocks and computation results are lost.</span></span> <span data-ttu-id="f543a-164">To recover from a driver failure, use *DStream checkpointing* as described in [Create Spark Streaming jobs with exactly-once event processing](apache-spark-streaming-exactly-once.md#use-checkpoints-for-drivers).</span><span class="sxs-lookup"><span data-stu-id="f543a-164">To recover from a driver failure, use *DStream checkpointing* as described in [Create Spark Streaming jobs with exactly-once event processing](apache-spark-streaming-exactly-once.md#use-checkpoints-for-drivers).</span></span> <span data-ttu-id="f543a-165">DStream checkpointing periodically saves the *directed acyclic graph* (DAG) of DStreams to fault-tolerant storage such as Azure Storage.</span><span class="sxs-lookup"><span data-stu-id="f543a-165">DStream checkpointing periodically saves the *directed acyclic graph* (DAG) of DStreams to fault-tolerant storage such as Azure Storage.</span></span>  <span data-ttu-id="f543a-166">Checkpointing allows Spark Structured Streaming to restart the failed driver from the checkpoint information.</span><span class="sxs-lookup"><span data-stu-id="f543a-166">Checkpointing allows Spark Structured Streaming to restart the failed driver from the checkpoint information.</span></span>  <span data-ttu-id="f543a-167">This driver restart launches new executors and also restarts receivers.</span><span class="sxs-lookup"><span data-stu-id="f543a-167">This driver restart launches new executors and also restarts receivers.</span></span>

<span data-ttu-id="f543a-168">To recover drivers with DStream checkpointing:</span><span class="sxs-lookup"><span data-stu-id="f543a-168">To recover drivers with DStream checkpointing:</span></span>

* <span data-ttu-id="f543a-169">Configure automatic driver restart on YARN with the configuration setting `yarn.resourcemanager.am.max-attempts`.</span><span class="sxs-lookup"><span data-stu-id="f543a-169">Configure automatic driver restart on YARN with the configuration setting `yarn.resourcemanager.am.max-attempts`.</span></span>
* <span data-ttu-id="f543a-170">Set a checkpoint directory in an HDFS-compatible file system with `streamingContext.checkpoint(hdfsDirectory)`.</span><span class="sxs-lookup"><span data-stu-id="f543a-170">Set a checkpoint directory in an HDFS-compatible file system with `streamingContext.checkpoint(hdfsDirectory)`.</span></span>
* <span data-ttu-id="f543a-171">Restructure source code to use checkpoints for recovery, for example:</span><span class="sxs-lookup"><span data-stu-id="f543a-171">Restructure source code to use checkpoints for recovery, for example:</span></span>

    ```scala
        def creatingFunc() : StreamingContext = {
            val context = new StreamingContext(...)
            val lines = KafkaUtils.createStream(...)
            val words = lines.flatMap(...)
            ...
            context.checkpoint(hdfsDir)
        }

        val context = StreamingContext.getOrCreate(hdfsDir, creatingFunc)
        context.start()
    ```

* <span data-ttu-id="f543a-172">Configure lost data recovery by enabling the write-ahead log (WAL) with `sparkConf.set("spark.streaming.receiver.writeAheadLog.enable","true")`, and disable in-memory replication for input DStreams with `StorageLevel.MEMORY_AND_DISK_SER`.</span><span class="sxs-lookup"><span data-stu-id="f543a-172">Configure lost data recovery by enabling the write-ahead log (WAL) with `sparkConf.set("spark.streaming.receiver.writeAheadLog.enable","true")`, and disable in-memory replication for input DStreams with `StorageLevel.MEMORY_AND_DISK_SER`.</span></span>

<span data-ttu-id="f543a-173">To summarize, using checkpointing + WAL + reliable receivers, you will be able to deliver "at least once" data recovery:</span><span class="sxs-lookup"><span data-stu-id="f543a-173">To summarize, using checkpointing + WAL + reliable receivers, you will be able to deliver "at least once" data recovery:</span></span>

* <span data-ttu-id="f543a-174">Exactly once, so long as received data is not lost and the outputs are either idempotent or transactional.</span><span class="sxs-lookup"><span data-stu-id="f543a-174">Exactly once, so long as received data is not lost and the outputs are either idempotent or transactional.</span></span>
* <span data-ttu-id="f543a-175">Exactly once, with the new Kafka Direct approach which uses Kafka as a replicated log, rather than using receivers or WALs.</span><span class="sxs-lookup"><span data-stu-id="f543a-175">Exactly once, with the new Kafka Direct approach which uses Kafka as a replicated log, rather than using receivers or WALs.</span></span>

### <a name="typical-concerns-for-high-availability"></a><span data-ttu-id="f543a-176">Typical concerns for high availability</span><span class="sxs-lookup"><span data-stu-id="f543a-176">Typical concerns for high availability</span></span>

* <span data-ttu-id="f543a-177">It is more difficult to monitor streaming jobs than batch jobs.</span><span class="sxs-lookup"><span data-stu-id="f543a-177">It is more difficult to monitor streaming jobs than batch jobs.</span></span> <span data-ttu-id="f543a-178">Spark Streaming jobs are typically long-running, and YARN doesn't aggregate logs until a job finishes.</span><span class="sxs-lookup"><span data-stu-id="f543a-178">Spark Streaming jobs are typically long-running, and YARN doesn't aggregate logs until a job finishes.</span></span>  <span data-ttu-id="f543a-179">Spark checkpoints are lost during application or Spark upgrades, and you'll need to clear the checkpoint directory during an upgrade.</span><span class="sxs-lookup"><span data-stu-id="f543a-179">Spark checkpoints are lost during application or Spark upgrades, and you'll need to clear the checkpoint directory during an upgrade.</span></span>

* <span data-ttu-id="f543a-180">Configure your YARN cluster mode to run drivers even if a client fails.</span><span class="sxs-lookup"><span data-stu-id="f543a-180">Configure your YARN cluster mode to run drivers even if a client fails.</span></span> <span data-ttu-id="f543a-181">To set up automatic restart for drivers:</span><span class="sxs-lookup"><span data-stu-id="f543a-181">To set up automatic restart for drivers:</span></span>

    ```
    spark.yarn.maxAppAttempts = 2
    spark.yarn.am.attemptFailuresValidityInterval=1h
    ```

* <span data-ttu-id="f543a-182">Spark and the Spark Streaming UI have a configurable metrics system.</span><span class="sxs-lookup"><span data-stu-id="f543a-182">Spark and the Spark Streaming UI have a configurable metrics system.</span></span> <span data-ttu-id="f543a-183">You can also use additional libraries, such as Graphite/Grafana to download dashboard metrics such as 'num records processed', 'memory/GC usage on driver & executors', 'total delay', 'utilization of the cluster' and so forth.</span><span class="sxs-lookup"><span data-stu-id="f543a-183">You can also use additional libraries, such as Graphite/Grafana to download dashboard metrics such as 'num records processed', 'memory/GC usage on driver & executors', 'total delay', 'utilization of the cluster' and so forth.</span></span> <span data-ttu-id="f543a-184">In Structured Streaming version 2.1 or greater, you can use `StreamingQueryListener` to gather additional metrics.</span><span class="sxs-lookup"><span data-stu-id="f543a-184">In Structured Streaming version 2.1 or greater, you can use `StreamingQueryListener` to gather additional metrics.</span></span>

* <span data-ttu-id="f543a-185">You should segment long-running jobs.</span><span class="sxs-lookup"><span data-stu-id="f543a-185">You should segment long-running jobs.</span></span>  <span data-ttu-id="f543a-186">When a Spark Streaming application is submitted to the cluster, the YARN queue where the job runs must be defined.</span><span class="sxs-lookup"><span data-stu-id="f543a-186">When a Spark Streaming application is submitted to the cluster, the YARN queue where the job runs must be defined.</span></span> <span data-ttu-id="f543a-187">You can use a [YARN Capacity Scheduler](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html) to submit long-running jobs to separate queues.</span><span class="sxs-lookup"><span data-stu-id="f543a-187">You can use a [YARN Capacity Scheduler](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html) to submit long-running jobs to separate queues.</span></span>

* <span data-ttu-id="f543a-188">Shut down your streaming application gracefully.</span><span class="sxs-lookup"><span data-stu-id="f543a-188">Shut down your streaming application gracefully.</span></span> <span data-ttu-id="f543a-189">If your offsets are known, and all application state is stored externally, then you can programmatically stop your streaming application at the appropriate place.</span><span class="sxs-lookup"><span data-stu-id="f543a-189">If your offsets are known, and all application state is stored externally, then you can programmatically stop your streaming application at the appropriate place.</span></span> <span data-ttu-id="f543a-190">One technique is to use "thread hooks" in Spark, by checking for an external flag every *n* seconds.</span><span class="sxs-lookup"><span data-stu-id="f543a-190">One technique is to use "thread hooks" in Spark, by checking for an external flag every *n* seconds.</span></span> <span data-ttu-id="f543a-191">You can also use a *marker file* that is created on HDFS when starting the application, then removed when you want to stop.</span><span class="sxs-lookup"><span data-stu-id="f543a-191">You can also use a *marker file* that is created on HDFS when starting the application, then removed when you want to stop.</span></span> <span data-ttu-id="f543a-192">For a marker file approach, use a separate thread in your Spark application that calls code similar to this:</span><span class="sxs-lookup"><span data-stu-id="f543a-192">For a marker file approach, use a separate thread in your Spark application that calls code similar to this:</span></span>

    ```scala
    streamingContext.stop(stopSparkContext = true, stopGracefully = true)
    // to be able to recover on restart, store all offsets in an external database
    ```

## <a name="next-steps"></a><span data-ttu-id="f543a-193">Next steps</span><span class="sxs-lookup"><span data-stu-id="f543a-193">Next steps</span></span>

* [<span data-ttu-id="f543a-194">Spark Streaming Overview</span><span class="sxs-lookup"><span data-stu-id="f543a-194">Spark Streaming Overview</span></span>](apache-spark-streaming-overview.md)
* [<span data-ttu-id="f543a-195">Create Spark Streaming jobs with exactly-once event processing</span><span class="sxs-lookup"><span data-stu-id="f543a-195">Create Spark Streaming jobs with exactly-once event processing</span></span>](apache-spark-streaming-exactly-once.md)
* [<span data-ttu-id="f543a-196">Long-running Spark Streaming Jobs on YARN</span><span class="sxs-lookup"><span data-stu-id="f543a-196">Long-running Spark Streaming Jobs on YARN</span></span>](http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/) 
* [<span data-ttu-id="f543a-197">Structured Streaming: Fault Tolerant Semantics</span><span class="sxs-lookup"><span data-stu-id="f543a-197">Structured Streaming: Fault Tolerant Semantics</span></span>](http://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html#fault-tolerance-semantics)
* [<span data-ttu-id="f543a-198">Discretized Streams: A Fault-Tolerant Model for Scalable Stream Processing</span><span class="sxs-lookup"><span data-stu-id="f543a-198">Discretized Streams: A Fault-Tolerant Model for Scalable Stream Processing</span></span>](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-259.pdf)
