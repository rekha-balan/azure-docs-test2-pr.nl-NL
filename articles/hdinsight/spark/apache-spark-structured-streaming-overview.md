---
title: Spark Structured Streaming in Azure HDInsight
description: How to use Spark Structured Streaming applications on HDInsight Spark clusters.
services: hdinsight
author: maxluk
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: conceptual
ms.date: 02/05/2018
ms.author: maxluk
ms.openlocfilehash: 7470783ba3ebac652c83c397ba2bbe683023c657
ms.sourcegitcommit: d1451406a010fd3aa854dc8e5b77dc5537d8050e
ms.translationtype: MT
ms.contentlocale: nl-NL
ms.lasthandoff: 09/13/2018
ms.locfileid: "44857179"
---
# <a name="overview-of-spark-structured-streaming"></a><span data-ttu-id="a0c29-103">Overview of Spark Structured Streaming</span><span class="sxs-lookup"><span data-stu-id="a0c29-103">Overview of Spark Structured Streaming</span></span>

<span data-ttu-id="a0c29-104">Spark Structured Streaming enables you to implement scalable, high-throughput, fault-tolerant applications for  processing  data streams.</span><span class="sxs-lookup"><span data-stu-id="a0c29-104">Spark Structured Streaming enables you to implement scalable, high-throughput, fault-tolerant applications for  processing  data streams.</span></span> <span data-ttu-id="a0c29-105">Structured Streaming is built upon the Spark SQL engine, and improves upon the constructs from Spark SQL Data Frames and Datasets so  you can write streaming queries in the same way you would write batch queries.</span><span class="sxs-lookup"><span data-stu-id="a0c29-105">Structured Streaming is built upon the Spark SQL engine, and improves upon the constructs from Spark SQL Data Frames and Datasets so  you can write streaming queries in the same way you would write batch queries.</span></span>  

<span data-ttu-id="a0c29-106">Structured Streaming applications run on HDInsight Spark clusters, and connect  to  streaming data from Kafka, a TCP socket (for debugging purposes), Azure Storage, or Azure Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="a0c29-106">Structured Streaming applications run on HDInsight Spark clusters, and connect  to  streaming data from Kafka, a TCP socket (for debugging purposes), Azure Storage, or Azure Data Lake Store.</span></span> <span data-ttu-id="a0c29-107">The latter two options, which rely on external storage services, enable you to watch for new files added into storage and process their contents as if they were streamed.</span><span class="sxs-lookup"><span data-stu-id="a0c29-107">The latter two options, which rely on external storage services, enable you to watch for new files added into storage and process their contents as if they were streamed.</span></span> 

<span data-ttu-id="a0c29-108">Structured Streaming creates a long-running query during which you  apply operations to the input data, such as selection, projection, aggregation, windowing, and joining the streaming DataFrame with reference DataFrames.</span><span class="sxs-lookup"><span data-stu-id="a0c29-108">Structured Streaming creates a long-running query during which you  apply operations to the input data, such as selection, projection, aggregation, windowing, and joining the streaming DataFrame with reference DataFrames.</span></span> <span data-ttu-id="a0c29-109">Next, you output the results to file storage (Azure Storage Blobs or Data Lake Store) or to any datastore by using custom code (such as SQL Database or Power BI).</span><span class="sxs-lookup"><span data-stu-id="a0c29-109">Next, you output the results to file storage (Azure Storage Blobs or Data Lake Store) or to any datastore by using custom code (such as SQL Database or Power BI).</span></span> <span data-ttu-id="a0c29-110">Structured Streaming also provides output to the console for debugging locally, and to an in-memory table so you can see the data generated for debugging in HDInsight.</span><span class="sxs-lookup"><span data-stu-id="a0c29-110">Structured Streaming also provides output to the console for debugging locally, and to an in-memory table so you can see the data generated for debugging in HDInsight.</span></span> 

![<span data-ttu-id="a0c29-111">Stream Processing with HDInsight and Spark Structured Streaming</span><span class="sxs-lookup"><span data-stu-id="a0c29-111">Stream Processing with HDInsight and Spark Structured Streaming</span></span> ](./media/apache-spark-structured-streaming-overview/hdinsight-spark-structured-streaming.png)

> [!NOTE]
> <span data-ttu-id="a0c29-112">Spark Structured Streaming is  replacing Spark Streaming (DStreams).</span><span class="sxs-lookup"><span data-stu-id="a0c29-112">Spark Structured Streaming is  replacing Spark Streaming (DStreams).</span></span> <span data-ttu-id="a0c29-113">Going forward, Structured Streaming will receive enhancements and maintenance, while DStreams will be in maintenance mode only.</span><span class="sxs-lookup"><span data-stu-id="a0c29-113">Going forward, Structured Streaming will receive enhancements and maintenance, while DStreams will be in maintenance mode only.</span></span> <span data-ttu-id="a0c29-114">Structured Streaming is currently not as feature-complete as DStreams for the sources and sinks that it supports out of the box, so evaluate your requirements to choose the appropriate Spark stream processing option.</span><span class="sxs-lookup"><span data-stu-id="a0c29-114">Structured Streaming is currently not as feature-complete as DStreams for the sources and sinks that it supports out of the box, so evaluate your requirements to choose the appropriate Spark stream processing option.</span></span> 

## <a name="streams-as-tables"></a><span data-ttu-id="a0c29-115">Streams as tables</span><span class="sxs-lookup"><span data-stu-id="a0c29-115">Streams as tables</span></span>

<span data-ttu-id="a0c29-116">Spark Structured Streaming represents a stream of data  as a table that is unbounded in depth, that is, the table  continues to grow as new data arrives.</span><span class="sxs-lookup"><span data-stu-id="a0c29-116">Spark Structured Streaming represents a stream of data  as a table that is unbounded in depth, that is, the table  continues to grow as new data arrives.</span></span> <span data-ttu-id="a0c29-117">This *input table* is continuously processed by a long-running query, and the results sent to an *output table*:</span><span class="sxs-lookup"><span data-stu-id="a0c29-117">This *input table* is continuously processed by a long-running query, and the results sent to an *output table*:</span></span>

![Structured Streaming Concept](./media/apache-spark-structured-streaming-overview/hdinsight-spark-structured-streaming-concept.png)

<span data-ttu-id="a0c29-119">In Structured Streaming, data arrives at the system and is immediately ingested into an input table.</span><span class="sxs-lookup"><span data-stu-id="a0c29-119">In Structured Streaming, data arrives at the system and is immediately ingested into an input table.</span></span> <span data-ttu-id="a0c29-120">You write queries (using the DataFrame and Dataset APIs) that perform operations against this input table.</span><span class="sxs-lookup"><span data-stu-id="a0c29-120">You write queries (using the DataFrame and Dataset APIs) that perform operations against this input table.</span></span> <span data-ttu-id="a0c29-121">The query output yields another table,  the *results table*.</span><span class="sxs-lookup"><span data-stu-id="a0c29-121">The query output yields another table,  the *results table*.</span></span> <span data-ttu-id="a0c29-122">The results table contains the results of your query, from which you draw data for an external datastore, such a relational database.</span><span class="sxs-lookup"><span data-stu-id="a0c29-122">The results table contains the results of your query, from which you draw data for an external datastore, such a relational database.</span></span> <span data-ttu-id="a0c29-123">The timing of when data is processed from the input table is controlled by the *trigger interval*.</span><span class="sxs-lookup"><span data-stu-id="a0c29-123">The timing of when data is processed from the input table is controlled by the *trigger interval*.</span></span> <span data-ttu-id="a0c29-124">By default, the trigger interval is zero, so Structured Streaming tries to process the data as soon as it arrives.</span><span class="sxs-lookup"><span data-stu-id="a0c29-124">By default, the trigger interval is zero, so Structured Streaming tries to process the data as soon as it arrives.</span></span> <span data-ttu-id="a0c29-125">In practice, this means that as soon as Structured Streaming is done processing the run of the previous query, it starts another processing run against any newly received data.</span><span class="sxs-lookup"><span data-stu-id="a0c29-125">In practice, this means that as soon as Structured Streaming is done processing the run of the previous query, it starts another processing run against any newly received data.</span></span> <span data-ttu-id="a0c29-126">You can configure the trigger to run at an interval, so that the streaming data is processed in time-based batches.</span><span class="sxs-lookup"><span data-stu-id="a0c29-126">You can configure the trigger to run at an interval, so that the streaming data is processed in time-based batches.</span></span> 

<span data-ttu-id="a0c29-127">The data in the results tables  may  contain only the data that is new since the last time the query was processed (*append mode*), or the table may be completely refreshed every time there is new data so the table includes all of the output data since the streaming query began (*complete mode*).</span><span class="sxs-lookup"><span data-stu-id="a0c29-127">The data in the results tables  may  contain only the data that is new since the last time the query was processed (*append mode*), or the table may be completely refreshed every time there is new data so the table includes all of the output data since the streaming query began (*complete mode*).</span></span>

### <a name="append-mode"></a><span data-ttu-id="a0c29-128">Append mode</span><span class="sxs-lookup"><span data-stu-id="a0c29-128">Append mode</span></span>

<span data-ttu-id="a0c29-129">In append mode, only the rows added to the results table since the last query run are present in the results table and written to external storage.</span><span class="sxs-lookup"><span data-stu-id="a0c29-129">In append mode, only the rows added to the results table since the last query run are present in the results table and written to external storage.</span></span> <span data-ttu-id="a0c29-130">For example,  the simplest query  just copies all data from the input table to the results table unaltered.</span><span class="sxs-lookup"><span data-stu-id="a0c29-130">For example,  the simplest query  just copies all data from the input table to the results table unaltered.</span></span> <span data-ttu-id="a0c29-131">Each time a trigger interval elapses, the new data is processed and the rows representing that new data appear in the results table.</span><span class="sxs-lookup"><span data-stu-id="a0c29-131">Each time a trigger interval elapses, the new data is processed and the rows representing that new data appear in the results table.</span></span> 

<span data-ttu-id="a0c29-132">Consider a scenario where you are processing telemetry from temperature sensors, such as a  thermostat.</span><span class="sxs-lookup"><span data-stu-id="a0c29-132">Consider a scenario where you are processing telemetry from temperature sensors, such as a  thermostat.</span></span> <span data-ttu-id="a0c29-133">Assume the first trigger processed one event at time 00:01 for device 1 with a temperature reading of 95 degrees.</span><span class="sxs-lookup"><span data-stu-id="a0c29-133">Assume the first trigger processed one event at time 00:01 for device 1 with a temperature reading of 95 degrees.</span></span> <span data-ttu-id="a0c29-134">In the first trigger of the query, only the row with time 00:01  appears in the results table.</span><span class="sxs-lookup"><span data-stu-id="a0c29-134">In the first trigger of the query, only the row with time 00:01  appears in the results table.</span></span> <span data-ttu-id="a0c29-135">At time 00:02 when another event arrives,  the only new row is the row with time 00:02 and so the results table would contain only that one row.</span><span class="sxs-lookup"><span data-stu-id="a0c29-135">At time 00:02 when another event arrives,  the only new row is the row with time 00:02 and so the results table would contain only that one row.</span></span>

![Structured Streaming Append Mode](./media/apache-spark-structured-streaming-overview/hdinsight-spark-structured-streaming-append-mode.png)

<span data-ttu-id="a0c29-137">When using append mode, your query would be applying projections (selecting the columns it cares about), filtering (picking only rows that match certain conditions) or joining (augmenting the data with data from a static lookup table).</span><span class="sxs-lookup"><span data-stu-id="a0c29-137">When using append mode, your query would be applying projections (selecting the columns it cares about), filtering (picking only rows that match certain conditions) or joining (augmenting the data with data from a static lookup table).</span></span> <span data-ttu-id="a0c29-138">Append mode  makes it easy to push only the relevant new data points out to external storage.</span><span class="sxs-lookup"><span data-stu-id="a0c29-138">Append mode  makes it easy to push only the relevant new data points out to external storage.</span></span>

### <a name="complete-mode"></a><span data-ttu-id="a0c29-139">Complete mode</span><span class="sxs-lookup"><span data-stu-id="a0c29-139">Complete mode</span></span>

<span data-ttu-id="a0c29-140">Consider the same scenario, this time using  complete mode.</span><span class="sxs-lookup"><span data-stu-id="a0c29-140">Consider the same scenario, this time using  complete mode.</span></span> <span data-ttu-id="a0c29-141">In complete mode, the entire output table is refreshed on every trigger so the table includes data not just from the most recent trigger run, but from all runs.</span><span class="sxs-lookup"><span data-stu-id="a0c29-141">In complete mode, the entire output table is refreshed on every trigger so the table includes data not just from the most recent trigger run, but from all runs.</span></span> <span data-ttu-id="a0c29-142">You could use  complete mode to copy the data unaltered from the input table to the results table.</span><span class="sxs-lookup"><span data-stu-id="a0c29-142">You could use  complete mode to copy the data unaltered from the input table to the results table.</span></span> <span data-ttu-id="a0c29-143">On every triggered run, the new result rows  appear along with all the previous rows.</span><span class="sxs-lookup"><span data-stu-id="a0c29-143">On every triggered run, the new result rows  appear along with all the previous rows.</span></span> <span data-ttu-id="a0c29-144">The output results table will end up storing all of the data collected since the query began, and  you would eventually run out of memory.</span><span class="sxs-lookup"><span data-stu-id="a0c29-144">The output results table will end up storing all of the data collected since the query began, and  you would eventually run out of memory.</span></span> <span data-ttu-id="a0c29-145">Complete mode is intended for use with aggregate queries that  summarize the incoming data in some way, and so on every trigger the results table is updated with a new summary.</span><span class="sxs-lookup"><span data-stu-id="a0c29-145">Complete mode is intended for use with aggregate queries that  summarize the incoming data in some way, and so on every trigger the results table is updated with a new summary.</span></span> 

<span data-ttu-id="a0c29-146">Assume so far there are five seconds' worth of data already processed, and it is time to   process the data for the sixth second.</span><span class="sxs-lookup"><span data-stu-id="a0c29-146">Assume so far there are five seconds' worth of data already processed, and it is time to   process the data for the sixth second.</span></span> <span data-ttu-id="a0c29-147">The input table has  events for time 00:01 and time 00:03.</span><span class="sxs-lookup"><span data-stu-id="a0c29-147">The input table has  events for time 00:01 and time 00:03.</span></span> <span data-ttu-id="a0c29-148">The goal of this example query is to give the average temperature of the device every five seconds.</span><span class="sxs-lookup"><span data-stu-id="a0c29-148">The goal of this example query is to give the average temperature of the device every five seconds.</span></span> <span data-ttu-id="a0c29-149">The implementation of this query  applies an aggregate that takes all of the values that fall within each 5-second window,  averages the temperature, and produces a row for  the average temperature over that interval.</span><span class="sxs-lookup"><span data-stu-id="a0c29-149">The implementation of this query  applies an aggregate that takes all of the values that fall within each 5-second window,  averages the temperature, and produces a row for  the average temperature over that interval.</span></span> <span data-ttu-id="a0c29-150">At the end of the first 5-second window, there are two tuples: (00:01, 1, 95) and (00:03, 1, 98).</span><span class="sxs-lookup"><span data-stu-id="a0c29-150">At the end of the first 5-second window, there are two tuples: (00:01, 1, 95) and (00:03, 1, 98).</span></span> <span data-ttu-id="a0c29-151">So for the window 00:00-00:05 the aggregation produces  a tuple with the average temperature of 96.5 degrees.</span><span class="sxs-lookup"><span data-stu-id="a0c29-151">So for the window 00:00-00:05 the aggregation produces  a tuple with the average temperature of 96.5 degrees.</span></span> <span data-ttu-id="a0c29-152">In the next 5-second window, there is only  one data point at time 00:06, so the resulting average temperature is 98 degrees.</span><span class="sxs-lookup"><span data-stu-id="a0c29-152">In the next 5-second window, there is only  one data point at time 00:06, so the resulting average temperature is 98 degrees.</span></span> <span data-ttu-id="a0c29-153">At time 00:10, using complete mode, the results table has the rows for both windows 00:00-00:05 and 00:05-00:10 because the query  outputs all the aggregated rows, not just the new ones.</span><span class="sxs-lookup"><span data-stu-id="a0c29-153">At time 00:10, using complete mode, the results table has the rows for both windows 00:00-00:05 and 00:05-00:10 because the query  outputs all the aggregated rows, not just the new ones.</span></span> <span data-ttu-id="a0c29-154">Therefore the results table continues to grow as new windows are added.</span><span class="sxs-lookup"><span data-stu-id="a0c29-154">Therefore the results table continues to grow as new windows are added.</span></span>    

![Structured Streaming Complete Mode](./media/apache-spark-structured-streaming-overview/hdinsight-spark-structured-streaming-complete-mode.png)

<span data-ttu-id="a0c29-156">Not all queries using complete mode will  cause the table to grow without bounds.</span><span class="sxs-lookup"><span data-stu-id="a0c29-156">Not all queries using complete mode will  cause the table to grow without bounds.</span></span>  <span data-ttu-id="a0c29-157">Consider in the previous example that rather than averaging the temperature by time window, it averaged instead by device ID.</span><span class="sxs-lookup"><span data-stu-id="a0c29-157">Consider in the previous example that rather than averaging the temperature by time window, it averaged instead by device ID.</span></span> <span data-ttu-id="a0c29-158">The result table  contains a fixed number of rows (one per device) with the average temperature for the device across all data points received from that device.</span><span class="sxs-lookup"><span data-stu-id="a0c29-158">The result table  contains a fixed number of rows (one per device) with the average temperature for the device across all data points received from that device.</span></span> <span data-ttu-id="a0c29-159">As new temperatures are received, the results table is updated so that the averages in the table are always current.</span><span class="sxs-lookup"><span data-stu-id="a0c29-159">As new temperatures are received, the results table is updated so that the averages in the table are always current.</span></span> 

## <a name="components-of-a-spark-structured-streaming-application"></a><span data-ttu-id="a0c29-160">Components of a Spark Structured Streaming application</span><span class="sxs-lookup"><span data-stu-id="a0c29-160">Components of a Spark Structured Streaming application</span></span>

<span data-ttu-id="a0c29-161">A simple example query can summarize the temperature readings by hour-long windows.</span><span class="sxs-lookup"><span data-stu-id="a0c29-161">A simple example query can summarize the temperature readings by hour-long windows.</span></span> <span data-ttu-id="a0c29-162">In this case, the data  is stored in JSON files in Azure Storage (attached as the default storage for the HDInsight cluster):</span><span class="sxs-lookup"><span data-stu-id="a0c29-162">In this case, the data  is stored in JSON files in Azure Storage (attached as the default storage for the HDInsight cluster):</span></span>

    {"time":1469501107,"temp":"95"}
    {"time":1469501147,"temp":"95"}
    {"time":1469501202,"temp":"95"}
    {"time":1469501219,"temp":"95"}
    {"time":1469501225,"temp":"95"}

<span data-ttu-id="a0c29-163">These JSON files are stored in the `temps` subfolder underneath  the HDInsight cluster's container.</span><span class="sxs-lookup"><span data-stu-id="a0c29-163">These JSON files are stored in the `temps` subfolder underneath  the HDInsight cluster's container.</span></span> 

### <a name="define-the-input-source"></a><span data-ttu-id="a0c29-164">Define the input source</span><span class="sxs-lookup"><span data-stu-id="a0c29-164">Define the input source</span></span>

<span data-ttu-id="a0c29-165">First configure a DataFrame that describes the source of the data and any settings required by that source.</span><span class="sxs-lookup"><span data-stu-id="a0c29-165">First configure a DataFrame that describes the source of the data and any settings required by that source.</span></span> <span data-ttu-id="a0c29-166">This example draws from the JSON files in Azure Storage and applies a schema to them at read time.</span><span class="sxs-lookup"><span data-stu-id="a0c29-166">This example draws from the JSON files in Azure Storage and applies a schema to them at read time.</span></span>

    import org.apache.spark.sql.types._
    import org.apache.spark.sql.functions._

    //Cluster-local path to the folder containing the JSON files
    val inputPath = "/temps/" 

    //Define the schema of the JSON files as having the "time" of type TimeStamp and the "temp" field of type String
    val jsonSchema = new StructType().add("time", TimestampType).add("temp", StringType)

    //Create a Streaming DataFrame by calling readStream and configuring it with the schema and path
    val streamingInputDF = spark.readStream.schema(jsonSchema).json(inputPath) 

#### <a name="apply-the-query"></a><span data-ttu-id="a0c29-167">Apply the query</span><span class="sxs-lookup"><span data-stu-id="a0c29-167">Apply the query</span></span>

<span data-ttu-id="a0c29-168">Next,  apply a query that contains the desired operations against the Streaming DataFrame.</span><span class="sxs-lookup"><span data-stu-id="a0c29-168">Next,  apply a query that contains the desired operations against the Streaming DataFrame.</span></span> <span data-ttu-id="a0c29-169">In this case, an aggregation groups all the rows into 1-hour windows, and then computes the minimum, average, and maximum temperatures in that 1-hour window.</span><span class="sxs-lookup"><span data-stu-id="a0c29-169">In this case, an aggregation groups all the rows into 1-hour windows, and then computes the minimum, average, and maximum temperatures in that 1-hour window.</span></span>

    val streamingAggDF = streamingInputDF.groupBy(window($"time", "1 hour")).agg(min($"temp"), avg($"temp"), max($"temp"))

### <a name="define-the-output-sink"></a><span data-ttu-id="a0c29-170">Define the output sink</span><span class="sxs-lookup"><span data-stu-id="a0c29-170">Define the output sink</span></span>

<span data-ttu-id="a0c29-171">Next,  define the destination for the rows that are added to the results table within each trigger interval.</span><span class="sxs-lookup"><span data-stu-id="a0c29-171">Next,  define the destination for the rows that are added to the results table within each trigger interval.</span></span> <span data-ttu-id="a0c29-172">This example  just outputs all  rows to an in-memory table `temps` that you can later query with SparkSQL.</span><span class="sxs-lookup"><span data-stu-id="a0c29-172">This example  just outputs all  rows to an in-memory table `temps` that you can later query with SparkSQL.</span></span> <span data-ttu-id="a0c29-173">Complete  output mode ensures that all rows for all windows are output every time.</span><span class="sxs-lookup"><span data-stu-id="a0c29-173">Complete  output mode ensures that all rows for all windows are output every time.</span></span>

    val streamingOutDF = streamingAggDF.writeStream.format("memory").queryName("temps").outputMode("complete") 

### <a name="start-the-query"></a><span data-ttu-id="a0c29-174">Start the query</span><span class="sxs-lookup"><span data-stu-id="a0c29-174">Start the query</span></span>

<span data-ttu-id="a0c29-175">Start the streaming query and run until a termination signal is received.</span><span class="sxs-lookup"><span data-stu-id="a0c29-175">Start the streaming query and run until a termination signal is received.</span></span> 

    val query = streamingOutDF.start()  

### <a name="view-the-results"></a><span data-ttu-id="a0c29-176">View the results</span><span class="sxs-lookup"><span data-stu-id="a0c29-176">View the results</span></span>

<span data-ttu-id="a0c29-177">While the query is running, in the same SparkSession, you can run a SparkSQL query against the `temps` table where the query results are stored.</span><span class="sxs-lookup"><span data-stu-id="a0c29-177">While the query is running, in the same SparkSession, you can run a SparkSQL query against the `temps` table where the query results are stored.</span></span> 

    select * from temps

<span data-ttu-id="a0c29-178">This query  yields results similar to the following:</span><span class="sxs-lookup"><span data-stu-id="a0c29-178">This query  yields results similar to the following:</span></span>


| <span data-ttu-id="a0c29-179">window</span><span class="sxs-lookup"><span data-stu-id="a0c29-179">window</span></span> |  <span data-ttu-id="a0c29-180">min(temp)</span><span class="sxs-lookup"><span data-stu-id="a0c29-180">min(temp)</span></span> | <span data-ttu-id="a0c29-181">avg(temp)</span><span class="sxs-lookup"><span data-stu-id="a0c29-181">avg(temp)</span></span> | <span data-ttu-id="a0c29-182">max(temp)</span><span class="sxs-lookup"><span data-stu-id="a0c29-182">max(temp)</span></span> |
| --- | --- | --- | --- |
|<span data-ttu-id="a0c29-183">{u'start': u'2016-07-26T02:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-183">{u'start': u'2016-07-26T02:00:00.000Z', u'end'...</span></span> |    <span data-ttu-id="a0c29-184">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-184">95</span></span> |    <span data-ttu-id="a0c29-185">95.231579</span><span class="sxs-lookup"><span data-stu-id="a0c29-185">95.231579</span></span> | <span data-ttu-id="a0c29-186">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-186">99</span></span> |
|<span data-ttu-id="a0c29-187">{u'start': u'2016-07-26T03:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-187">{u'start': u'2016-07-26T03:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-188">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-188">95</span></span> |   <span data-ttu-id="a0c29-189">96.023048</span><span class="sxs-lookup"><span data-stu-id="a0c29-189">96.023048</span></span> | <span data-ttu-id="a0c29-190">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-190">99</span></span> |
|<span data-ttu-id="a0c29-191">{u'start': u'2016-07-26T04:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-191">{u'start': u'2016-07-26T04:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-192">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-192">95</span></span> |   <span data-ttu-id="a0c29-193">96.797133</span><span class="sxs-lookup"><span data-stu-id="a0c29-193">96.797133</span></span> | <span data-ttu-id="a0c29-194">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-194">99</span></span> |
|<span data-ttu-id="a0c29-195">{u'start': u'2016-07-26T05:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-195">{u'start': u'2016-07-26T05:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-196">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-196">95</span></span> |   <span data-ttu-id="a0c29-197">96.984639</span><span class="sxs-lookup"><span data-stu-id="a0c29-197">96.984639</span></span> | <span data-ttu-id="a0c29-198">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-198">99</span></span> |
|<span data-ttu-id="a0c29-199">{u'start': u'2016-07-26T06:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-199">{u'start': u'2016-07-26T06:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-200">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-200">95</span></span> |   <span data-ttu-id="a0c29-201">97.014749</span><span class="sxs-lookup"><span data-stu-id="a0c29-201">97.014749</span></span> | <span data-ttu-id="a0c29-202">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-202">99</span></span> |
|<span data-ttu-id="a0c29-203">{u'start': u'2016-07-26T07:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-203">{u'start': u'2016-07-26T07:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-204">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-204">95</span></span> |   <span data-ttu-id="a0c29-205">96.980971</span><span class="sxs-lookup"><span data-stu-id="a0c29-205">96.980971</span></span> | <span data-ttu-id="a0c29-206">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-206">99</span></span> |
|<span data-ttu-id="a0c29-207">{u'start': u'2016-07-26T08:00:00.000Z', u'end'...</span><span class="sxs-lookup"><span data-stu-id="a0c29-207">{u'start': u'2016-07-26T08:00:00.000Z', u'end'...</span></span>  |<span data-ttu-id="a0c29-208">95</span><span class="sxs-lookup"><span data-stu-id="a0c29-208">95</span></span> |   <span data-ttu-id="a0c29-209">96.965997</span><span class="sxs-lookup"><span data-stu-id="a0c29-209">96.965997</span></span> | <span data-ttu-id="a0c29-210">99</span><span class="sxs-lookup"><span data-stu-id="a0c29-210">99</span></span> |  

<span data-ttu-id="a0c29-211">For details on the Spark Structured Stream API, along with the input data sources, operations, and output sinks it supports, see [Spark Structured Streaming Programming Guide](http://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html).</span><span class="sxs-lookup"><span data-stu-id="a0c29-211">For details on the Spark Structured Stream API, along with the input data sources, operations, and output sinks it supports, see [Spark Structured Streaming Programming Guide](http://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html).</span></span>

## <a name="checkpointing-and-write-ahead-logs"></a><span data-ttu-id="a0c29-212">Checkpointing and write-ahead logs</span><span class="sxs-lookup"><span data-stu-id="a0c29-212">Checkpointing and write-ahead logs</span></span>

<span data-ttu-id="a0c29-213">To deliver resiliency and fault tolerance, Structured Streaming relies on *checkpointing* to ensure that stream processing can continue uninterrupted, even with node failures.</span><span class="sxs-lookup"><span data-stu-id="a0c29-213">To deliver resiliency and fault tolerance, Structured Streaming relies on *checkpointing* to ensure that stream processing can continue uninterrupted, even with node failures.</span></span> <span data-ttu-id="a0c29-214">In HDInsight, Spark creates checkpoints to durable storage, either Azure Storage or Data Lake Store.</span><span class="sxs-lookup"><span data-stu-id="a0c29-214">In HDInsight, Spark creates checkpoints to durable storage, either Azure Storage or Data Lake Store.</span></span> <span data-ttu-id="a0c29-215">These checkpoints store the progress information about the streaming query.</span><span class="sxs-lookup"><span data-stu-id="a0c29-215">These checkpoints store the progress information about the streaming query.</span></span> <span data-ttu-id="a0c29-216">In addition, Structured Streaming uses a *write-ahead log* (WAL).</span><span class="sxs-lookup"><span data-stu-id="a0c29-216">In addition, Structured Streaming uses a *write-ahead log* (WAL).</span></span> <span data-ttu-id="a0c29-217">The  WAL  captures ingested data that has been received but not yet processed by a query.</span><span class="sxs-lookup"><span data-stu-id="a0c29-217">The  WAL  captures ingested data that has been received but not yet processed by a query.</span></span> <span data-ttu-id="a0c29-218">If  a failure occurs and processing is restarted from the WAL, any events received from the source are not lost.</span><span class="sxs-lookup"><span data-stu-id="a0c29-218">If  a failure occurs and processing is restarted from the WAL, any events received from the source are not lost.</span></span>

## <a name="deploying-spark-streaming-applications"></a><span data-ttu-id="a0c29-219">Deploying Spark Streaming applications</span><span class="sxs-lookup"><span data-stu-id="a0c29-219">Deploying Spark Streaming applications</span></span>

<span data-ttu-id="a0c29-220">You typically build a Spark Streaming application locally into a JAR file and then deploy it to Spark on HDInsight by copying the JAR file  to the default storage attached to your HDInsight cluster.</span><span class="sxs-lookup"><span data-stu-id="a0c29-220">You typically build a Spark Streaming application locally into a JAR file and then deploy it to Spark on HDInsight by copying the JAR file  to the default storage attached to your HDInsight cluster.</span></span> <span data-ttu-id="a0c29-221">You can start your application  with the LIVY REST APIs available from your cluster using  a POST operation.</span><span class="sxs-lookup"><span data-stu-id="a0c29-221">You can start your application  with the LIVY REST APIs available from your cluster using  a POST operation.</span></span> <span data-ttu-id="a0c29-222">The body of the POST includes a JSON document that provides the path to your JAR, the name of the class whose main method defines and runs the streaming application, and optionally the resource requirements of the job (such as the number of executors, memory and cores), and any configuration settings your application code requires.</span><span class="sxs-lookup"><span data-stu-id="a0c29-222">The body of the POST includes a JSON document that provides the path to your JAR, the name of the class whose main method defines and runs the streaming application, and optionally the resource requirements of the job (such as the number of executors, memory and cores), and any configuration settings your application code requires.</span></span>

![Deploying a Spark Streaming application](./media/apache-spark-streaming-overview/hdinsight-spark-streaming-livy.png)

<span data-ttu-id="a0c29-224">The status of all applications can also be checked with a GET request against a LIVY endpoint.</span><span class="sxs-lookup"><span data-stu-id="a0c29-224">The status of all applications can also be checked with a GET request against a LIVY endpoint.</span></span> <span data-ttu-id="a0c29-225">Finally, you can terminate a running application by issuing a DELETE request against the LIVY endpoint.</span><span class="sxs-lookup"><span data-stu-id="a0c29-225">Finally, you can terminate a running application by issuing a DELETE request against the LIVY endpoint.</span></span> <span data-ttu-id="a0c29-226">For details on the LIVY API, see [Remote jobs with LIVY](apache-spark-livy-rest-interface.md)</span><span class="sxs-lookup"><span data-stu-id="a0c29-226">For details on the LIVY API, see [Remote jobs with LIVY](apache-spark-livy-rest-interface.md)</span></span>

## <a name="next-steps"></a><span data-ttu-id="a0c29-227">Next steps</span><span class="sxs-lookup"><span data-stu-id="a0c29-227">Next steps</span></span>

* [<span data-ttu-id="a0c29-228">Create an Apache Spark cluster in HDInsight</span><span class="sxs-lookup"><span data-stu-id="a0c29-228">Create an Apache Spark cluster in HDInsight</span></span>](../hdinsight-hadoop-create-linux-clusters-portal.md)
* [<span data-ttu-id="a0c29-229">Spark Structured Streaming Programming Guide</span><span class="sxs-lookup"><span data-stu-id="a0c29-229">Spark Structured Streaming Programming Guide</span></span>](http://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
* [<span data-ttu-id="a0c29-230">Launch Spark jobs remotely with LIVY</span><span class="sxs-lookup"><span data-stu-id="a0c29-230">Launch Spark jobs remotely with LIVY</span></span>](apache-spark-livy-rest-interface.md)
